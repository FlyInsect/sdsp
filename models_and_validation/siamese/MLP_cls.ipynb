{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32279ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve,auc,roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db1018c",
   "metadata": {},
   "source": [
    "### label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec7bb0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32827\n",
      "2418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    0.0\n",
       "2    0.0\n",
       "3    0.0\n",
       "4    0.0\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(\"../cls_labelWithfea.csv\")\n",
    "print(len(y.loc[y[\"label\"]==0]))\n",
    "print(len(y.loc[y[\"label\"]==1]))\n",
    "y = y[\"label\"]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83196b88",
   "metadata": {},
   "source": [
    "### input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43339ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35245, 644)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>312.1</th>\n",
       "      <th>313.1</th>\n",
       "      <th>314.1</th>\n",
       "      <th>315.1</th>\n",
       "      <th>316.1</th>\n",
       "      <th>317.1</th>\n",
       "      <th>318.1</th>\n",
       "      <th>319.1</th>\n",
       "      <th>320.1</th>\n",
       "      <th>321.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44542</th>\n",
       "      <td>12.516695</td>\n",
       "      <td>3.429878</td>\n",
       "      <td>56.012541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884564</td>\n",
       "      <td>20.813841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.298384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44544</th>\n",
       "      <td>12.516695</td>\n",
       "      <td>3.429878</td>\n",
       "      <td>56.012541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884564</td>\n",
       "      <td>20.813841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.298384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.58734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44545</th>\n",
       "      <td>41.173339</td>\n",
       "      <td>13.719514</td>\n",
       "      <td>66.142468</td>\n",
       "      <td>5.570439</td>\n",
       "      <td>3.380668</td>\n",
       "      <td>17.691285</td>\n",
       "      <td>70.767060</td>\n",
       "      <td>4.527233</td>\n",
       "      <td>3.374051</td>\n",
       "      <td>13.933278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44547</th>\n",
       "      <td>41.173339</td>\n",
       "      <td>13.719514</td>\n",
       "      <td>66.142468</td>\n",
       "      <td>5.570439</td>\n",
       "      <td>3.380668</td>\n",
       "      <td>17.691285</td>\n",
       "      <td>70.767060</td>\n",
       "      <td>4.527233</td>\n",
       "      <td>3.374051</td>\n",
       "      <td>13.933278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.58734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44549</th>\n",
       "      <td>8.234668</td>\n",
       "      <td>2.400915</td>\n",
       "      <td>20.855733</td>\n",
       "      <td>19.806004</td>\n",
       "      <td>8.451669</td>\n",
       "      <td>13.268464</td>\n",
       "      <td>15.818519</td>\n",
       "      <td>0.905447</td>\n",
       "      <td>2.530538</td>\n",
       "      <td>9.952341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.58734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 644 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0          1          2          3         4          5  \\\n",
       "44542  12.516695   3.429878  56.012541   0.000000  0.000000   0.884564   \n",
       "44544  12.516695   3.429878  56.012541   0.000000  0.000000   0.884564   \n",
       "44545  41.173339  13.719514  66.142468   5.570439  3.380668  17.691285   \n",
       "44547  41.173339  13.719514  66.142468   5.570439  3.380668  17.691285   \n",
       "44549   8.234668   2.400915  20.855733  19.806004  8.451669  13.268464   \n",
       "\n",
       "               6         7         8          9  ...  312.1     313.1  314.1  \\\n",
       "44542  20.813841  0.000000  0.000000   7.298384  ...    0.0   0.00000    0.0   \n",
       "44544  20.813841  0.000000  0.000000   7.298384  ...    0.0  17.58734    0.0   \n",
       "44545  70.767060  4.527233  3.374051  13.933278  ...    0.0   0.00000    0.0   \n",
       "44547  70.767060  4.527233  3.374051  13.933278  ...    0.0  17.58734    0.0   \n",
       "44549  15.818519  0.905447  2.530538   9.952341  ...    0.0  17.58734    0.0   \n",
       "\n",
       "       315.1  316.1  317.1  318.1  319.1  320.1  321.1  \n",
       "44542    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "44544    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "44545    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "44547    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "44549    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 644 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = pd.read_csv(\"dis_fea_mat_noActi.csv\",index_col=0)\n",
    "x = pd.read_csv(\"../dis_fea_mat.csv\",index_col=0)\n",
    "print(x.shape)\n",
    "x.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4307fe",
   "metadata": {},
   "source": [
    "### model---only symptom input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d01dd759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus_y的长度: 4836\n",
      "--------------Kfold: 1 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 7.3176 - binary_accuracy: 0.5358 - auc: 0.5572\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 2.6089 - binary_accuracy: 0.5678 - auc: 0.5921\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 2.2240 - binary_accuracy: 0.5758 - auc: 0.6019\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 1.7279 - binary_accuracy: 0.5742 - auc: 0.6091\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 1.2833 - binary_accuracy: 0.5767 - auc: 0.6199\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 1.0125 - binary_accuracy: 0.5974 - auc: 0.6532\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.9394 - binary_accuracy: 0.5915 - auc: 0.6503\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.9154 - binary_accuracy: 0.5931 - auc: 0.6491\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7896 - binary_accuracy: 0.6068 - auc: 0.6689\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7807 - binary_accuracy: 0.6103 - auc: 0.6729\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7018 - binary_accuracy: 0.6206 - auc: 0.6908\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7180 - binary_accuracy: 0.6324 - auc: 0.7025\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6827 - binary_accuracy: 0.6429 - auc: 0.7073\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6579 - binary_accuracy: 0.6551 - auc: 0.7253\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6593 - binary_accuracy: 0.6645 - auc: 0.7411\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6382 - binary_accuracy: 0.6705 - auc: 0.7476\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6268 - binary_accuracy: 0.6691 - auc: 0.7439\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 1000us/step - loss: 0.6157 - binary_accuracy: 0.6785 - auc: 0.7513\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6252 - binary_accuracy: 0.6781 - auc: 0.7526\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6114 - binary_accuracy: 0.6838 - auc: 0.7631\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5978 - binary_accuracy: 0.6997 - auc: 0.7717\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 989us/step - loss: 0.5970 - binary_accuracy: 0.6949 - auc: 0.7692\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5906 - binary_accuracy: 0.6932 - auc: 0.7782\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5828 - binary_accuracy: 0.7024 - auc: 0.7826\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5849 - binary_accuracy: 0.6969 - auc: 0.7783\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5829 - binary_accuracy: 0.7020 - auc: 0.7805\n",
      "16/16 [==============================] - 0s 886us/step - loss: 0.6229 - binary_accuracy: 0.6921 - auc: 0.7414\n",
      " binary_accuracy: 69.21%\t ------ auc: 74.14%\t \n",
      "--------------Kfold: 2 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 4.7652 - binary_accuracy: 0.5450 - auc_1: 0.5540\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 2.0574 - binary_accuracy: 0.5487 - auc_1: 0.5820\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 1.2457 - binary_accuracy: 0.5581 - auc_1: 0.5923\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.9060 - binary_accuracy: 0.5646 - auc_1: 0.6024\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8001 - binary_accuracy: 0.5682 - auc_1: 0.6109\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7598 - binary_accuracy: 0.5885 - auc_1: 0.6334\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7292 - binary_accuracy: 0.6105 - auc_1: 0.6590\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6952 - binary_accuracy: 0.6094 - auc_1: 0.6667\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6844 - binary_accuracy: 0.6216 - auc_1: 0.6824\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7037 - binary_accuracy: 0.6220 - auc_1: 0.6867\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6694 - binary_accuracy: 0.6259 - auc_1: 0.6914\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6473 - binary_accuracy: 0.6379 - auc_1: 0.7026\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6580 - binary_accuracy: 0.6445 - auc_1: 0.7193\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6357 - binary_accuracy: 0.6427 - auc_1: 0.7189\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6563 - binary_accuracy: 0.6680 - auc_1: 0.7446\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6196 - binary_accuracy: 0.6599 - auc_1: 0.7360\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6214 - binary_accuracy: 0.6684 - auc_1: 0.7422\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6087 - binary_accuracy: 0.6710 - auc_1: 0.7510\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6024 - binary_accuracy: 0.6751 - auc_1: 0.7561\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6562 - binary_accuracy: 0.6811 - auc_1: 0.7627\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5916 - binary_accuracy: 0.6857 - auc_1: 0.7672\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5951 - binary_accuracy: 0.6997 - auc_1: 0.7788\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5941 - binary_accuracy: 0.7001 - auc_1: 0.7821\n",
      "16/16 [==============================] - 0s 874us/step - loss: 0.6395 - binary_accuracy: 0.6529 - auc_1: 0.7139\n",
      " binary_accuracy: 65.29%\t ------ auc_1: 71.39%\t \n",
      "--------------Kfold: 3 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 3.5837 - binary_accuracy: 0.5347 - auc_2: 0.5507\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 1.4424 - binary_accuracy: 0.5676 - auc_2: 0.6000\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 1.1115 - binary_accuracy: 0.5919 - auc_2: 0.6308\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.9607 - binary_accuracy: 0.6068 - auc_2: 0.6496\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8033 - binary_accuracy: 0.6073 - auc_2: 0.6550\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7914 - binary_accuracy: 0.6289 - auc_2: 0.6813\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6788 - binary_accuracy: 0.6278 - auc_2: 0.6865\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6512 - binary_accuracy: 0.6429 - auc_2: 0.7119\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6464 - binary_accuracy: 0.6505 - auc_2: 0.7133\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6274 - binary_accuracy: 0.6537 - auc_2: 0.7234\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6142 - binary_accuracy: 0.6677 - auc_2: 0.7384\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6146 - binary_accuracy: 0.6664 - auc_2: 0.7431\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6120 - binary_accuracy: 0.6744 - auc_2: 0.7496\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5995 - binary_accuracy: 0.6864 - auc_2: 0.7613\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5977 - binary_accuracy: 0.6889 - auc_2: 0.7661\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5887 - binary_accuracy: 0.6949 - auc_2: 0.7690\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5875 - binary_accuracy: 0.6985 - auc_2: 0.7735\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5694 - binary_accuracy: 0.7029 - auc_2: 0.7880\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5753 - binary_accuracy: 0.7070 - auc_2: 0.7854\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5806 - binary_accuracy: 0.7160 - auc_2: 0.7919\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6613 - binary_accuracy: 0.6550 - auc_2: 0.7087\n",
      " binary_accuracy: 65.50%\t ------ auc_2: 70.87%\t \n",
      "--------------Kfold: 4 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 6.6890 - binary_accuracy: 0.5184 - auc_3: 0.5362\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 3.2456 - binary_accuracy: 0.5694 - auc_3: 0.6008\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 2.5189 - binary_accuracy: 0.5777 - auc_3: 0.6084\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 1.6783 - binary_accuracy: 0.5843 - auc_3: 0.6239\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 1.1817 - binary_accuracy: 0.5990 - auc_3: 0.6396\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.9691 - binary_accuracy: 0.6011 - auc_3: 0.6519\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.9526 - binary_accuracy: 0.6096 - auc_3: 0.6582\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8721 - binary_accuracy: 0.6188 - auc_3: 0.6743\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8056 - binary_accuracy: 0.6310 - auc_3: 0.6918\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7313 - binary_accuracy: 0.6330 - auc_3: 0.6925\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7049 - binary_accuracy: 0.6383 - auc_3: 0.7011\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7099 - binary_accuracy: 0.6390 - auc_3: 0.7117\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7169 - binary_accuracy: 0.6519 - auc_3: 0.7227\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6442 - binary_accuracy: 0.6116 - auc_3: 0.6848\n",
      " binary_accuracy: 61.16%\t ------ auc_3: 68.48%\t \n",
      "--------------Kfold: 5 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 6.6303 - binary_accuracy: 0.5260 - auc_4: 0.5394\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 3.0463 - binary_accuracy: 0.5639 - auc_4: 0.5902\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 1.8824 - binary_accuracy: 0.5749 - auc_4: 0.6065\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 1.5015 - binary_accuracy: 0.5832 - auc_4: 0.6145\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 1.2167 - binary_accuracy: 0.5940 - auc_4: 0.6299\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8901 - binary_accuracy: 0.5687 - auc_4: 0.6057\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8100 - binary_accuracy: 0.5813 - auc_4: 0.6274\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7160 - binary_accuracy: 0.6080 - auc_4: 0.6585\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7579 - binary_accuracy: 0.6183 - auc_4: 0.6727\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7065 - binary_accuracy: 0.6149 - auc_4: 0.6734\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 988us/step - loss: 0.6821 - binary_accuracy: 0.6395 - auc_4: 0.7015\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6604 - binary_accuracy: 0.6491 - auc_4: 0.7171\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6791 - binary_accuracy: 0.6528 - auc_4: 0.7217\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6519 - binary_accuracy: 0.6441 - auc_4: 0.7184\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6526 - binary_accuracy: 0.6562 - auc_4: 0.7360\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6387 - binary_accuracy: 0.6611 - auc_4: 0.7336\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6356 - binary_accuracy: 0.6673 - auc_4: 0.7488\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6254 - binary_accuracy: 0.6654 - auc_4: 0.7443\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6163 - binary_accuracy: 0.6824 - auc_4: 0.7579\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6174 - binary_accuracy: 0.6739 - auc_4: 0.7559\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6095 - binary_accuracy: 0.6889 - auc_4: 0.7715\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5925 - binary_accuracy: 0.6875 - auc_4: 0.7745\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5890 - binary_accuracy: 0.6944 - auc_4: 0.7825\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6131 - binary_accuracy: 0.6916 - auc_4: 0.7780\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5851 - binary_accuracy: 0.6994 - auc_4: 0.7850\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 989us/step - loss: 0.5654 - binary_accuracy: 0.6965 - auc_4: 0.7873\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5784 - binary_accuracy: 0.7061 - auc_4: 0.7939\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5689 - binary_accuracy: 0.7020 - auc_4: 0.7944\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6211 - binary_accuracy: 0.6674 - auc_4: 0.7418\n",
      " binary_accuracy: 66.74%\t ------ auc_4: 74.18%\t \n",
      "--------------Kfold: 6 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 4.8369 - binary_accuracy: 0.5464 - auc_5: 0.5583\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 2.4671 - binary_accuracy: 0.5793 - auc_5: 0.6040\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 1.2835 - binary_accuracy: 0.5722 - auc_5: 0.6013\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 1.3388 - binary_accuracy: 0.5777 - auc_5: 0.5999\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8881 - binary_accuracy: 0.5830 - auc_5: 0.6176\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7583 - binary_accuracy: 0.5871 - auc_5: 0.6289\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7319 - binary_accuracy: 0.6144 - auc_5: 0.6601\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7651 - binary_accuracy: 0.6218 - auc_5: 0.6756\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6931 - binary_accuracy: 0.6197 - auc_5: 0.6839\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6453 - binary_accuracy: 0.6326 - auc_5: 0.7013\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6625 - binary_accuracy: 0.6436 - auc_5: 0.7059\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6792 - binary_accuracy: 0.6422 - auc_5: 0.7097\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6703 - binary_accuracy: 0.6302 - auc_5: 0.7028\n",
      " binary_accuracy: 63.02%\t ------ auc_5: 70.28%\t \n",
      "--------------Kfold: 7 iter\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 2ms/step - loss: 7.9837 - binary_accuracy: 0.5206 - auc_6: 0.5361\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 2.7906 - binary_accuracy: 0.5573 - auc_6: 0.5852\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 1.8237 - binary_accuracy: 0.5762 - auc_6: 0.6158\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 1.1611 - binary_accuracy: 0.5840 - auc_6: 0.6291\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 1.0448 - binary_accuracy: 0.5847 - auc_6: 0.6339\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.8972 - binary_accuracy: 0.5934 - auc_6: 0.6431\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.7923 - binary_accuracy: 0.5989 - auc_6: 0.6511\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.7111 - binary_accuracy: 0.6138 - auc_6: 0.6741\n",
      "Epoch 9/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6834 - binary_accuracy: 0.6143 - auc_6: 0.6776\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6851 - binary_accuracy: 0.6221 - auc_6: 0.6894\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6785 - binary_accuracy: 0.6258 - auc_6: 0.6932\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6545 - binary_accuracy: 0.6396 - auc_6: 0.7132\n",
      "Epoch 13/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6506 - binary_accuracy: 0.6380 - auc_6: 0.7169\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6513 - binary_accuracy: 0.6483 - auc_6: 0.7226\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6321 - binary_accuracy: 0.6582 - auc_6: 0.7356\n",
      "Epoch 16/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6287 - binary_accuracy: 0.6657 - auc_6: 0.7374\n",
      "Epoch 17/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6479 - binary_accuracy: 0.6701 - auc_6: 0.7439\n",
      "Epoch 18/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6146 - binary_accuracy: 0.6692 - auc_6: 0.7469\n",
      "Epoch 19/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6099 - binary_accuracy: 0.6811 - auc_6: 0.7524\n",
      "Epoch 20/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6213 - binary_accuracy: 0.6894 - auc_6: 0.7557\n",
      "Epoch 21/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6045 - binary_accuracy: 0.6901 - auc_6: 0.7632\n",
      "Epoch 22/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6092 - binary_accuracy: 0.7007 - auc_6: 0.7716\n",
      "Epoch 23/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5988 - binary_accuracy: 0.7020 - auc_6: 0.7725\n",
      "Epoch 24/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5868 - binary_accuracy: 0.7066 - auc_6: 0.7721\n",
      "Epoch 25/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5809 - binary_accuracy: 0.7110 - auc_6: 0.7844\n",
      "Epoch 26/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5833 - binary_accuracy: 0.7085 - auc_6: 0.7772\n",
      "Epoch 27/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5777 - binary_accuracy: 0.7122 - auc_6: 0.7860\n",
      "Epoch 28/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5745 - binary_accuracy: 0.7179 - auc_6: 0.7931\n",
      "Epoch 29/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5777 - binary_accuracy: 0.7257 - auc_6: 0.7975\n",
      "Epoch 30/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5631 - binary_accuracy: 0.7232 - auc_6: 0.7990\n",
      "Epoch 31/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5658 - binary_accuracy: 0.7229 - auc_6: 0.8018\n",
      "Epoch 32/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5720 - binary_accuracy: 0.7259 - auc_6: 0.8022\n",
      "16/16 [==============================] - 0s 565us/step - loss: 0.6337 - binary_accuracy: 0.6480 - auc_6: 0.7140\n",
      " binary_accuracy: 64.80%\t ------ auc_6: 71.40%\t \n",
      "--------------Kfold: 8 iter\n",
      "Epoch 1/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 6.6674 - binary_accuracy: 0.5327 - auc_7: 0.5532\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 2.6130 - binary_accuracy: 0.5552 - auc_7: 0.5843\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 1.5524 - binary_accuracy: 0.5665 - auc_7: 0.5954\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 1.0774 - binary_accuracy: 0.5768 - auc_7: 0.6148\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.8798 - binary_accuracy: 0.5787 - auc_7: 0.6264\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.8381 - binary_accuracy: 0.5814 - auc_7: 0.6331\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.7273 - binary_accuracy: 0.6033 - auc_7: 0.6641\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.7091 - binary_accuracy: 0.5987 - auc_7: 0.6639\n",
      "Epoch 9/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6727 - binary_accuracy: 0.6173 - auc_7: 0.6871\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6925 - binary_accuracy: 0.6265 - auc_7: 0.6988\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6616 - binary_accuracy: 0.6329 - auc_7: 0.7088\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6483 - binary_accuracy: 0.6336 - auc_7: 0.7206\n",
      "Epoch 13/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6386 - binary_accuracy: 0.6448 - auc_7: 0.7184\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6427 - binary_accuracy: 0.6538 - auc_7: 0.7326\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6892 - binary_accuracy: 0.6568 - auc_7: 0.7364\n",
      "16/16 [==============================] - 0s 415us/step - loss: 0.6653 - binary_accuracy: 0.6542 - auc_7: 0.7138\n",
      " binary_accuracy: 65.42%\t ------ auc_7: 71.38%\t \n",
      "--------------Kfold: 9 iter\n",
      "Epoch 1/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 6.4544 - binary_accuracy: 0.5456 - auc_8: 0.5636\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 2.5635 - binary_accuracy: 0.5660 - auc_8: 0.5885\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 1.3318 - binary_accuracy: 0.5727 - auc_8: 0.6047\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.8562 - binary_accuracy: 0.5752 - auc_8: 0.6055\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.8492 - binary_accuracy: 0.5840 - auc_8: 0.6312\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.7332 - binary_accuracy: 0.5980 - auc_8: 0.6530\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6840 - binary_accuracy: 0.6108 - auc_8: 0.6794\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6831 - binary_accuracy: 0.6329 - auc_8: 0.7026\n",
      "Epoch 9/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6739 - binary_accuracy: 0.6366 - auc_8: 0.7061\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6673 - binary_accuracy: 0.6398 - auc_8: 0.7142\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6393 - binary_accuracy: 0.6522 - auc_8: 0.7244\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6741 - binary_accuracy: 0.6492 - auc_8: 0.7349\n",
      "Epoch 13/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6134 - binary_accuracy: 0.6680 - auc_8: 0.7441\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6272 - binary_accuracy: 0.6614 - auc_8: 0.7448\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6031 - binary_accuracy: 0.6722 - auc_8: 0.7578\n",
      "Epoch 16/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5923 - binary_accuracy: 0.6800 - auc_8: 0.7662\n",
      "Epoch 17/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5998 - binary_accuracy: 0.6756 - auc_8: 0.7661\n",
      "Epoch 18/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5991 - binary_accuracy: 0.6805 - auc_8: 0.7700\n",
      "16/16 [==============================] - 0s 938us/step - loss: 0.6977 - binary_accuracy: 0.6812 - auc_8: 0.7405\n",
      " binary_accuracy: 68.12%\t ------ auc_8: 74.05%\t \n",
      "--------------Kfold: 10 iter\n",
      "Epoch 1/100\n",
      "  1/137 [..............................] - ETA: 0s - loss: 3.8202 - binary_accuracy: 0.3438 - auc_9: 0.3917WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0156s). Check your callbacks.\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 4.0275 - binary_accuracy: 0.5327 - auc_9: 0.5514\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 1.6654 - binary_accuracy: 0.5465 - auc_9: 0.5721\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.9040 - binary_accuracy: 0.5428 - auc_9: 0.5704\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.8228 - binary_accuracy: 0.5348 - auc_9: 0.5592\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7635 - binary_accuracy: 0.5486 - auc_9: 0.5984\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7253 - binary_accuracy: 0.5729 - auc_9: 0.6302\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7063 - binary_accuracy: 0.5964 - auc_9: 0.6578\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6912 - binary_accuracy: 0.6083 - auc_9: 0.6705\n",
      "Epoch 9/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6930 - binary_accuracy: 0.6246 - auc_9: 0.6844\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6791 - binary_accuracy: 0.6421 - auc_9: 0.6968\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6642 - binary_accuracy: 0.6437 - auc_9: 0.7112\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6391 - binary_accuracy: 0.6490 - auc_9: 0.7162\n",
      "Epoch 13/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6901 - binary_accuracy: 0.6481 - auc_9: 0.7141\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6417 - binary_accuracy: 0.6520 - auc_9: 0.7234\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6414 - binary_accuracy: 0.6460 - auc_9: 0.7018\n",
      " binary_accuracy: 64.60%\t ------ auc_9: 70.18%\t \n",
      "ave_acc: 65.38% (+/- 2.20%)\t ------ ave_auc: 71.63% (+/- 1.82%)\t \n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 32)                20640     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 22,241\n",
      "Trainable params: 22,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "x_arr = x.values\n",
    "y_arr = y.values\n",
    "\n",
    "i=1\n",
    "cvacc=[]\n",
    "cvauc=[]\n",
    "\n",
    "seed = 22\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# np.random.seed(seed)\n",
    "python_random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy=1)\n",
    "\n",
    "rus_x,rus_y = rus.fit_resample(X=x_arr,y=y_arr)\n",
    "print('rus_y的长度:',len(rus_y))\n",
    "\n",
    "\n",
    "\"\"\"10-fold\"\"\"\n",
    "Kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\n",
    "for train,test in Kfold.split(X=rus_x,y=rus_y):\n",
    "    print(\"--------------Kfold: {} iter\".format(i))\n",
    "    i+=1\n",
    "    rms = RMSprop()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=2)\n",
    "    ##################################################################################################\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_dim=644))\n",
    "    model.add(Dropout(0.1))\n",
    "#     model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    \"\"\"L2\"\"\"\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer,'kernel_regularizer'):\n",
    "            layer.kernel_regularizer = tf.keras.regularizers.l2(0.001)\n",
    "    ##################################################################################################\n",
    "    model.compile(\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            optimizer=rms,\n",
    "            metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    \n",
    "    # fit()中 shuffle=True\n",
    "    model.fit(rus_x[train],rus_y[train],\n",
    "              #class_weight={0:1,1:1},\n",
    "              batch_size=None,#defalut 32\n",
    "              epochs=100,\n",
    "              callbacks=callback,\n",
    "              shuffle=True)\n",
    "    \n",
    "    scores = model.evaluate(rus_x[test],rus_y[test],\n",
    "                            verbose=1,\n",
    "                            batch_size=None,\n",
    "                            callbacks=callback)\n",
    "    \n",
    "    cvacc.append(scores[1] * 100)\n",
    "    cvauc.append(scores[2] * 100)\n",
    "    \n",
    "    print(\" %s: %.2f%%\\t ------ %s: %.2f%%\\t \" % \n",
    "           (model.metrics_names[1],scores[1]*100,model.metrics_names[2],scores[2]*100))\n",
    "    \n",
    "#print(\"ave_auc_keras: %.2f%% (+/- %.2f%%)\\t ave_topk_pre: %.2f%% (+/- %.2f%%)\\t ave_auc:%.2f%% (+/- %.2f%%)\" % \n",
    "print(\"ave_acc: %.2f%% (+/- %.2f%%)\\t ------ ave_auc: %.2f%% (+/- %.2f%%)\\t \" % \n",
    "      (np.mean(cvacc), np.std(cvacc),np.mean(cvauc), np.std(cvauc)))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd29fc",
   "metadata": {},
   "source": [
    "#### results:\n",
    "32-32-16-1/patience=2:\n",
    "    ave_acc: 65.38% (+/- 2.20%)\t ------ ave_auc: 71.63% (+/- 1.82%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094956f4",
   "metadata": {},
   "source": [
    "### model---sym+mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f803a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35245, 692)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38.1.1</th>\n",
       "      <th>39.1.1</th>\n",
       "      <th>40.1.1</th>\n",
       "      <th>41.1.1</th>\n",
       "      <th>42.1.1</th>\n",
       "      <th>43.1.1</th>\n",
       "      <th>44.1.1</th>\n",
       "      <th>45.1.1</th>\n",
       "      <th>46.1.1</th>\n",
       "      <th>47.1.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35240</th>\n",
       "      <td>12.516695</td>\n",
       "      <td>3.429878</td>\n",
       "      <td>56.012541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884564</td>\n",
       "      <td>20.813841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.298384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35241</th>\n",
       "      <td>12.516695</td>\n",
       "      <td>3.429878</td>\n",
       "      <td>56.012541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884564</td>\n",
       "      <td>20.813841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.298384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35242</th>\n",
       "      <td>41.173339</td>\n",
       "      <td>13.719514</td>\n",
       "      <td>66.142468</td>\n",
       "      <td>5.570439</td>\n",
       "      <td>3.380668</td>\n",
       "      <td>17.691285</td>\n",
       "      <td>70.767060</td>\n",
       "      <td>4.527233</td>\n",
       "      <td>3.374051</td>\n",
       "      <td>13.933278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35243</th>\n",
       "      <td>41.173339</td>\n",
       "      <td>13.719514</td>\n",
       "      <td>66.142468</td>\n",
       "      <td>5.570439</td>\n",
       "      <td>3.380668</td>\n",
       "      <td>17.691285</td>\n",
       "      <td>70.767060</td>\n",
       "      <td>4.527233</td>\n",
       "      <td>3.374051</td>\n",
       "      <td>13.933278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35244</th>\n",
       "      <td>8.234668</td>\n",
       "      <td>2.400915</td>\n",
       "      <td>20.855733</td>\n",
       "      <td>19.806004</td>\n",
       "      <td>8.451669</td>\n",
       "      <td>13.268464</td>\n",
       "      <td>15.818519</td>\n",
       "      <td>0.905447</td>\n",
       "      <td>2.530538</td>\n",
       "      <td>9.952341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 692 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0          1          2          3         4          5  \\\n",
       "35240  12.516695   3.429878  56.012541   0.000000  0.000000   0.884564   \n",
       "35241  12.516695   3.429878  56.012541   0.000000  0.000000   0.884564   \n",
       "35242  41.173339  13.719514  66.142468   5.570439  3.380668  17.691285   \n",
       "35243  41.173339  13.719514  66.142468   5.570439  3.380668  17.691285   \n",
       "35244   8.234668   2.400915  20.855733  19.806004  8.451669  13.268464   \n",
       "\n",
       "               6         7         8          9  ...  38.1.1  39.1.1  40.1.1  \\\n",
       "35240  20.813841  0.000000  0.000000   7.298384  ...     0.0     0.0     0.0   \n",
       "35241  20.813841  0.000000  0.000000   7.298384  ...     0.0     0.0     0.0   \n",
       "35242  70.767060  4.527233  3.374051  13.933278  ...     0.0     0.0     0.0   \n",
       "35243  70.767060  4.527233  3.374051  13.933278  ...     0.0     0.0     0.0   \n",
       "35244  15.818519  0.905447  2.530538   9.952341  ...     0.0     0.0     0.0   \n",
       "\n",
       "       41.1.1  42.1.1  43.1.1  44.1.1  45.1.1  46.1.1  47.1.1  \n",
       "35240     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "35241     0.0     0.0     0.0     0.0     1.0     1.0     0.0  \n",
       "35242     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "35243     0.0     0.0     0.0     0.0     1.0     1.0     0.0  \n",
       "35244     0.0     0.0     0.0     0.0     1.0     1.0     0.0  \n",
       "\n",
       "[5 rows x 692 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.read_csv(\"../sym_mesh_concat.csv\",index_col=0)\n",
    "print(x.shape)\n",
    "x.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dd391cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus_y的长度: 4836\n",
      "--------------Kfold: 1 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 3.5687 - binary_accuracy: 0.5186 - auc_62: 0.5399\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 1.3294 - binary_accuracy: 0.5528 - auc_62: 0.5696\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.9977 - binary_accuracy: 0.5634 - auc_62: 0.5752\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8381 - binary_accuracy: 0.5641 - auc_62: 0.5863\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7286 - binary_accuracy: 0.5850 - auc_62: 0.6142\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7087 - binary_accuracy: 0.5823 - auc_62: 0.6239\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7081 - binary_accuracy: 0.5924 - auc_62: 0.6375\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6848 - binary_accuracy: 0.6055 - auc_62: 0.6577\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6613 - binary_accuracy: 0.6167 - auc_62: 0.6768\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6697 - binary_accuracy: 0.6248 - auc_62: 0.6904\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6603 - binary_accuracy: 0.6560 - auc_62: 0.7278\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6573 - binary_accuracy: 0.6581 - auc_62: 0.7412\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6422 - binary_accuracy: 0.6682 - auc_62: 0.7446\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6169 - binary_accuracy: 0.6799 - auc_62: 0.7577\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6022 - binary_accuracy: 0.6898 - auc_62: 0.7702\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6045 - binary_accuracy: 0.6949 - auc_62: 0.7720\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5938 - binary_accuracy: 0.7011 - auc_62: 0.7821\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5913 - binary_accuracy: 0.7031 - auc_62: 0.7854\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5753 - binary_accuracy: 0.7063 - auc_62: 0.7920\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5717 - binary_accuracy: 0.7105 - auc_62: 0.7989\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5602 - binary_accuracy: 0.7199 - auc_62: 0.8043\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5597 - binary_accuracy: 0.7192 - auc_62: 0.8103\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5555 - binary_accuracy: 0.7330 - auc_62: 0.8176\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5478 - binary_accuracy: 0.7307 - auc_62: 0.8226\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5527 - binary_accuracy: 0.7482 - auc_62: 0.8355\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5462 - binary_accuracy: 0.7390 - auc_62: 0.8253\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5426 - binary_accuracy: 0.7445 - auc_62: 0.8379\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5239 - binary_accuracy: 0.7498 - auc_62: 0.8414\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5044 - binary_accuracy: 0.7518 - auc_62: 0.8471\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5092 - binary_accuracy: 0.7665 - auc_62: 0.8565\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4922 - binary_accuracy: 0.7619 - auc_62: 0.8568\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5116 - binary_accuracy: 0.7654 - auc_62: 0.8607\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4952 - binary_accuracy: 0.7727 - auc_62: 0.8621\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7000 - binary_accuracy: 0.6777 - auc_62: 0.7769\n",
      " binary_accuracy: 67.77%\t ------ auc_62: 77.69%\t \n",
      "--------------Kfold: 2 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 1.8113 - binary_accuracy: 0.5303 - auc_63: 0.5452\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.9152 - binary_accuracy: 0.5712 - auc_63: 0.5881\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7322 - binary_accuracy: 0.5664 - auc_63: 0.5810\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7152 - binary_accuracy: 0.5756 - auc_63: 0.6065\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6880 - binary_accuracy: 0.6000 - auc_63: 0.6438\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6753 - binary_accuracy: 0.6068 - auc_63: 0.6664\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6699 - binary_accuracy: 0.6245 - auc_63: 0.6923\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6574 - binary_accuracy: 0.6342 - auc_63: 0.7094\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6342 - binary_accuracy: 0.6484 - auc_63: 0.7226\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6225 - binary_accuracy: 0.6618 - auc_63: 0.7357\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6154 - binary_accuracy: 0.6719 - auc_63: 0.7435\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6017 - binary_accuracy: 0.6820 - auc_63: 0.7604\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5987 - binary_accuracy: 0.6854 - auc_63: 0.7632\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5921 - binary_accuracy: 0.7001 - auc_63: 0.7801\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5777 - binary_accuracy: 0.7045 - auc_63: 0.7829\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5702 - binary_accuracy: 0.7075 - auc_63: 0.7896\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5710 - binary_accuracy: 0.7116 - auc_63: 0.8039\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5563 - binary_accuracy: 0.7174 - auc_63: 0.8092\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5409 - binary_accuracy: 0.7224 - auc_63: 0.8143\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5468 - binary_accuracy: 0.7381 - auc_63: 0.8233\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5310 - binary_accuracy: 0.7316 - auc_63: 0.8230\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5341 - binary_accuracy: 0.7332 - auc_63: 0.8263\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5237 - binary_accuracy: 0.7401 - auc_63: 0.8341\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5264 - binary_accuracy: 0.7466 - auc_63: 0.8357\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5162 - binary_accuracy: 0.7511 - auc_63: 0.8447\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4882 - binary_accuracy: 0.7610 - auc_63: 0.8530\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5061 - binary_accuracy: 0.7617 - auc_63: 0.8543\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4904 - binary_accuracy: 0.7642 - auc_63: 0.8583\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6977 - binary_accuracy: 0.6818 - auc_63: 0.7492\n",
      " binary_accuracy: 68.18%\t ------ auc_63: 74.92%\t \n",
      "--------------Kfold: 3 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 3.3202 - binary_accuracy: 0.5285 - auc_64: 0.5425\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8740 - binary_accuracy: 0.5303 - auc_64: 0.5611\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7297 - binary_accuracy: 0.5593 - auc_64: 0.5919\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7299 - binary_accuracy: 0.5719 - auc_64: 0.6182\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6977 - binary_accuracy: 0.5875 - auc_64: 0.6465\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6934 - binary_accuracy: 0.6105 - auc_64: 0.6737\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7368 - binary_accuracy: 0.6149 - auc_64: 0.6835\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6517 - binary_accuracy: 0.6282 - auc_64: 0.7032\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6674 - binary_accuracy: 0.6379 - auc_64: 0.7100\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6392 - binary_accuracy: 0.6615 - auc_64: 0.7258\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6395 - binary_accuracy: 0.6574 - auc_64: 0.7284\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6866 - binary_accuracy: 0.6595 - auc_64: 0.7324\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6776 - binary_accuracy: 0.6632 - auc_64: 0.7000\n",
      " binary_accuracy: 66.32%\t ------ auc_64: 70.00%\t \n",
      "--------------Kfold: 4 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 2.8152 - binary_accuracy: 0.5237 - auc_65: 0.5287\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 1.3836 - binary_accuracy: 0.5485 - auc_65: 0.5581\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.9354 - binary_accuracy: 0.5568 - auc_65: 0.5718\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7440 - binary_accuracy: 0.5607 - auc_65: 0.5866\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7940 - binary_accuracy: 0.5864 - auc_65: 0.6265\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7309 - binary_accuracy: 0.5981 - auc_65: 0.6462\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6859 - binary_accuracy: 0.6066 - auc_65: 0.6598\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6619 - binary_accuracy: 0.6186 - auc_65: 0.6894\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6605 - binary_accuracy: 0.6356 - auc_65: 0.7030\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6953 - binary_accuracy: 0.6441 - auc_65: 0.7168\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6354 - binary_accuracy: 0.6544 - auc_65: 0.7295\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6588 - binary_accuracy: 0.6627 - auc_65: 0.7352\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6284 - binary_accuracy: 0.6728 - auc_65: 0.7469\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6400 - binary_accuracy: 0.6758 - auc_65: 0.7458\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6378 - binary_accuracy: 0.6854 - auc_65: 0.7611\n",
      "16/16 [==============================] - 0s 423us/step - loss: 0.6082 - binary_accuracy: 0.6674 - auc_65: 0.7235\n",
      " binary_accuracy: 66.74%\t ------ auc_65: 72.35%\t \n",
      "--------------Kfold: 5 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 1.8663 - binary_accuracy: 0.5124 - auc_66: 0.5256\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8125 - binary_accuracy: 0.5285 - auc_66: 0.5419\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7380 - binary_accuracy: 0.5432 - auc_66: 0.5543\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7228 - binary_accuracy: 0.5503 - auc_66: 0.5649\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7065 - binary_accuracy: 0.5836 - auc_66: 0.6172\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6890 - binary_accuracy: 0.5834 - auc_66: 0.6176\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7072 - binary_accuracy: 0.5947 - auc_66: 0.6394\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6929 - binary_accuracy: 0.6023 - auc_66: 0.6390\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6905 - binary_accuracy: 0.6157 - auc_66: 0.7019\n",
      " binary_accuracy: 61.57%\t ------ auc_66: 70.19%\t \n",
      "--------------Kfold: 6 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 2.6409 - binary_accuracy: 0.5237 - auc_67: 0.5280\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.9949 - binary_accuracy: 0.5478 - auc_67: 0.5588\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8900 - binary_accuracy: 0.5657 - auc_67: 0.5848\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7463 - binary_accuracy: 0.5761 - auc_67: 0.6079\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7218 - binary_accuracy: 0.5919 - auc_67: 0.6246\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6903 - binary_accuracy: 0.6158 - auc_67: 0.6722\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6729 - binary_accuracy: 0.6216 - auc_67: 0.6796\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6724 - binary_accuracy: 0.6234 - auc_67: 0.6945\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6531 - binary_accuracy: 0.6344 - auc_67: 0.7118\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6355 - binary_accuracy: 0.6427 - auc_67: 0.7263\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6333 - binary_accuracy: 0.6581 - auc_67: 0.7386\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6246 - binary_accuracy: 0.6583 - auc_67: 0.7475\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6268 - binary_accuracy: 0.6776 - auc_67: 0.7612\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6236 - binary_accuracy: 0.6822 - auc_67: 0.7627\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6119 - binary_accuracy: 0.6852 - auc_67: 0.7700\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6134 - binary_accuracy: 0.6845 - auc_67: 0.7749\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5996 - binary_accuracy: 0.7050 - auc_67: 0.7843\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5947 - binary_accuracy: 0.7082 - auc_67: 0.7974\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5763 - binary_accuracy: 0.7135 - auc_67: 0.7957\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5687 - binary_accuracy: 0.7128 - auc_67: 0.7991\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5535 - binary_accuracy: 0.7282 - auc_67: 0.8151\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5567 - binary_accuracy: 0.7321 - auc_67: 0.8176\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5354 - binary_accuracy: 0.7406 - auc_67: 0.8287\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5472 - binary_accuracy: 0.7374 - auc_67: 0.8282\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5229 - binary_accuracy: 0.7514 - auc_67: 0.8348\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5347 - binary_accuracy: 0.7530 - auc_67: 0.8413\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5370 - binary_accuracy: 0.7479 - auc_67: 0.8400\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6458 - binary_accuracy: 0.6860 - auc_67: 0.7684\n",
      " binary_accuracy: 68.60%\t ------ auc_67: 76.84%\t \n",
      "--------------Kfold: 7 iter\n",
      "Epoch 1/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 3.3689 - binary_accuracy: 0.5288 - auc_68: 0.5470\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 1.6739 - binary_accuracy: 0.5284 - auc_68: 0.5417\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 1.0851 - binary_accuracy: 0.5376 - auc_68: 0.5414\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8918 - binary_accuracy: 0.5376 - auc_68: 0.5478\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7558 - binary_accuracy: 0.5555 - auc_68: 0.5672\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7657 - binary_accuracy: 0.5647 - auc_68: 0.5893\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7211 - binary_accuracy: 0.5791 - auc_68: 0.6188\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7131 - binary_accuracy: 0.5920 - auc_68: 0.6318\n",
      "Epoch 9/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7050 - binary_accuracy: 0.6026 - auc_68: 0.6626\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7148 - binary_accuracy: 0.6035 - auc_68: 0.6684\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6975 - binary_accuracy: 0.6115 - auc_68: 0.6863\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6635 - binary_accuracy: 0.6334 - auc_68: 0.7024\n",
      "Epoch 13/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6596 - binary_accuracy: 0.6373 - auc_68: 0.7081\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6574 - binary_accuracy: 0.6435 - auc_68: 0.7256\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7024 - binary_accuracy: 0.6527 - auc_68: 0.7356\n",
      "Epoch 16/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6708 - binary_accuracy: 0.6701 - auc_68: 0.7482\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6673 - binary_accuracy: 0.6356 - auc_68: 0.6938\n",
      " binary_accuracy: 63.56%\t ------ auc_68: 69.38%\t \n",
      "--------------Kfold: 8 iter\n",
      "Epoch 1/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 1.8978 - binary_accuracy: 0.5091 - auc_69: 0.5150\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8276 - binary_accuracy: 0.5270 - auc_69: 0.5470\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7461 - binary_accuracy: 0.5663 - auc_69: 0.5898\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6970 - binary_accuracy: 0.5879 - auc_69: 0.6264\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6782 - binary_accuracy: 0.5961 - auc_69: 0.6416\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6978 - binary_accuracy: 0.6079 - auc_69: 0.6656\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6793 - binary_accuracy: 0.6223 - auc_69: 0.6886\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6284 - binary_accuracy: 0.6563 - auc_69: 0.7147\n",
      " binary_accuracy: 65.63%\t ------ auc_69: 71.47%\t \n",
      "--------------Kfold: 9 iter\n",
      "Epoch 1/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 2.8204 - binary_accuracy: 0.5281 - auc_70: 0.5350\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 1.0448 - binary_accuracy: 0.5146 - auc_70: 0.5263\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8585 - binary_accuracy: 0.5288 - auc_70: 0.5391\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7773 - binary_accuracy: 0.5385 - auc_70: 0.5562\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7437 - binary_accuracy: 0.5484 - auc_70: 0.5844\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7636 - binary_accuracy: 0.5504 - auc_70: 0.6125\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7366 - binary_accuracy: 0.5532 - auc_70: 0.6329\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7223 - binary_accuracy: 0.5879 - auc_70: 0.6544\n",
      "Epoch 9/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6931 - binary_accuracy: 0.6180 - auc_70: 0.6873\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6594 - binary_accuracy: 0.6295 - auc_70: 0.6979\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6497 - binary_accuracy: 0.6460 - auc_70: 0.7149\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6570 - binary_accuracy: 0.6425 - auc_70: 0.7181\n",
      "Epoch 13/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6324 - binary_accuracy: 0.6584 - auc_70: 0.7410\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6341 - binary_accuracy: 0.6697 - auc_70: 0.7434\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6198 - binary_accuracy: 0.6798 - auc_70: 0.7546\n",
      "Epoch 16/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5959 - binary_accuracy: 0.6899 - auc_70: 0.7726\n",
      "Epoch 17/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6357 - binary_accuracy: 0.6887 - auc_70: 0.7704\n",
      "Epoch 18/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5962 - binary_accuracy: 0.6938 - auc_70: 0.7785\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6303 - binary_accuracy: 0.6729 - auc_70: 0.7538\n",
      " binary_accuracy: 67.29%\t ------ auc_70: 75.38%\t \n",
      "--------------Kfold: 10 iter\n",
      "Epoch 1/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 1.8121 - binary_accuracy: 0.5291 - auc_71: 0.5436\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8426 - binary_accuracy: 0.5536 - auc_71: 0.5684\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7393 - binary_accuracy: 0.5722 - auc_71: 0.6016\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.7119 - binary_accuracy: 0.5941 - auc_71: 0.6400\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7095 - binary_accuracy: 0.5927 - auc_71: 0.6374\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6879 - binary_accuracy: 0.6010 - auc_71: 0.6504\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6724 - binary_accuracy: 0.6290 - auc_71: 0.6975\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6570 - binary_accuracy: 0.6423 - auc_71: 0.7045\n",
      "Epoch 9/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6555 - binary_accuracy: 0.6391 - auc_71: 0.7112\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6567 - binary_accuracy: 0.6531 - auc_71: 0.7296\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6333 - binary_accuracy: 0.6710 - auc_71: 0.7418\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6159 - binary_accuracy: 0.6839 - auc_71: 0.7539\n",
      "Epoch 13/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6095 - binary_accuracy: 0.6788 - auc_71: 0.7570\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6061 - binary_accuracy: 0.6940 - auc_71: 0.7711\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5946 - binary_accuracy: 0.7016 - auc_71: 0.7781\n",
      "Epoch 16/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5973 - binary_accuracy: 0.7002 - auc_71: 0.7799\n",
      "Epoch 17/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5811 - binary_accuracy: 0.7179 - auc_71: 0.7954\n",
      "Epoch 18/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5926 - binary_accuracy: 0.7154 - auc_71: 0.7973\n",
      "Epoch 19/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5737 - binary_accuracy: 0.7243 - auc_71: 0.8044\n",
      "Epoch 20/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5481 - binary_accuracy: 0.7289 - auc_71: 0.8134\n",
      "Epoch 21/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5464 - binary_accuracy: 0.7393 - auc_71: 0.8193\n",
      "Epoch 22/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5427 - binary_accuracy: 0.7381 - auc_71: 0.8251\n",
      "Epoch 23/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5348 - binary_accuracy: 0.7374 - auc_71: 0.8253\n",
      "Epoch 24/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5316 - binary_accuracy: 0.7496 - auc_71: 0.8357\n",
      "Epoch 25/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5252 - binary_accuracy: 0.7528 - auc_71: 0.8410\n",
      "Epoch 26/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5210 - binary_accuracy: 0.7521 - auc_71: 0.8385\n",
      "Epoch 27/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5310 - binary_accuracy: 0.7604 - auc_71: 0.8436\n",
      "Epoch 28/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5183 - binary_accuracy: 0.7611 - auc_71: 0.8458\n",
      "Epoch 29/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4990 - binary_accuracy: 0.7717 - auc_71: 0.8532\n",
      "Epoch 30/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.5050 - binary_accuracy: 0.7733 - auc_71: 0.8534\n",
      "Epoch 31/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.5036 - binary_accuracy: 0.7710 - auc_71: 0.8550\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6309 - binary_accuracy: 0.6977 - auc_71: 0.7631\n",
      " binary_accuracy: 69.77%\t ------ auc_71: 76.31%\t \n",
      "ave_acc: 66.54% (+/- 2.32%)\t ------ ave_auc: 73.46% (+/- 2.96%)\t \n",
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_352 (Dense)            (None, 128)               88704     \n",
      "_________________________________________________________________\n",
      "dropout_271 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_353 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_272 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_354 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_273 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_355 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_274 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_356 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_275 (Dropout)        (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_357 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 103,745\n",
      "Trainable params: 103,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "x_arr = x.values\n",
    "y_arr = y.values\n",
    "\n",
    "i=1\n",
    "cvacc=[]\n",
    "cvauc=[]\n",
    "\n",
    "seed = 22\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# np.random.seed(seed)\n",
    "python_random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy=1)\n",
    "\n",
    "rus_x,rus_y = rus.fit_resample(X=x_arr,y=y_arr)\n",
    "print('rus_y的长度:',len(rus_y))\n",
    "\n",
    "\n",
    "\"\"\"10-fold\"\"\"\n",
    "Kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\n",
    "for train,test in Kfold.split(X=rus_x,y=rus_y):\n",
    "    print(\"--------------Kfold: {} iter\".format(i))\n",
    "    i+=1\n",
    "    rms = RMSprop()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=2)\n",
    "    ##################################################################################################\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=692))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    \"\"\"L2\"\"\"\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer,'kernel_regularizer'):\n",
    "            layer.kernel_regularizer = tf.keras.regularizers.l2(0.001)\n",
    "    ##################################################################################################\n",
    "    model.compile(\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            optimizer=rms,\n",
    "            metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    \n",
    "    # fit()中 shuffle=True\n",
    "    model.fit(rus_x[train],rus_y[train],\n",
    "              #class_weight={0:1,1:1},\n",
    "              batch_size=None,\n",
    "              epochs=100,\n",
    "              callbacks=callback,\n",
    "              shuffle=True)\n",
    "    \n",
    "    scores = model.evaluate(rus_x[test],rus_y[test],\n",
    "                            verbose=1,\n",
    "                            batch_size=None,\n",
    "                            callbacks=callback)\n",
    "    \n",
    "    cvacc.append(scores[1] * 100)\n",
    "    cvauc.append(scores[2] * 100)\n",
    "    \n",
    "    print(\" %s: %.2f%%\\t ------ %s: %.2f%%\\t \" % \n",
    "           (model.metrics_names[1],scores[1]*100,model.metrics_names[2],scores[2]*100))\n",
    "    \n",
    "#print(\"ave_auc_keras: %.2f%% (+/- %.2f%%)\\t ave_topk_pre: %.2f%% (+/- %.2f%%)\\t ave_auc:%.2f%% (+/- %.2f%%)\" % \n",
    "print(\"ave_acc: %.2f%% (+/- %.2f%%)\\t ------ ave_auc: %.2f%% (+/- %.2f%%)\\t \" % \n",
    "      (np.mean(cvacc), np.std(cvacc),np.mean(cvauc), np.std(cvauc)))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f69b5",
   "metadata": {},
   "source": [
    "#### results\n",
    "    1.64-32-16-1/patience=2:        ave_acc: 64.85% (+/- 3.89%)\t ------ ave_auc: 71.16% (+/- 4.31%)\n",
    "    2.128-64-64-32-16-1/patience=2:   ave_acc: 66.54% (+/- 2.32%)\t ------ ave_auc: 73.46% (+/- 2.96%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7675272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
