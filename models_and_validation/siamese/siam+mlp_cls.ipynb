{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y label 0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32827\n",
      "2418\n"
     ]
    }
   ],
   "source": [
    "y = pd.read_csv(\"../cls_labelWithfea.csv\")\n",
    "print(len(y.loc[y[\"label\"] == 0]))\n",
    "print(len(y.loc[y[\"label\"] == 1]))\n",
    "y = y[\"label\"]\n",
    "#y.head()\n",
    "#len(y[y==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### symptom（，322）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.646934</td>\n",
       "      <td>16.806405</td>\n",
       "      <td>21.451611</td>\n",
       "      <td>11.759815</td>\n",
       "      <td>115.787869</td>\n",
       "      <td>2.653693</td>\n",
       "      <td>93.246009</td>\n",
       "      <td>2261.805817</td>\n",
       "      <td>5.061076</td>\n",
       "      <td>18.577704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.646934</td>\n",
       "      <td>16.806405</td>\n",
       "      <td>21.451611</td>\n",
       "      <td>11.759815</td>\n",
       "      <td>115.787869</td>\n",
       "      <td>2.653693</td>\n",
       "      <td>93.246009</td>\n",
       "      <td>2261.805817</td>\n",
       "      <td>5.061076</td>\n",
       "      <td>18.577704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.646934</td>\n",
       "      <td>16.806405</td>\n",
       "      <td>21.451611</td>\n",
       "      <td>11.759815</td>\n",
       "      <td>115.787869</td>\n",
       "      <td>2.653693</td>\n",
       "      <td>93.246009</td>\n",
       "      <td>2261.805817</td>\n",
       "      <td>5.061076</td>\n",
       "      <td>18.577704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.646934</td>\n",
       "      <td>16.806405</td>\n",
       "      <td>21.451611</td>\n",
       "      <td>11.759815</td>\n",
       "      <td>115.787869</td>\n",
       "      <td>2.653693</td>\n",
       "      <td>93.246009</td>\n",
       "      <td>2261.805817</td>\n",
       "      <td>5.061076</td>\n",
       "      <td>18.577704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.646934</td>\n",
       "      <td>16.806405</td>\n",
       "      <td>21.451611</td>\n",
       "      <td>11.759815</td>\n",
       "      <td>115.787869</td>\n",
       "      <td>2.653693</td>\n",
       "      <td>93.246009</td>\n",
       "      <td>2261.805817</td>\n",
       "      <td>5.061076</td>\n",
       "      <td>18.577704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 322 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3           4         5  \\\n",
       "43  1.646934  16.806405  21.451611  11.759815  115.787869  2.653693   \n",
       "43  1.646934  16.806405  21.451611  11.759815  115.787869  2.653693   \n",
       "43  1.646934  16.806405  21.451611  11.759815  115.787869  2.653693   \n",
       "43  1.646934  16.806405  21.451611  11.759815  115.787869  2.653693   \n",
       "43  1.646934  16.806405  21.451611  11.759815  115.787869  2.653693   \n",
       "\n",
       "            6            7         8          9  ...  312  313  314  315  316  \\\n",
       "43  93.246009  2261.805817  5.061076  18.577704  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "43  93.246009  2261.805817  5.061076  18.577704  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "43  93.246009  2261.805817  5.061076  18.577704  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "43  93.246009  2261.805817  5.061076  18.577704  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "43  93.246009  2261.805817  5.061076  18.577704  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    317  318  319  320  321  \n",
       "43  0.0  0.0  0.0  0.0  0.0  \n",
       "43  0.0  0.0  0.0  0.0  0.0  \n",
       "43  0.0  0.0  0.0  0.0  0.0  \n",
       "43  0.0  0.0  0.0  0.0  0.0  \n",
       "43  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 322 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disA_fea_mat = pd.read_csv(\"disA_fea_mat.csv\", index_col=0, header=\"infer\")\n",
    "disB_fea_mat = pd.read_csv(\"disB_fea_mat.csv\", index_col=0, header=\"infer\")\n",
    "\n",
    "# disA_fea_mat = pd.read_csv(\"concat_mesh_label/con_feaA.csv\",index_col=0,header=\"infer\")\n",
    "# disB_fea_mat = pd.read_csv(\"concat_mesh_label/con_feaB.csv\",index_col=0,header=\"infer\")\n",
    "disA_fea_mat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     9,
     37,
     97
    ]
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def e_auc(y_true, y_pred):\n",
    "    def euclidean_AUC(y_true,y_pred):\n",
    "        #tf.compat.v1.disable_eager_execution()\n",
    "        #with tf.compat.v1.Session() as sess:\n",
    "            #fpr,tpr,threshold = roc_curve(y_true.eval(),y_pred.eval())\n",
    "        max_pre = np.max(y_pred)\n",
    "        N_y_pred = [(max_pre-i)/max_pre for i in y_pred]\n",
    "        \n",
    "        roc_auc = roc_auc_score(y_true,N_y_pred)\n",
    "        return roc_auc\n",
    "\n",
    "    return tf.py_function(euclidean_AUC, (y_true, y_pred),Tout=float)\n",
    "\n",
    "def top_precision(y_true,y_pred,k):\n",
    "    sorted_pred = np.sort(y_pred)\n",
    "    k_threshold = sorted_pred[k-1] \n",
    "    \n",
    "    y_top = [i <= k_threshold for i in y_pred] \n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_top = np.array(y_top) #boolean\n",
    "    y_true_top = y_true[y_top] \n",
    "    \n",
    "    y_true_top = list(y_true_top)\n",
    "    top_k_precision = y_true_top.count(1.)/k\n",
    "    \n",
    "    return top_k_precision\n",
    "####################################################################################################################\n",
    "def create_base_network_copy(input_shape):\n",
    "    '''Base network to be shared (eq. to feature extraction).'''\n",
    "    input = Input(shape=input_shape)\n",
    "    \n",
    "    x= Activation(activation='relu')(Dense(128)(input))\n",
    "#     x= Activation(activation='relu')(Dense(64)(input))\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x= Activation(activation='relu')(Dense(64)(x))\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    \n",
    "    x= Activation(activation='relu')(Dense(64)(x))\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    \n",
    "    x= Activation(activation='relu')(Dense(32)(x))\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "#     x= Activation(activation='relu')(Dense(32)(x))\n",
    "#     x = Dropout(0.1)(x)\n",
    "    \n",
    "    x= Activation(activation='relu')(Dense(32)(x))\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    x= Activation(activation='sigmoid')(Dense(32)(x))\n",
    "#     x = Dense(32)(x)\n",
    "    \n",
    "    return Model(input, x,name=\"base_net\")\n",
    "\n",
    "def create_base_network(input_shape):\n",
    "    '''Base network to be shared (eq. to feature extraction).'''\n",
    "    input = Input(shape=input_shape)\n",
    "    \n",
    "    x= Activation(activation='relu')(Dense(128)(input))\n",
    "#     x= Activation(activation='relu')(Dense(64)(x))\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    \n",
    "    x= Activation(activation='relu')(Dense(64)(x))\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x= Activation(activation='relu')(Dense(32)(x))\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "#     x= Activation(activation='sigmoid')(Dense(32)(x))\n",
    "    x= Activation(activation='sigmoid')(Dense(32)(x))\n",
    "    return Model(input, x,name=\"base_net\")\n",
    "\n",
    "def create_euclLayer(input_shape):\n",
    "    pair1 = Input(shape=(input_shape))\n",
    "    pair2 = Input(shape=(input_shape))\n",
    "              \n",
    "    x = euclLayer()(pair1,pair2)\n",
    "    x = Activation(activation='relu')(x)\n",
    "#     x = Activation(activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(inputs=[pair1,pair2],outputs=x,name=\"siamese\")\n",
    "    \n",
    "\"\"\"state-of-the-art：input(64)-64-32-16-1\"\"\"\n",
    "def create_MLP_copy(input_shape):\n",
    "    \n",
    "    input = Input(shape=input_shape)\n",
    "    x= Activation(activation='relu')(Dense(64)(input))\n",
    "#     x= Activation(activation='relu')(Dense(32)(input))\n",
    "    x= Activation(activation='relu')(Dense(32)(x))\n",
    "    x= Activation(activation='relu')(Dense(16)(x))\n",
    "#     x= Activation(activation='relu')(Dense(8)(x))\n",
    "    \n",
    "#     x= Activation(activation='sigmoid',name=\"MLP_out\")(Dense(1)(x))\n",
    "    x= Dense(1,activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(input, x ,name=\"mlp\")\n",
    "\n",
    "def create_MLP(input_shape):\n",
    "    \n",
    "    input = Input(shape=input_shape)\n",
    "    x= Activation(activation='relu')(Dense(64)(input))\n",
    "    \n",
    "    x= Activation(activation='relu')(Dense(32)(x))\n",
    "    \n",
    "#     x= Activation(activation='relu')(Dense(32)(x))\n",
    "    x= Activation(activation='relu')(Dense(16)(x))\n",
    "    \n",
    "    x= Activation(activation='relu')(Dense(8)(x))\n",
    "    \n",
    "#     x= Activation(activation='sigmoid',name=\"MLP_out\")(Dense(1)(x))\n",
    "    x= Dense(1,activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(input, x ,name=\"mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 1:only input symptom (siamese +mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus_y的长度: 4836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'PYTHONHASHSEED' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat_fea的shape： (None, 64)\n",
      "-------------------------------------Kfold: 1 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6848 - siamese_loss: 3.0520 - mlp_loss: 0.6848 - mlp_binary_accuracy: 0.5469 - mlp_auc_177: 0.5659\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6411 - siamese_loss: 3.2738 - mlp_loss: 0.6411 - mlp_binary_accuracy: 0.6266 - mlp_auc_177: 0.6707\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6094 - siamese_loss: 3.4145 - mlp_loss: 0.6094 - mlp_binary_accuracy: 0.6730 - mlp_auc_177: 0.7284\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5926 - siamese_loss: 3.7201 - mlp_loss: 0.5926 - mlp_binary_accuracy: 0.6870 - mlp_auc_177: 0.7449\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5777 - siamese_loss: 3.9795 - mlp_loss: 0.5777 - mlp_binary_accuracy: 0.7027 - mlp_auc_177: 0.7637\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5727 - siamese_loss: 4.1341 - mlp_loss: 0.5727 - mlp_binary_accuracy: 0.7008 - mlp_auc_177: 0.7686\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5554 - siamese_loss: 4.1288 - mlp_loss: 0.5554 - mlp_binary_accuracy: 0.7263 - mlp_auc_177: 0.7867\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5515 - siamese_loss: 4.0182 - mlp_loss: 0.5515 - mlp_binary_accuracy: 0.7231 - mlp_auc_177: 0.7923\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5405 - siamese_loss: 4.0230 - mlp_loss: 0.5405 - mlp_binary_accuracy: 0.7316 - mlp_auc_177: 0.8012\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5415 - siamese_loss: 4.0992 - mlp_loss: 0.5415 - mlp_binary_accuracy: 0.7279 - mlp_auc_177: 0.8011\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5377 - siamese_loss: 4.0930 - mlp_loss: 0.5377 - mlp_binary_accuracy: 0.7302 - mlp_auc_177: 0.8051\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5243 - siamese_loss: 4.2062 - mlp_loss: 0.5243 - mlp_binary_accuracy: 0.7477 - mlp_auc_177: 0.8165\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5211 - siamese_loss: 4.2429 - mlp_loss: 0.5211 - mlp_binary_accuracy: 0.7486 - mlp_auc_177: 0.8186\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5133 - siamese_loss: 4.4785 - mlp_loss: 0.5133 - mlp_binary_accuracy: 0.7509 - mlp_auc_177: 0.8258\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5082 - siamese_loss: 4.3336 - mlp_loss: 0.5082 - mlp_binary_accuracy: 0.7541 - mlp_auc_177: 0.8304\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5084 - siamese_loss: 4.4447 - mlp_loss: 0.5084 - mlp_binary_accuracy: 0.7546 - mlp_auc_177: 0.8296\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5019 - siamese_loss: 4.4791 - mlp_loss: 0.5019 - mlp_binary_accuracy: 0.7599 - mlp_auc_177: 0.8353\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4968 - siamese_loss: 4.5324 - mlp_loss: 0.4968 - mlp_binary_accuracy: 0.7619 - mlp_auc_177: 0.8387\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4936 - siamese_loss: 4.4058 - mlp_loss: 0.4936 - mlp_binary_accuracy: 0.7613 - mlp_auc_177: 0.8411\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4943 - siamese_loss: 4.6789 - mlp_loss: 0.4943 - mlp_binary_accuracy: 0.7649 - mlp_auc_177: 0.8403\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4917 - siamese_loss: 4.5973 - mlp_loss: 0.4917 - mlp_binary_accuracy: 0.7670 - mlp_auc_177: 0.8425\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4840 - siamese_loss: 4.6001 - mlp_loss: 0.4840 - mlp_binary_accuracy: 0.7714 - mlp_auc_177: 0.8476\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4913 - siamese_loss: 4.7682 - mlp_loss: 0.4913 - mlp_binary_accuracy: 0.7691 - mlp_auc_177: 0.8426\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4910 - siamese_loss: 4.6618 - mlp_loss: 0.4910 - mlp_binary_accuracy: 0.7661 - mlp_auc_177: 0.8430\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4832 - siamese_loss: 4.7602 - mlp_loss: 0.4832 - mlp_binary_accuracy: 0.7709 - mlp_auc_177: 0.8490\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4836 - siamese_loss: 4.7718 - mlp_loss: 0.4836 - mlp_binary_accuracy: 0.7716 - mlp_auc_177: 0.8483\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4772 - siamese_loss: 4.7853 - mlp_loss: 0.4772 - mlp_binary_accuracy: 0.7808 - mlp_auc_177: 0.8526\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4720 - siamese_loss: 5.1056 - mlp_loss: 0.4720 - mlp_binary_accuracy: 0.7845 - mlp_auc_177: 0.8566\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4736 - siamese_loss: 5.0890 - mlp_loss: 0.4736 - mlp_binary_accuracy: 0.7785 - mlp_auc_177: 0.8555\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4675 - siamese_loss: 5.0027 - mlp_loss: 0.4675 - mlp_binary_accuracy: 0.7792 - mlp_auc_177: 0.8595\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4726 - siamese_loss: 4.8650 - mlp_loss: 0.4726 - mlp_binary_accuracy: 0.7822 - mlp_auc_177: 0.8561\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4628 - siamese_loss: 5.0893 - mlp_loss: 0.4628 - mlp_binary_accuracy: 0.7794 - mlp_auc_177: 0.8621\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4639 - siamese_loss: 5.0769 - mlp_loss: 0.4639 - mlp_binary_accuracy: 0.7835 - mlp_auc_177: 0.8624\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4667 - siamese_loss: 5.0126 - mlp_loss: 0.4667 - mlp_binary_accuracy: 0.7808 - mlp_auc_177: 0.8613\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4582 - siamese_loss: 5.1951 - mlp_loss: 0.4582 - mlp_binary_accuracy: 0.7904 - mlp_auc_177: 0.8659\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4582 - siamese_loss: 5.0686 - mlp_loss: 0.4582 - mlp_binary_accuracy: 0.7861 - mlp_auc_177: 0.8660\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4566 - siamese_loss: 5.2810 - mlp_loss: 0.4566 - mlp_binary_accuracy: 0.7868 - mlp_auc_177: 0.8660\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4472 - siamese_loss: 5.3364 - mlp_loss: 0.4472 - mlp_binary_accuracy: 0.7941 - mlp_auc_177: 0.8725\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4492 - siamese_loss: 5.1095 - mlp_loss: 0.4492 - mlp_binary_accuracy: 0.7943 - mlp_auc_177: 0.8710\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4449 - siamese_loss: 5.1284 - mlp_loss: 0.4449 - mlp_binary_accuracy: 0.7960 - mlp_auc_177: 0.8744\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4477 - siamese_loss: 5.1499 - mlp_loss: 0.4477 - mlp_binary_accuracy: 0.7948 - mlp_auc_177: 0.8728\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4429 - siamese_loss: 5.1915 - mlp_loss: 0.4429 - mlp_binary_accuracy: 0.7957 - mlp_auc_177: 0.8747\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4346 - siamese_loss: 5.1719 - mlp_loss: 0.4346 - mlp_binary_accuracy: 0.7978 - mlp_auc_177: 0.8798\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4429 - siamese_loss: 5.1052 - mlp_loss: 0.4429 - mlp_binary_accuracy: 0.7992 - mlp_auc_177: 0.8753\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4360 - siamese_loss: 5.4571 - mlp_loss: 0.4360 - mlp_binary_accuracy: 0.8019 - mlp_auc_177: 0.8792\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4504 - siamese_loss: 5.7086 - mlp_loss: 0.4504 - mlp_binary_accuracy: 0.7927 - mlp_auc_177: 0.8706\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4322 - siamese_loss: 5.8005 - mlp_loss: 0.4322 - mlp_binary_accuracy: 0.8019 - mlp_auc_177: 0.8821\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4425 - siamese_loss: 5.6788 - mlp_loss: 0.4425 - mlp_binary_accuracy: 0.8017 - mlp_auc_177: 0.8751\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4397 - siamese_loss: 5.6474 - mlp_loss: 0.4397 - mlp_binary_accuracy: 0.7948 - mlp_auc_177: 0.8779\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4402 - siamese_loss: 5.9124 - mlp_loss: 0.4402 - mlp_binary_accuracy: 0.8001 - mlp_auc_177: 0.8774\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4369 - siamese_loss: 5.9163 - mlp_loss: 0.4369 - mlp_binary_accuracy: 0.8063 - mlp_auc_177: 0.8793\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4374 - siamese_loss: 5.9412 - mlp_loss: 0.4374 - mlp_binary_accuracy: 0.8010 - mlp_auc_177: 0.8797\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5386 - siamese_loss: 5.4963 - mlp_loss: 0.5386 - mlp_binary_accuracy: 0.7665 - mlp_auc_177: 0.8159\n",
      "------ mlp_binary_accuracy: 76.65%\t ----- mlp_auc_177: 81.59%\n",
      "concat_fea的shape： (None, 64)\n",
      "-------------------------------------Kfold: 2 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6770 - siamese_loss: 3.1366 - mlp_loss: 0.6770 - mlp_binary_accuracy: 0.5758 - mlp_auc_178: 0.6044\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6378 - siamese_loss: 3.6259 - mlp_loss: 0.6378 - mlp_binary_accuracy: 0.6344 - mlp_auc_178: 0.6883\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6080 - siamese_loss: 3.5098 - mlp_loss: 0.6080 - mlp_binary_accuracy: 0.6712 - mlp_auc_178: 0.7300\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5960 - siamese_loss: 3.8989 - mlp_loss: 0.5960 - mlp_binary_accuracy: 0.6778 - mlp_auc_178: 0.7448\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5838 - siamese_loss: 4.1123 - mlp_loss: 0.5838 - mlp_binary_accuracy: 0.6937 - mlp_auc_178: 0.7582\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5705 - siamese_loss: 4.1487 - mlp_loss: 0.5705 - mlp_binary_accuracy: 0.7038 - mlp_auc_178: 0.7714\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5642 - siamese_loss: 4.2290 - mlp_loss: 0.5642 - mlp_binary_accuracy: 0.7137 - mlp_auc_178: 0.7800\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5536 - siamese_loss: 4.0263 - mlp_loss: 0.5536 - mlp_binary_accuracy: 0.7224 - mlp_auc_178: 0.7905\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5433 - siamese_loss: 4.0864 - mlp_loss: 0.5433 - mlp_binary_accuracy: 0.7325 - mlp_auc_178: 0.7998\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5429 - siamese_loss: 4.2369 - mlp_loss: 0.5429 - mlp_binary_accuracy: 0.7312 - mlp_auc_178: 0.8004\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5412 - siamese_loss: 3.9952 - mlp_loss: 0.5412 - mlp_binary_accuracy: 0.7298 - mlp_auc_178: 0.8039\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5254 - siamese_loss: 4.0881 - mlp_loss: 0.5254 - mlp_binary_accuracy: 0.7454 - mlp_auc_178: 0.8160\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5323 - siamese_loss: 4.4087 - mlp_loss: 0.5323 - mlp_binary_accuracy: 0.7420 - mlp_auc_178: 0.8113\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5306 - siamese_loss: 4.2842 - mlp_loss: 0.5306 - mlp_binary_accuracy: 0.7394 - mlp_auc_178: 0.8126\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5235 - siamese_loss: 4.2926 - mlp_loss: 0.5235 - mlp_binary_accuracy: 0.7449 - mlp_auc_178: 0.8184\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5185 - siamese_loss: 4.3350 - mlp_loss: 0.5185 - mlp_binary_accuracy: 0.7477 - mlp_auc_178: 0.8218\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5194 - siamese_loss: 4.4348 - mlp_loss: 0.5194 - mlp_binary_accuracy: 0.7456 - mlp_auc_178: 0.8208\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5144 - siamese_loss: 4.6068 - mlp_loss: 0.5144 - mlp_binary_accuracy: 0.7518 - mlp_auc_178: 0.8253\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5046 - siamese_loss: 4.7293 - mlp_loss: 0.5046 - mlp_binary_accuracy: 0.7583 - mlp_auc_178: 0.8324\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4980 - siamese_loss: 4.5990 - mlp_loss: 0.4980 - mlp_binary_accuracy: 0.7560 - mlp_auc_178: 0.8372\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5010 - siamese_loss: 4.5061 - mlp_loss: 0.5010 - mlp_binary_accuracy: 0.7626 - mlp_auc_178: 0.8354\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5030 - siamese_loss: 4.5833 - mlp_loss: 0.5030 - mlp_binary_accuracy: 0.7585 - mlp_auc_178: 0.8338\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4966 - siamese_loss: 4.6513 - mlp_loss: 0.4966 - mlp_binary_accuracy: 0.7663 - mlp_auc_178: 0.8389\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4988 - siamese_loss: 4.9845 - mlp_loss: 0.4988 - mlp_binary_accuracy: 0.7645 - mlp_auc_178: 0.8372\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4954 - siamese_loss: 4.8347 - mlp_loss: 0.4954 - mlp_binary_accuracy: 0.7638 - mlp_auc_178: 0.8396\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4944 - siamese_loss: 4.7635 - mlp_loss: 0.4944 - mlp_binary_accuracy: 0.7626 - mlp_auc_178: 0.8404\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4906 - siamese_loss: 4.7683 - mlp_loss: 0.4906 - mlp_binary_accuracy: 0.7691 - mlp_auc_178: 0.8434\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4994 - siamese_loss: 4.7422 - mlp_loss: 0.4994 - mlp_binary_accuracy: 0.7686 - mlp_auc_178: 0.8372\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4841 - siamese_loss: 4.8975 - mlp_loss: 0.4841 - mlp_binary_accuracy: 0.7705 - mlp_auc_178: 0.8480\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4798 - siamese_loss: 4.7464 - mlp_loss: 0.4798 - mlp_binary_accuracy: 0.7750 - mlp_auc_178: 0.8513\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4909 - siamese_loss: 4.7055 - mlp_loss: 0.4909 - mlp_binary_accuracy: 0.7661 - mlp_auc_178: 0.8434\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4762 - siamese_loss: 4.5677 - mlp_loss: 0.4762 - mlp_binary_accuracy: 0.7771 - mlp_auc_178: 0.8525\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4859 - siamese_loss: 4.6512 - mlp_loss: 0.4859 - mlp_binary_accuracy: 0.7702 - mlp_auc_178: 0.8461\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4767 - siamese_loss: 4.6965 - mlp_loss: 0.4767 - mlp_binary_accuracy: 0.7737 - mlp_auc_178: 0.8521\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4851 - siamese_loss: 4.6405 - mlp_loss: 0.4851 - mlp_binary_accuracy: 0.7707 - mlp_auc_178: 0.8471\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4812 - siamese_loss: 4.7749 - mlp_loss: 0.4812 - mlp_binary_accuracy: 0.7741 - mlp_auc_178: 0.8509\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4875 - siamese_loss: 4.7769 - mlp_loss: 0.4875 - mlp_binary_accuracy: 0.7705 - mlp_auc_178: 0.8451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 0s/step - loss: 0.5319 - siamese_loss: 4.4932 - mlp_loss: 0.5319 - mlp_binary_accuracy: 0.7459 - mlp_auc_178: 0.8161\n",
      "------ mlp_binary_accuracy: 74.59%\t ----- mlp_auc_178: 81.61%\n",
      "concat_fea的shape： (None, 64)\n",
      "-------------------------------------Kfold: 3 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6789 - siamese_loss: 3.2782 - mlp_loss: 0.6789 - mlp_binary_accuracy: 0.5636 - mlp_auc_179: 0.5906\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6431 - siamese_loss: 3.6611 - mlp_loss: 0.6431 - mlp_binary_accuracy: 0.6317 - mlp_auc_179: 0.6744\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6172 - siamese_loss: 3.8962 - mlp_loss: 0.6172 - mlp_binary_accuracy: 0.6599 - mlp_auc_179: 0.7162\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5996 - siamese_loss: 4.2007 - mlp_loss: 0.5996 - mlp_binary_accuracy: 0.6808 - mlp_auc_179: 0.7384\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5913 - siamese_loss: 4.2942 - mlp_loss: 0.5913 - mlp_binary_accuracy: 0.6864 - mlp_auc_179: 0.7490\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5744 - siamese_loss: 4.4081 - mlp_loss: 0.5744 - mlp_binary_accuracy: 0.7050 - mlp_auc_179: 0.7685\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5734 - siamese_loss: 4.6276 - mlp_loss: 0.5734 - mlp_binary_accuracy: 0.7045 - mlp_auc_179: 0.7706\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5656 - siamese_loss: 4.6849 - mlp_loss: 0.5656 - mlp_binary_accuracy: 0.7091 - mlp_auc_179: 0.7788\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5574 - siamese_loss: 4.6115 - mlp_loss: 0.5574 - mlp_binary_accuracy: 0.7188 - mlp_auc_179: 0.7866\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5493 - siamese_loss: 4.7167 - mlp_loss: 0.5493 - mlp_binary_accuracy: 0.7266 - mlp_auc_179: 0.7943\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5441 - siamese_loss: 4.5018 - mlp_loss: 0.5441 - mlp_binary_accuracy: 0.7353 - mlp_auc_179: 0.7996\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5376 - siamese_loss: 4.6122 - mlp_loss: 0.5376 - mlp_binary_accuracy: 0.7385 - mlp_auc_179: 0.8066\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5388 - siamese_loss: 4.6957 - mlp_loss: 0.5388 - mlp_binary_accuracy: 0.7344 - mlp_auc_179: 0.8053\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5336 - siamese_loss: 4.7912 - mlp_loss: 0.5336 - mlp_binary_accuracy: 0.7351 - mlp_auc_179: 0.8092\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5295 - siamese_loss: 4.8596 - mlp_loss: 0.5295 - mlp_binary_accuracy: 0.7415 - mlp_auc_179: 0.8131\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5243 - siamese_loss: 5.0748 - mlp_loss: 0.5243 - mlp_binary_accuracy: 0.7420 - mlp_auc_179: 0.8175\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5209 - siamese_loss: 5.1391 - mlp_loss: 0.5209 - mlp_binary_accuracy: 0.7477 - mlp_auc_179: 0.8213\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5220 - siamese_loss: 4.9856 - mlp_loss: 0.5220 - mlp_binary_accuracy: 0.7456 - mlp_auc_179: 0.8192\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5175 - siamese_loss: 4.9463 - mlp_loss: 0.5175 - mlp_binary_accuracy: 0.7498 - mlp_auc_179: 0.8233\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5114 - siamese_loss: 4.7674 - mlp_loss: 0.5114 - mlp_binary_accuracy: 0.7468 - mlp_auc_179: 0.8282\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5085 - siamese_loss: 5.0329 - mlp_loss: 0.5085 - mlp_binary_accuracy: 0.7518 - mlp_auc_179: 0.8297\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5086 - siamese_loss: 5.1479 - mlp_loss: 0.5086 - mlp_binary_accuracy: 0.7537 - mlp_auc_179: 0.8303\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5073 - siamese_loss: 4.7552 - mlp_loss: 0.5073 - mlp_binary_accuracy: 0.7594 - mlp_auc_179: 0.8310\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5016 - siamese_loss: 5.0455 - mlp_loss: 0.5016 - mlp_binary_accuracy: 0.7555 - mlp_auc_179: 0.8352\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5068 - siamese_loss: 5.4723 - mlp_loss: 0.5068 - mlp_binary_accuracy: 0.7548 - mlp_auc_179: 0.8317\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5041 - siamese_loss: 5.5023 - mlp_loss: 0.5041 - mlp_binary_accuracy: 0.7553 - mlp_auc_179: 0.8339\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5003 - siamese_loss: 5.4606 - mlp_loss: 0.5003 - mlp_binary_accuracy: 0.7564 - mlp_auc_179: 0.8354\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4953 - siamese_loss: 5.2795 - mlp_loss: 0.4953 - mlp_binary_accuracy: 0.7686 - mlp_auc_179: 0.8411\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4929 - siamese_loss: 5.2827 - mlp_loss: 0.4929 - mlp_binary_accuracy: 0.7665 - mlp_auc_179: 0.8424\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4943 - siamese_loss: 5.2509 - mlp_loss: 0.4943 - mlp_binary_accuracy: 0.7638 - mlp_auc_179: 0.8406\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4764 - siamese_loss: 5.3742 - mlp_loss: 0.4764 - mlp_binary_accuracy: 0.7755 - mlp_auc_179: 0.8539\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4800 - siamese_loss: 5.2677 - mlp_loss: 0.4800 - mlp_binary_accuracy: 0.7790 - mlp_auc_179: 0.8508\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4844 - siamese_loss: 5.2873 - mlp_loss: 0.4844 - mlp_binary_accuracy: 0.7730 - mlp_auc_179: 0.8470\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4779 - siamese_loss: 5.3806 - mlp_loss: 0.4779 - mlp_binary_accuracy: 0.7764 - mlp_auc_179: 0.8517\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4714 - siamese_loss: 5.4040 - mlp_loss: 0.4714 - mlp_binary_accuracy: 0.7838 - mlp_auc_179: 0.8558\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4765 - siamese_loss: 5.3638 - mlp_loss: 0.4765 - mlp_binary_accuracy: 0.7750 - mlp_auc_179: 0.8525\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4665 - siamese_loss: 5.4048 - mlp_loss: 0.4665 - mlp_binary_accuracy: 0.7835 - mlp_auc_179: 0.8595\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4712 - siamese_loss: 5.3894 - mlp_loss: 0.4712 - mlp_binary_accuracy: 0.7824 - mlp_auc_179: 0.8564\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4655 - siamese_loss: 5.4658 - mlp_loss: 0.4655 - mlp_binary_accuracy: 0.7831 - mlp_auc_179: 0.8607\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4662 - siamese_loss: 5.3856 - mlp_loss: 0.4662 - mlp_binary_accuracy: 0.7812 - mlp_auc_179: 0.8607\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4655 - siamese_loss: 5.4090 - mlp_loss: 0.4655 - mlp_binary_accuracy: 0.7801 - mlp_auc_179: 0.8595\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4688 - siamese_loss: 5.3766 - mlp_loss: 0.4688 - mlp_binary_accuracy: 0.7787 - mlp_auc_179: 0.8586\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4630 - siamese_loss: 5.3453 - mlp_loss: 0.4630 - mlp_binary_accuracy: 0.7881 - mlp_auc_179: 0.8613\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4719 - siamese_loss: 5.3848 - mlp_loss: 0.4719 - mlp_binary_accuracy: 0.7757 - mlp_auc_179: 0.8562\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4603 - siamese_loss: 5.4873 - mlp_loss: 0.4603 - mlp_binary_accuracy: 0.7840 - mlp_auc_179: 0.8644\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4627 - siamese_loss: 5.3142 - mlp_loss: 0.4627 - mlp_binary_accuracy: 0.7865 - mlp_auc_179: 0.8618\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4617 - siamese_loss: 5.7289 - mlp_loss: 0.4617 - mlp_binary_accuracy: 0.7920 - mlp_auc_179: 0.8640\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4594 - siamese_loss: 5.5721 - mlp_loss: 0.4594 - mlp_binary_accuracy: 0.7888 - mlp_auc_179: 0.8659\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4533 - siamese_loss: 5.5461 - mlp_loss: 0.4533 - mlp_binary_accuracy: 0.7987 - mlp_auc_179: 0.8668\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4536 - siamese_loss: 5.5914 - mlp_loss: 0.4536 - mlp_binary_accuracy: 0.7884 - mlp_auc_179: 0.8673\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4538 - siamese_loss: 5.6354 - mlp_loss: 0.4538 - mlp_binary_accuracy: 0.7930 - mlp_auc_179: 0.8677\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4524 - siamese_loss: 5.5101 - mlp_loss: 0.4524 - mlp_binary_accuracy: 0.7881 - mlp_auc_179: 0.8685\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4575 - siamese_loss: 5.5714 - mlp_loss: 0.4575 - mlp_binary_accuracy: 0.7870 - mlp_auc_179: 0.8649\n",
      "Epoch 54/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4516 - siamese_loss: 5.4249 - mlp_loss: 0.4516 - mlp_binary_accuracy: 0.7900 - mlp_auc_179: 0.8700\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4490 - siamese_loss: 5.3849 - mlp_loss: 0.4490 - mlp_binary_accuracy: 0.7943 - mlp_auc_179: 0.8706\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4487 - siamese_loss: 5.5327 - mlp_loss: 0.4487 - mlp_binary_accuracy: 0.7902 - mlp_auc_179: 0.8714\n",
      "Epoch 57/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4502 - siamese_loss: 5.4705 - mlp_loss: 0.4502 - mlp_binary_accuracy: 0.7962 - mlp_auc_179: 0.8701\n",
      "Epoch 58/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4392 - siamese_loss: 5.3887 - mlp_loss: 0.4392 - mlp_binary_accuracy: 0.7983 - mlp_auc_179: 0.8768\n",
      "Epoch 59/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4408 - siamese_loss: 5.6264 - mlp_loss: 0.4408 - mlp_binary_accuracy: 0.7966 - mlp_auc_179: 0.8759\n",
      "Epoch 60/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4380 - siamese_loss: 5.6149 - mlp_loss: 0.4380 - mlp_binary_accuracy: 0.7969 - mlp_auc_179: 0.8780\n",
      "Epoch 61/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4358 - siamese_loss: 5.4890 - mlp_loss: 0.4358 - mlp_binary_accuracy: 0.8024 - mlp_auc_179: 0.8786\n",
      "Epoch 62/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4361 - siamese_loss: 5.3768 - mlp_loss: 0.4361 - mlp_binary_accuracy: 0.7985 - mlp_auc_179: 0.8787\n",
      "Epoch 63/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4479 - siamese_loss: 5.4583 - mlp_loss: 0.4479 - mlp_binary_accuracy: 0.7964 - mlp_auc_179: 0.8724\n",
      "Epoch 64/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4299 - siamese_loss: 5.5267 - mlp_loss: 0.4299 - mlp_binary_accuracy: 0.8093 - mlp_auc_179: 0.8816\n",
      "Epoch 65/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4397 - siamese_loss: 5.4858 - mlp_loss: 0.4397 - mlp_binary_accuracy: 0.8033 - mlp_auc_179: 0.8780\n",
      "Epoch 66/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4381 - siamese_loss: 5.4915 - mlp_loss: 0.4381 - mlp_binary_accuracy: 0.8022 - mlp_auc_179: 0.8781\n",
      "Epoch 67/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4395 - siamese_loss: 5.6091 - mlp_loss: 0.4395 - mlp_binary_accuracy: 0.7983 - mlp_auc_179: 0.8775\n",
      "Epoch 68/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4337 - siamese_loss: 5.6661 - mlp_loss: 0.4337 - mlp_binary_accuracy: 0.8006 - mlp_auc_179: 0.8804\n",
      "Epoch 69/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4329 - siamese_loss: 5.4766 - mlp_loss: 0.4329 - mlp_binary_accuracy: 0.8111 - mlp_auc_179: 0.8809\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5809 - siamese_loss: 5.3046 - mlp_loss: 0.5809 - mlp_binary_accuracy: 0.7293 - mlp_auc_179: 0.7999\n",
      "------ mlp_binary_accuracy: 72.93%\t ----- mlp_auc_179: 79.99%\n",
      "concat_fea的shape： (None, 64)\n",
      "-------------------------------------Kfold: 4 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6873 - siamese_loss: 3.0610 - mlp_loss: 0.6873 - mlp_binary_accuracy: 0.5448 - mlp_auc_180: 0.5592\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6491 - siamese_loss: 3.4194 - mlp_loss: 0.6491 - mlp_binary_accuracy: 0.6183 - mlp_auc_180: 0.6678\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6266 - siamese_loss: 3.5781 - mlp_loss: 0.6266 - mlp_binary_accuracy: 0.6523 - mlp_auc_180: 0.7041\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5993 - siamese_loss: 3.6676 - mlp_loss: 0.5993 - mlp_binary_accuracy: 0.6707 - mlp_auc_180: 0.7388\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5783 - siamese_loss: 3.6688 - mlp_loss: 0.5783 - mlp_binary_accuracy: 0.7022 - mlp_auc_180: 0.7637\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5660 - siamese_loss: 3.8036 - mlp_loss: 0.5660 - mlp_binary_accuracy: 0.7174 - mlp_auc_180: 0.7774\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5615 - siamese_loss: 3.9867 - mlp_loss: 0.5615 - mlp_binary_accuracy: 0.7224 - mlp_auc_180: 0.7830\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5503 - siamese_loss: 3.7702 - mlp_loss: 0.5503 - mlp_binary_accuracy: 0.7220 - mlp_auc_180: 0.7934\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5472 - siamese_loss: 3.8342 - mlp_loss: 0.5472 - mlp_binary_accuracy: 0.7323 - mlp_auc_180: 0.7972\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5415 - siamese_loss: 4.0197 - mlp_loss: 0.5415 - mlp_binary_accuracy: 0.7295 - mlp_auc_180: 0.8021\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5363 - siamese_loss: 4.1530 - mlp_loss: 0.5363 - mlp_binary_accuracy: 0.7417 - mlp_auc_180: 0.8074\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5275 - siamese_loss: 4.0260 - mlp_loss: 0.5275 - mlp_binary_accuracy: 0.7390 - mlp_auc_180: 0.8145\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5263 - siamese_loss: 4.0671 - mlp_loss: 0.5263 - mlp_binary_accuracy: 0.7456 - mlp_auc_180: 0.8153\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5234 - siamese_loss: 3.9238 - mlp_loss: 0.5234 - mlp_binary_accuracy: 0.7429 - mlp_auc_180: 0.8180\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5106 - siamese_loss: 4.1063 - mlp_loss: 0.5106 - mlp_binary_accuracy: 0.7551 - mlp_auc_180: 0.8279\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5146 - siamese_loss: 4.2535 - mlp_loss: 0.5146 - mlp_binary_accuracy: 0.7514 - mlp_auc_180: 0.8251\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5185 - siamese_loss: 4.4401 - mlp_loss: 0.5185 - mlp_binary_accuracy: 0.7507 - mlp_auc_180: 0.8219\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5151 - siamese_loss: 4.3437 - mlp_loss: 0.5151 - mlp_binary_accuracy: 0.7489 - mlp_auc_180: 0.8237\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.5021 - siamese_loss: 4.3765 - mlp_loss: 0.5021 - mlp_binary_accuracy: 0.7553 - mlp_auc_180: 0.834 - 0s 2ms/step - loss: 0.5036 - siamese_loss: 4.4191 - mlp_loss: 0.5036 - mlp_binary_accuracy: 0.7553 - mlp_auc_180: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4970 - siamese_loss: 4.4716 - mlp_loss: 0.4970 - mlp_binary_accuracy: 0.7592 - mlp_auc_180: 0.8372\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4968 - siamese_loss: 4.5611 - mlp_loss: 0.4968 - mlp_binary_accuracy: 0.7562 - mlp_auc_180: 0.8380\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4982 - siamese_loss: 4.7387 - mlp_loss: 0.4982 - mlp_binary_accuracy: 0.7633 - mlp_auc_180: 0.8362\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4887 - siamese_loss: 4.8430 - mlp_loss: 0.4887 - mlp_binary_accuracy: 0.7668 - mlp_auc_180: 0.8435\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4876 - siamese_loss: 4.7489 - mlp_loss: 0.4876 - mlp_binary_accuracy: 0.7705 - mlp_auc_180: 0.8451\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4923 - siamese_loss: 4.7131 - mlp_loss: 0.4923 - mlp_binary_accuracy: 0.7647 - mlp_auc_180: 0.8418\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4882 - siamese_loss: 4.8292 - mlp_loss: 0.4882 - mlp_binary_accuracy: 0.7682 - mlp_auc_180: 0.8446\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4965 - siamese_loss: 4.6959 - mlp_loss: 0.4965 - mlp_binary_accuracy: 0.7661 - mlp_auc_180: 0.8385\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4936 - siamese_loss: 4.6195 - mlp_loss: 0.4936 - mlp_binary_accuracy: 0.7608 - mlp_auc_180: 0.8410\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4834 - siamese_loss: 4.6958 - mlp_loss: 0.4834 - mlp_binary_accuracy: 0.7675 - mlp_auc_180: 0.8468\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4838 - siamese_loss: 4.9254 - mlp_loss: 0.4838 - mlp_binary_accuracy: 0.7661 - mlp_auc_180: 0.8479\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4775 - siamese_loss: 5.0856 - mlp_loss: 0.4775 - mlp_binary_accuracy: 0.7741 - mlp_auc_180: 0.8514\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4802 - siamese_loss: 5.0938 - mlp_loss: 0.4802 - mlp_binary_accuracy: 0.7783 - mlp_auc_180: 0.8495\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4805 - siamese_loss: 5.3636 - mlp_loss: 0.4805 - mlp_binary_accuracy: 0.7739 - mlp_auc_180: 0.8501\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4818 - siamese_loss: 5.1437 - mlp_loss: 0.4818 - mlp_binary_accuracy: 0.7723 - mlp_auc_180: 0.8498\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4838 - siamese_loss: 5.2245 - mlp_loss: 0.4838 - mlp_binary_accuracy: 0.7748 - mlp_auc_180: 0.8485\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4745 - siamese_loss: 5.1940 - mlp_loss: 0.4745 - mlp_binary_accuracy: 0.7773 - mlp_auc_180: 0.8541\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4756 - siamese_loss: 5.0533 - mlp_loss: 0.4756 - mlp_binary_accuracy: 0.7746 - mlp_auc_180: 0.8535\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4650 - siamese_loss: 5.1686 - mlp_loss: 0.4650 - mlp_binary_accuracy: 0.7803 - mlp_auc_180: 0.8609\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4636 - siamese_loss: 5.1975 - mlp_loss: 0.4636 - mlp_binary_accuracy: 0.7831 - mlp_auc_180: 0.8625\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4643 - siamese_loss: 5.0898 - mlp_loss: 0.4643 - mlp_binary_accuracy: 0.7803 - mlp_auc_180: 0.8616\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4524 - siamese_loss: 5.1143 - mlp_loss: 0.4524 - mlp_binary_accuracy: 0.7900 - mlp_auc_180: 0.8684\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4480 - siamese_loss: 5.2047 - mlp_loss: 0.4480 - mlp_binary_accuracy: 0.7914 - mlp_auc_180: 0.8718\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4503 - siamese_loss: 5.2195 - mlp_loss: 0.4503 - mlp_binary_accuracy: 0.7927 - mlp_auc_180: 0.8696\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4502 - siamese_loss: 5.0553 - mlp_loss: 0.4502 - mlp_binary_accuracy: 0.7911 - mlp_auc_180: 0.8708\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4549 - siamese_loss: 5.2584 - mlp_loss: 0.4549 - mlp_binary_accuracy: 0.7863 - mlp_auc_180: 0.8674\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4479 - siamese_loss: 5.1940 - mlp_loss: 0.4479 - mlp_binary_accuracy: 0.7884 - mlp_auc_180: 0.8726\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4517 - siamese_loss: 5.0501 - mlp_loss: 0.4517 - mlp_binary_accuracy: 0.7888 - mlp_auc_180: 0.8701\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4574 - siamese_loss: 5.0986 - mlp_loss: 0.4574 - mlp_binary_accuracy: 0.7817 - mlp_auc_180: 0.8660\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4444 - siamese_loss: 5.1493 - mlp_loss: 0.4444 - mlp_binary_accuracy: 0.7907 - mlp_auc_180: 0.8733\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4397 - siamese_loss: 5.1716 - mlp_loss: 0.4397 - mlp_binary_accuracy: 0.7957 - mlp_auc_180: 0.8763\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4428 - siamese_loss: 5.1747 - mlp_loss: 0.4428 - mlp_binary_accuracy: 0.7976 - mlp_auc_180: 0.8748\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4321 - siamese_loss: 5.1632 - mlp_loss: 0.4321 - mlp_binary_accuracy: 0.7999 - mlp_auc_180: 0.8819\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4458 - siamese_loss: 5.1641 - mlp_loss: 0.4458 - mlp_binary_accuracy: 0.7918 - mlp_auc_180: 0.8733\n",
      "Epoch 54/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4373 - siamese_loss: 5.1002 - mlp_loss: 0.4373 - mlp_binary_accuracy: 0.7964 - mlp_auc_180: 0.8776\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4356 - siamese_loss: 5.2602 - mlp_loss: 0.4356 - mlp_binary_accuracy: 0.7983 - mlp_auc_180: 0.8796\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4302 - siamese_loss: 5.3870 - mlp_loss: 0.4302 - mlp_binary_accuracy: 0.8040 - mlp_auc_180: 0.8818\n",
      "Epoch 57/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4294 - siamese_loss: 5.2037 - mlp_loss: 0.4294 - mlp_binary_accuracy: 0.8070 - mlp_auc_180: 0.8833\n",
      "Epoch 58/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4300 - siamese_loss: 5.0073 - mlp_loss: 0.4300 - mlp_binary_accuracy: 0.8001 - mlp_auc_180: 0.8821\n",
      "Epoch 59/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4407 - siamese_loss: 4.9442 - mlp_loss: 0.4407 - mlp_binary_accuracy: 0.7953 - mlp_auc_180: 0.8764\n",
      "Epoch 60/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4302 - siamese_loss: 5.2075 - mlp_loss: 0.4302 - mlp_binary_accuracy: 0.8028 - mlp_auc_180: 0.8826\n",
      "Epoch 61/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4253 - siamese_loss: 5.1198 - mlp_loss: 0.4253 - mlp_binary_accuracy: 0.8035 - mlp_auc_180: 0.8853\n",
      "Epoch 62/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4239 - siamese_loss: 5.2534 - mlp_loss: 0.4239 - mlp_binary_accuracy: 0.8054 - mlp_auc_180: 0.8856\n",
      "Epoch 63/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4234 - siamese_loss: 5.1313 - mlp_loss: 0.4234 - mlp_binary_accuracy: 0.8093 - mlp_auc_180: 0.8861\n",
      "Epoch 64/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4167 - siamese_loss: 5.3037 - mlp_loss: 0.4167 - mlp_binary_accuracy: 0.8063 - mlp_auc_180: 0.8902\n",
      "Epoch 65/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4282 - siamese_loss: 5.2440 - mlp_loss: 0.4282 - mlp_binary_accuracy: 0.7966 - mlp_auc_180: 0.8832\n",
      "Epoch 66/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4271 - siamese_loss: 5.1994 - mlp_loss: 0.4271 - mlp_binary_accuracy: 0.8047 - mlp_auc_180: 0.8841\n",
      "Epoch 67/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4248 - siamese_loss: 5.1524 - mlp_loss: 0.4248 - mlp_binary_accuracy: 0.8095 - mlp_auc_180: 0.8852\n",
      "Epoch 68/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4301 - siamese_loss: 5.0618 - mlp_loss: 0.4301 - mlp_binary_accuracy: 0.8024 - mlp_auc_180: 0.8824\n",
      "Epoch 69/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4270 - siamese_loss: 5.0413 - mlp_loss: 0.4270 - mlp_binary_accuracy: 0.8079 - mlp_auc_180: 0.8843\n",
      "WARNING:tensorflow:5 out of the last 20 calls to <function Model.make_test_function.<locals>.test_function at 0x0000026123BB4678> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5109 - siamese_loss: 5.1161 - mlp_loss: 0.5109 - mlp_binary_accuracy: 0.7769 - mlp_auc_180: 0.8398\n",
      "------ mlp_binary_accuracy: 77.69%\t ----- mlp_auc_180: 83.98%\n",
      "concat_fea的shape： (None, 64)\n",
      "-------------------------------------Kfold: 5 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6882 - siamese_loss: 3.1832 - mlp_loss: 0.6882 - mlp_binary_accuracy: 0.5519 - mlp_auc_181: 0.5772\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6527 - siamese_loss: 3.6047 - mlp_loss: 0.6527 - mlp_binary_accuracy: 0.6363 - mlp_auc_181: 0.6766\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6139 - siamese_loss: 3.8847 - mlp_loss: 0.6139 - mlp_binary_accuracy: 0.6661 - mlp_auc_181: 0.7260\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5978 - siamese_loss: 3.6347 - mlp_loss: 0.5978 - mlp_binary_accuracy: 0.6753 - mlp_auc_181: 0.7433\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5841 - siamese_loss: 3.6868 - mlp_loss: 0.5841 - mlp_binary_accuracy: 0.6976 - mlp_auc_181: 0.7605\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5759 - siamese_loss: 3.8499 - mlp_loss: 0.5759 - mlp_binary_accuracy: 0.7001 - mlp_auc_181: 0.7684\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5667 - siamese_loss: 3.7824 - mlp_loss: 0.5667 - mlp_binary_accuracy: 0.7089 - mlp_auc_181: 0.7780\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5595 - siamese_loss: 3.8789 - mlp_loss: 0.5595 - mlp_binary_accuracy: 0.7199 - mlp_auc_181: 0.7858\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5463 - siamese_loss: 4.1481 - mlp_loss: 0.5463 - mlp_binary_accuracy: 0.7293 - mlp_auc_181: 0.7990\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5433 - siamese_loss: 4.2844 - mlp_loss: 0.5433 - mlp_binary_accuracy: 0.7270 - mlp_auc_181: 0.8006\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5404 - siamese_loss: 4.2624 - mlp_loss: 0.5404 - mlp_binary_accuracy: 0.7328 - mlp_auc_181: 0.8044\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5311 - siamese_loss: 4.2230 - mlp_loss: 0.5311 - mlp_binary_accuracy: 0.7401 - mlp_auc_181: 0.8121\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5320 - siamese_loss: 4.3420 - mlp_loss: 0.5320 - mlp_binary_accuracy: 0.7394 - mlp_auc_181: 0.8111\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5258 - siamese_loss: 4.3581 - mlp_loss: 0.5258 - mlp_binary_accuracy: 0.7443 - mlp_auc_181: 0.8167\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5204 - siamese_loss: 4.4209 - mlp_loss: 0.5204 - mlp_binary_accuracy: 0.7456 - mlp_auc_181: 0.8209\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5138 - siamese_loss: 4.4196 - mlp_loss: 0.5138 - mlp_binary_accuracy: 0.7438 - mlp_auc_181: 0.8245\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5099 - siamese_loss: 4.4801 - mlp_loss: 0.5099 - mlp_binary_accuracy: 0.7555 - mlp_auc_181: 0.8288\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5070 - siamese_loss: 4.6129 - mlp_loss: 0.5070 - mlp_binary_accuracy: 0.7511 - mlp_auc_181: 0.8305\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5114 - siamese_loss: 4.5916 - mlp_loss: 0.5114 - mlp_binary_accuracy: 0.7518 - mlp_auc_181: 0.8276\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5056 - siamese_loss: 4.6769 - mlp_loss: 0.5056 - mlp_binary_accuracy: 0.7571 - mlp_auc_181: 0.8319\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5019 - siamese_loss: 4.5232 - mlp_loss: 0.5019 - mlp_binary_accuracy: 0.7592 - mlp_auc_181: 0.8353\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5003 - siamese_loss: 4.3709 - mlp_loss: 0.5003 - mlp_binary_accuracy: 0.7546 - mlp_auc_181: 0.8367\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4995 - siamese_loss: 4.3587 - mlp_loss: 0.4995 - mlp_binary_accuracy: 0.7546 - mlp_auc_181: 0.8360\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5027 - siamese_loss: 4.2960 - mlp_loss: 0.5027 - mlp_binary_accuracy: 0.7546 - mlp_auc_181: 0.8346\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5075 - siamese_loss: 4.4794 - mlp_loss: 0.5075 - mlp_binary_accuracy: 0.7489 - mlp_auc_181: 0.8314\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5010 - siamese_loss: 4.6677 - mlp_loss: 0.5010 - mlp_binary_accuracy: 0.7615 - mlp_auc_181: 0.8358\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4979 - siamese_loss: 4.6484 - mlp_loss: 0.4979 - mlp_binary_accuracy: 0.7590 - mlp_auc_181: 0.8384\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4928 - siamese_loss: 4.5295 - mlp_loss: 0.4928 - mlp_binary_accuracy: 0.7629 - mlp_auc_181: 0.8417\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4922 - siamese_loss: 4.4795 - mlp_loss: 0.4922 - mlp_binary_accuracy: 0.7626 - mlp_auc_181: 0.8417\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4987 - siamese_loss: 4.5893 - mlp_loss: 0.4987 - mlp_binary_accuracy: 0.7541 - mlp_auc_181: 0.8371\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4943 - siamese_loss: 4.6775 - mlp_loss: 0.4943 - mlp_binary_accuracy: 0.7626 - mlp_auc_181: 0.8400\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4851 - siamese_loss: 4.9639 - mlp_loss: 0.4851 - mlp_binary_accuracy: 0.7675 - mlp_auc_181: 0.8463\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4871 - siamese_loss: 5.0524 - mlp_loss: 0.4871 - mlp_binary_accuracy: 0.7675 - mlp_auc_181: 0.8456\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4787 - siamese_loss: 4.9664 - mlp_loss: 0.4787 - mlp_binary_accuracy: 0.7707 - mlp_auc_181: 0.8516\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4821 - siamese_loss: 4.9011 - mlp_loss: 0.4821 - mlp_binary_accuracy: 0.7677 - mlp_auc_181: 0.8487\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4765 - siamese_loss: 4.9574 - mlp_loss: 0.4765 - mlp_binary_accuracy: 0.7739 - mlp_auc_181: 0.8526\n",
      "Epoch 37/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4732 - siamese_loss: 5.1307 - mlp_loss: 0.4732 - mlp_binary_accuracy: 0.7677 - mlp_auc_181: 0.8547\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4684 - siamese_loss: 5.2435 - mlp_loss: 0.4684 - mlp_binary_accuracy: 0.7760 - mlp_auc_181: 0.8579\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4695 - siamese_loss: 5.2572 - mlp_loss: 0.4695 - mlp_binary_accuracy: 0.7769 - mlp_auc_181: 0.8565\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4716 - siamese_loss: 5.3998 - mlp_loss: 0.4716 - mlp_binary_accuracy: 0.7711 - mlp_auc_181: 0.8560\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4690 - siamese_loss: 5.4662 - mlp_loss: 0.4690 - mlp_binary_accuracy: 0.7771 - mlp_auc_181: 0.8578\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4659 - siamese_loss: 5.4346 - mlp_loss: 0.4659 - mlp_binary_accuracy: 0.7810 - mlp_auc_181: 0.8594\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4701 - siamese_loss: 5.3926 - mlp_loss: 0.4701 - mlp_binary_accuracy: 0.7695 - mlp_auc_181: 0.8548\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4620 - siamese_loss: 5.2423 - mlp_loss: 0.4620 - mlp_binary_accuracy: 0.7817 - mlp_auc_181: 0.8616\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4726 - siamese_loss: 5.1820 - mlp_loss: 0.4726 - mlp_binary_accuracy: 0.7753 - mlp_auc_181: 0.8544\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4659 - siamese_loss: 5.0509 - mlp_loss: 0.4659 - mlp_binary_accuracy: 0.7725 - mlp_auc_181: 0.8583\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4637 - siamese_loss: 5.1541 - mlp_loss: 0.4637 - mlp_binary_accuracy: 0.7762 - mlp_auc_181: 0.8596\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4698 - siamese_loss: 4.8745 - mlp_loss: 0.4698 - mlp_binary_accuracy: 0.7810 - mlp_auc_181: 0.8571\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4666 - siamese_loss: 4.7234 - mlp_loss: 0.4666 - mlp_binary_accuracy: 0.7796 - mlp_auc_181: 0.8585\n",
      "WARNING:tensorflow:6 out of the last 21 calls to <function Model.make_test_function.<locals>.test_function at 0x0000026154680708> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5561 - siamese_loss: 4.3722 - mlp_loss: 0.5561 - mlp_binary_accuracy: 0.7335 - mlp_auc_181: 0.8039\n",
      "------ mlp_binary_accuracy: 73.35%\t ----- mlp_auc_181: 80.39%\n",
      "concat_fea的shape： (None, 64)\n",
      "-------------------------------------Kfold: 6 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6834 - siamese_loss: 3.1468 - mlp_loss: 0.6834 - mlp_binary_accuracy: 0.5607 - mlp_auc_182: 0.5796\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6443 - siamese_loss: 3.3994 - mlp_loss: 0.6443 - mlp_binary_accuracy: 0.6330 - mlp_auc_182: 0.6773\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6120 - siamese_loss: 3.4230 - mlp_loss: 0.6120 - mlp_binary_accuracy: 0.6742 - mlp_auc_182: 0.7236\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5925 - siamese_loss: 3.8264 - mlp_loss: 0.5925 - mlp_binary_accuracy: 0.6896 - mlp_auc_182: 0.7491\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5788 - siamese_loss: 4.0573 - mlp_loss: 0.5788 - mlp_binary_accuracy: 0.6981 - mlp_auc_182: 0.7647\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5669 - siamese_loss: 4.1390 - mlp_loss: 0.5669 - mlp_binary_accuracy: 0.7075 - mlp_auc_182: 0.7770\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5612 - siamese_loss: 4.2672 - mlp_loss: 0.5612 - mlp_binary_accuracy: 0.7148 - mlp_auc_182: 0.7831\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5476 - siamese_loss: 4.4564 - mlp_loss: 0.5476 - mlp_binary_accuracy: 0.7252 - mlp_auc_182: 0.7964\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5430 - siamese_loss: 4.5634 - mlp_loss: 0.5430 - mlp_binary_accuracy: 0.7220 - mlp_auc_182: 0.8006\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5434 - siamese_loss: 4.8212 - mlp_loss: 0.5434 - mlp_binary_accuracy: 0.7268 - mlp_auc_182: 0.8002\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5329 - siamese_loss: 4.7606 - mlp_loss: 0.5329 - mlp_binary_accuracy: 0.7353 - mlp_auc_182: 0.8099\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5248 - siamese_loss: 4.9851 - mlp_loss: 0.5248 - mlp_binary_accuracy: 0.7406 - mlp_auc_182: 0.8164\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5203 - siamese_loss: 5.1023 - mlp_loss: 0.5203 - mlp_binary_accuracy: 0.7514 - mlp_auc_182: 0.8212\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5158 - siamese_loss: 5.3035 - mlp_loss: 0.5158 - mlp_binary_accuracy: 0.7466 - mlp_auc_182: 0.8240\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5113 - siamese_loss: 5.1823 - mlp_loss: 0.5113 - mlp_binary_accuracy: 0.7523 - mlp_auc_182: 0.8272\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5065 - siamese_loss: 5.0089 - mlp_loss: 0.5065 - mlp_binary_accuracy: 0.7541 - mlp_auc_182: 0.8310\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5041 - siamese_loss: 4.9606 - mlp_loss: 0.5041 - mlp_binary_accuracy: 0.7617 - mlp_auc_182: 0.8343\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4974 - siamese_loss: 5.0171 - mlp_loss: 0.4974 - mlp_binary_accuracy: 0.7583 - mlp_auc_182: 0.8377\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4959 - siamese_loss: 5.2684 - mlp_loss: 0.4959 - mlp_binary_accuracy: 0.7677 - mlp_auc_182: 0.8404\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4957 - siamese_loss: 5.3458 - mlp_loss: 0.4957 - mlp_binary_accuracy: 0.7636 - mlp_auc_182: 0.8396\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4835 - siamese_loss: 5.2428 - mlp_loss: 0.4835 - mlp_binary_accuracy: 0.7732 - mlp_auc_182: 0.8484\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4847 - siamese_loss: 5.3591 - mlp_loss: 0.4847 - mlp_binary_accuracy: 0.7682 - mlp_auc_182: 0.8474\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4832 - siamese_loss: 5.5862 - mlp_loss: 0.4832 - mlp_binary_accuracy: 0.7686 - mlp_auc_182: 0.8480\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4820 - siamese_loss: 5.4154 - mlp_loss: 0.4820 - mlp_binary_accuracy: 0.7764 - mlp_auc_182: 0.8494\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4753 - siamese_loss: 5.3788 - mlp_loss: 0.4753 - mlp_binary_accuracy: 0.7773 - mlp_auc_182: 0.8532\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4822 - siamese_loss: 5.6390 - mlp_loss: 0.4822 - mlp_binary_accuracy: 0.7714 - mlp_auc_182: 0.8494\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4784 - siamese_loss: 5.7791 - mlp_loss: 0.4784 - mlp_binary_accuracy: 0.7796 - mlp_auc_182: 0.8524\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4822 - siamese_loss: 5.5527 - mlp_loss: 0.4822 - mlp_binary_accuracy: 0.7714 - mlp_auc_182: 0.8488\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4771 - siamese_loss: 5.5808 - mlp_loss: 0.4771 - mlp_binary_accuracy: 0.7778 - mlp_auc_182: 0.8527\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4708 - siamese_loss: 5.4816 - mlp_loss: 0.4708 - mlp_binary_accuracy: 0.7835 - mlp_auc_182: 0.8574\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4724 - siamese_loss: 5.5091 - mlp_loss: 0.4724 - mlp_binary_accuracy: 0.7808 - mlp_auc_182: 0.8547\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4825 - siamese_loss: 5.7110 - mlp_loss: 0.4825 - mlp_binary_accuracy: 0.7746 - mlp_auc_182: 0.8490\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4735 - siamese_loss: 5.5143 - mlp_loss: 0.4735 - mlp_binary_accuracy: 0.7757 - mlp_auc_182: 0.8547\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4737 - siamese_loss: 5.5360 - mlp_loss: 0.4737 - mlp_binary_accuracy: 0.7826 - mlp_auc_182: 0.8546\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4660 - siamese_loss: 5.6569 - mlp_loss: 0.4660 - mlp_binary_accuracy: 0.7783 - mlp_auc_182: 0.8590\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4672 - siamese_loss: 5.8631 - mlp_loss: 0.4672 - mlp_binary_accuracy: 0.7780 - mlp_auc_182: 0.8591\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4607 - siamese_loss: 5.6572 - mlp_loss: 0.4607 - mlp_binary_accuracy: 0.7861 - mlp_auc_182: 0.8643\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4643 - siamese_loss: 5.4560 - mlp_loss: 0.4643 - mlp_binary_accuracy: 0.7865 - mlp_auc_182: 0.8616\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4614 - siamese_loss: 5.4002 - mlp_loss: 0.4614 - mlp_binary_accuracy: 0.7852 - mlp_auc_182: 0.8629\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4613 - siamese_loss: 5.3445 - mlp_loss: 0.4613 - mlp_binary_accuracy: 0.7854 - mlp_auc_182: 0.8633\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4531 - siamese_loss: 5.3954 - mlp_loss: 0.4531 - mlp_binary_accuracy: 0.7909 - mlp_auc_182: 0.8675\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4536 - siamese_loss: 5.3273 - mlp_loss: 0.4536 - mlp_binary_accuracy: 0.7904 - mlp_auc_182: 0.8673\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4568 - siamese_loss: 5.2016 - mlp_loss: 0.4568 - mlp_binary_accuracy: 0.7895 - mlp_auc_182: 0.8658\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4516 - siamese_loss: 5.4178 - mlp_loss: 0.4516 - mlp_binary_accuracy: 0.7904 - mlp_auc_182: 0.8691\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4614 - siamese_loss: 5.2817 - mlp_loss: 0.4614 - mlp_binary_accuracy: 0.7886 - mlp_auc_182: 0.8619\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4557 - siamese_loss: 5.3469 - mlp_loss: 0.4557 - mlp_binary_accuracy: 0.7902 - mlp_auc_182: 0.8664\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4511 - siamese_loss: 5.4702 - mlp_loss: 0.4511 - mlp_binary_accuracy: 0.7925 - mlp_auc_182: 0.8684\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4459 - siamese_loss: 5.3090 - mlp_loss: 0.4459 - mlp_binary_accuracy: 0.7966 - mlp_auc_182: 0.8725\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4434 - siamese_loss: 5.2582 - mlp_loss: 0.4434 - mlp_binary_accuracy: 0.7939 - mlp_auc_182: 0.8736\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4542 - siamese_loss: 5.0378 - mlp_loss: 0.4542 - mlp_binary_accuracy: 0.7916 - mlp_auc_182: 0.8674\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4496 - siamese_loss: 5.2428 - mlp_loss: 0.4496 - mlp_binary_accuracy: 0.7916 - mlp_auc_182: 0.8701\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4460 - siamese_loss: 5.2672 - mlp_loss: 0.4460 - mlp_binary_accuracy: 0.7907 - mlp_auc_182: 0.8718\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4431 - siamese_loss: 5.2583 - mlp_loss: 0.4431 - mlp_binary_accuracy: 0.7978 - mlp_auc_182: 0.8741\n",
      "Epoch 54/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4437 - siamese_loss: 5.2488 - mlp_loss: 0.4437 - mlp_binary_accuracy: 0.7943 - mlp_auc_182: 0.8734\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4539 - siamese_loss: 5.1646 - mlp_loss: 0.4539 - mlp_binary_accuracy: 0.7920 - mlp_auc_182: 0.8682\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4468 - siamese_loss: 5.1263 - mlp_loss: 0.4468 - mlp_binary_accuracy: 0.7983 - mlp_auc_182: 0.8724\n",
      "Epoch 57/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4448 - siamese_loss: 5.1163 - mlp_loss: 0.4448 - mlp_binary_accuracy: 0.7930 - mlp_auc_182: 0.8734\n",
      "Epoch 58/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4429 - siamese_loss: 5.2905 - mlp_loss: 0.4429 - mlp_binary_accuracy: 0.7950 - mlp_auc_182: 0.8755\n",
      "Epoch 59/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4449 - siamese_loss: 5.1827 - mlp_loss: 0.4449 - mlp_binary_accuracy: 0.7946 - mlp_auc_182: 0.8735\n",
      "Epoch 60/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4435 - siamese_loss: 5.0473 - mlp_loss: 0.4435 - mlp_binary_accuracy: 0.7960 - mlp_auc_182: 0.8737\n",
      "Epoch 61/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4381 - siamese_loss: 5.2631 - mlp_loss: 0.4381 - mlp_binary_accuracy: 0.8035 - mlp_auc_182: 0.8771\n",
      "Epoch 62/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4387 - siamese_loss: 5.1965 - mlp_loss: 0.4387 - mlp_binary_accuracy: 0.7966 - mlp_auc_182: 0.8772\n",
      "Epoch 63/200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.4304 - siamese_loss: 5.4338 - mlp_loss: 0.4304 - mlp_binary_accuracy: 0.8019 - mlp_auc_182: 0.8817\n",
      "Epoch 64/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4247 - siamese_loss: 5.4512 - mlp_loss: 0.4247 - mlp_binary_accuracy: 0.8033 - mlp_auc_182: 0.8848\n",
      "Epoch 65/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4346 - siamese_loss: 5.5366 - mlp_loss: 0.4346 - mlp_binary_accuracy: 0.8017 - mlp_auc_182: 0.8793\n",
      "Epoch 66/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4379 - siamese_loss: 5.4528 - mlp_loss: 0.4379 - mlp_binary_accuracy: 0.7939 - mlp_auc_182: 0.8767\n",
      "Epoch 67/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4317 - siamese_loss: 5.3975 - mlp_loss: 0.4317 - mlp_binary_accuracy: 0.8033 - mlp_auc_182: 0.8804\n",
      "Epoch 68/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4278 - siamese_loss: 5.5664 - mlp_loss: 0.4278 - mlp_binary_accuracy: 0.8045 - mlp_auc_182: 0.8830\n",
      "Epoch 69/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4271 - siamese_loss: 5.4126 - mlp_loss: 0.4271 - mlp_binary_accuracy: 0.8086 - mlp_auc_182: 0.8839\n",
      "WARNING:tensorflow:7 out of the last 22 calls to <function Model.make_test_function.<locals>.test_function at 0x000002613E5135E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 999us/step - loss: 0.5394 - siamese_loss: 5.8680 - mlp_loss: 0.5394 - mlp_binary_accuracy: 0.7293 - mlp_auc_182: 0.8184\n",
      "------ mlp_binary_accuracy: 72.93%\t ----- mlp_auc_182: 81.84%\n",
      "concat_fea的shape： (None, 64)\n",
      "-------------------------------------Kfold: 7 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6828 - siamese_loss: 3.4861 - mlp_loss: 0.6828 - mlp_binary_accuracy: 0.5608 - mlp_auc_183: 0.5906\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6371 - siamese_loss: 3.9304 - mlp_loss: 0.6371 - mlp_binary_accuracy: 0.6414 - mlp_auc_183: 0.6915\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6094 - siamese_loss: 4.0108 - mlp_loss: 0.6094 - mlp_binary_accuracy: 0.6678 - mlp_auc_183: 0.7279\n",
      "Epoch 4/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5881 - siamese_loss: 3.9900 - mlp_loss: 0.5881 - mlp_binary_accuracy: 0.6963 - mlp_auc_183: 0.7521\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.5766 - siamese_loss: 4.0093 - mlp_loss: 0.5766 - mlp_binary_accuracy: 0.7034 - mlp_auc_183: 0.7654\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.5680 - siamese_loss: 4.0610 - mlp_loss: 0.5680 - mlp_binary_accuracy: 0.7110 - mlp_auc_183: 0.7759\n",
      "Epoch 7/200\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.5620 - siamese_loss: 4.1066 - mlp_loss: 0.5620 - mlp_binary_accuracy: 0.7147 - mlp_auc_183: 0.7816\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5535 - siamese_loss: 4.3682 - mlp_loss: 0.5535 - mlp_binary_accuracy: 0.7207 - mlp_auc_183: 0.7907\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5456 - siamese_loss: 4.4750 - mlp_loss: 0.5456 - mlp_binary_accuracy: 0.7319 - mlp_auc_183: 0.7968\n",
      "Epoch 10/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5506 - siamese_loss: 4.5764 - mlp_loss: 0.5506 - mlp_binary_accuracy: 0.7229 - mlp_auc_183: 0.7919\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5446 - siamese_loss: 4.6231 - mlp_loss: 0.5446 - mlp_binary_accuracy: 0.7328 - mlp_auc_183: 0.7976\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5391 - siamese_loss: 4.5290 - mlp_loss: 0.5391 - mlp_binary_accuracy: 0.7360 - mlp_auc_183: 0.8034\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5276 - siamese_loss: 4.5940 - mlp_loss: 0.5276 - mlp_binary_accuracy: 0.7503 - mlp_auc_183: 0.8135\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5258 - siamese_loss: 4.6186 - mlp_loss: 0.5258 - mlp_binary_accuracy: 0.7420 - mlp_auc_183: 0.8153\n",
      "Epoch 15/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5254 - siamese_loss: 4.8376 - mlp_loss: 0.5254 - mlp_binary_accuracy: 0.7450 - mlp_auc_183: 0.8159\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.5180 - siamese_loss: 4.8103 - mlp_loss: 0.5180 - mlp_binary_accuracy: 0.7546 - mlp_auc_183: 0.821 - 0s 2ms/step - loss: 0.5175 - siamese_loss: 4.8370 - mlp_loss: 0.5175 - mlp_binary_accuracy: 0.7537 - mlp_auc_183: 0.8221\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5163 - siamese_loss: 4.8492 - mlp_loss: 0.5163 - mlp_binary_accuracy: 0.7482 - mlp_auc_183: 0.8232\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5114 - siamese_loss: 4.9285 - mlp_loss: 0.5114 - mlp_binary_accuracy: 0.7551 - mlp_auc_183: 0.8276\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.5023 - siamese_loss: 4.8254 - mlp_loss: 0.5023 - mlp_binary_accuracy: 0.7558 - mlp_auc_183: 0.834 - 0s 2ms/step - loss: 0.5039 - siamese_loss: 4.8420 - mlp_loss: 0.5039 - mlp_binary_accuracy: 0.7551 - mlp_auc_183: 0.8335\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5056 - siamese_loss: 4.9409 - mlp_loss: 0.5056 - mlp_binary_accuracy: 0.7583 - mlp_auc_183: 0.8325\n",
      "Epoch 21/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5041 - siamese_loss: 4.8124 - mlp_loss: 0.5041 - mlp_binary_accuracy: 0.7618 - mlp_auc_183: 0.8339\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4990 - siamese_loss: 4.8806 - mlp_loss: 0.4990 - mlp_binary_accuracy: 0.7650 - mlp_auc_183: 0.8376\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4950 - siamese_loss: 4.8544 - mlp_loss: 0.4950 - mlp_binary_accuracy: 0.7641 - mlp_auc_183: 0.8405\n",
      "Epoch 24/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4921 - siamese_loss: 4.8426 - mlp_loss: 0.4921 - mlp_binary_accuracy: 0.7622 - mlp_auc_183: 0.8427\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4906 - siamese_loss: 4.8132 - mlp_loss: 0.4906 - mlp_binary_accuracy: 0.7673 - mlp_auc_183: 0.8439\n",
      "Epoch 26/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4915 - siamese_loss: 4.6675 - mlp_loss: 0.4915 - mlp_binary_accuracy: 0.7643 - mlp_auc_183: 0.8428\n",
      "Epoch 27/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4869 - siamese_loss: 4.8794 - mlp_loss: 0.4869 - mlp_binary_accuracy: 0.7661 - mlp_auc_183: 0.8466\n",
      "Epoch 28/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4920 - siamese_loss: 4.9222 - mlp_loss: 0.4920 - mlp_binary_accuracy: 0.7611 - mlp_auc_183: 0.8424\n",
      "Epoch 29/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4823 - siamese_loss: 4.9570 - mlp_loss: 0.4823 - mlp_binary_accuracy: 0.7671 - mlp_auc_183: 0.8495\n",
      "Epoch 30/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4811 - siamese_loss: 4.6427 - mlp_loss: 0.4811 - mlp_binary_accuracy: 0.7671 - mlp_auc_183: 0.8495\n",
      "Epoch 31/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4837 - siamese_loss: 4.7579 - mlp_loss: 0.4837 - mlp_binary_accuracy: 0.7648 - mlp_auc_183: 0.8484\n",
      "Epoch 32/200\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.4775 - siamese_loss: 4.9267 - mlp_loss: 0.4775 - mlp_binary_accuracy: 0.7714 - mlp_auc_183: 0.8514\n",
      "Epoch 33/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4798 - siamese_loss: 4.8712 - mlp_loss: 0.4798 - mlp_binary_accuracy: 0.7682 - mlp_auc_183: 0.8501\n",
      "Epoch 34/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4718 - siamese_loss: 4.9031 - mlp_loss: 0.4718 - mlp_binary_accuracy: 0.7730 - mlp_auc_183: 0.8557\n",
      "Epoch 35/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4750 - siamese_loss: 4.8281 - mlp_loss: 0.4750 - mlp_binary_accuracy: 0.7687 - mlp_auc_183: 0.8535\n",
      "Epoch 36/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4685 - siamese_loss: 4.9519 - mlp_loss: 0.4685 - mlp_binary_accuracy: 0.7728 - mlp_auc_183: 0.8583\n",
      "Epoch 37/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4716 - siamese_loss: 4.8591 - mlp_loss: 0.4716 - mlp_binary_accuracy: 0.7703 - mlp_auc_183: 0.8554\n",
      "Epoch 38/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4721 - siamese_loss: 4.7064 - mlp_loss: 0.4721 - mlp_binary_accuracy: 0.7726 - mlp_auc_183: 0.8561\n",
      "Epoch 39/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4665 - siamese_loss: 4.8040 - mlp_loss: 0.4665 - mlp_binary_accuracy: 0.7682 - mlp_auc_183: 0.8598\n",
      "Epoch 40/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4689 - siamese_loss: 4.9416 - mlp_loss: 0.4689 - mlp_binary_accuracy: 0.7714 - mlp_auc_183: 0.8584\n",
      "Epoch 41/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4687 - siamese_loss: 5.0433 - mlp_loss: 0.4687 - mlp_binary_accuracy: 0.7744 - mlp_auc_183: 0.8580\n",
      "Epoch 42/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4678 - siamese_loss: 4.8832 - mlp_loss: 0.4678 - mlp_binary_accuracy: 0.7696 - mlp_auc_183: 0.8586\n",
      "Epoch 43/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4671 - siamese_loss: 4.8603 - mlp_loss: 0.4671 - mlp_binary_accuracy: 0.7726 - mlp_auc_183: 0.8595\n",
      "Epoch 44/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4597 - siamese_loss: 5.0177 - mlp_loss: 0.4597 - mlp_binary_accuracy: 0.7804 - mlp_auc_183: 0.8641\n",
      "Epoch 45/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4670 - siamese_loss: 4.9704 - mlp_loss: 0.4670 - mlp_binary_accuracy: 0.7783 - mlp_auc_183: 0.8599\n",
      "Epoch 46/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4592 - siamese_loss: 4.9206 - mlp_loss: 0.4592 - mlp_binary_accuracy: 0.7836 - mlp_auc_183: 0.8649\n",
      "Epoch 47/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4587 - siamese_loss: 4.8401 - mlp_loss: 0.4587 - mlp_binary_accuracy: 0.7785 - mlp_auc_183: 0.8651\n",
      "Epoch 48/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4584 - siamese_loss: 4.9888 - mlp_loss: 0.4584 - mlp_binary_accuracy: 0.7797 - mlp_auc_183: 0.8654\n",
      "Epoch 49/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4539 - siamese_loss: 4.8271 - mlp_loss: 0.4539 - mlp_binary_accuracy: 0.7893 - mlp_auc_183: 0.8692\n",
      "Epoch 50/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4498 - siamese_loss: 4.8679 - mlp_loss: 0.4498 - mlp_binary_accuracy: 0.7788 - mlp_auc_183: 0.8698\n",
      "Epoch 51/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4568 - siamese_loss: 5.2940 - mlp_loss: 0.4568 - mlp_binary_accuracy: 0.7877 - mlp_auc_183: 0.8675\n",
      "Epoch 52/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4538 - siamese_loss: 5.2226 - mlp_loss: 0.4538 - mlp_binary_accuracy: 0.7889 - mlp_auc_183: 0.8687\n",
      "Epoch 53/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4483 - siamese_loss: 5.0925 - mlp_loss: 0.4483 - mlp_binary_accuracy: 0.7912 - mlp_auc_183: 0.8727\n",
      "Epoch 54/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4514 - siamese_loss: 4.9734 - mlp_loss: 0.4514 - mlp_binary_accuracy: 0.7838 - mlp_auc_183: 0.8694\n",
      "Epoch 55/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4434 - siamese_loss: 5.0208 - mlp_loss: 0.4434 - mlp_binary_accuracy: 0.7967 - mlp_auc_183: 0.8753\n",
      "Epoch 56/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4535 - siamese_loss: 5.1195 - mlp_loss: 0.4535 - mlp_binary_accuracy: 0.7824 - mlp_auc_183: 0.8687\n",
      "Epoch 57/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4474 - siamese_loss: 4.9154 - mlp_loss: 0.4474 - mlp_binary_accuracy: 0.7914 - mlp_auc_183: 0.8726\n",
      "Epoch 58/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4493 - siamese_loss: 4.7896 - mlp_loss: 0.4493 - mlp_binary_accuracy: 0.7824 - mlp_auc_183: 0.8709\n",
      "Epoch 59/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4504 - siamese_loss: 4.9614 - mlp_loss: 0.4504 - mlp_binary_accuracy: 0.7905 - mlp_auc_183: 0.8707\n",
      "Epoch 60/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4436 - siamese_loss: 4.9570 - mlp_loss: 0.4436 - mlp_binary_accuracy: 0.7985 - mlp_auc_183: 0.8758\n",
      "WARNING:tensorflow:8 out of the last 23 calls to <function Model.make_test_function.<locals>.test_function at 0x000002613B431F78> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5799 - siamese_loss: 4.8067 - mlp_loss: 0.5799 - mlp_binary_accuracy: 0.7205 - mlp_auc_183: 0.8061\n",
      "------ mlp_binary_accuracy: 72.05%\t ----- mlp_auc_183: 80.61%\n",
      "concat_fea的shape： (None, 64)\n",
      "-------------------------------------Kfold: 8 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6801 - siamese_loss: 2.7179 - mlp_loss: 0.6801 - mlp_binary_accuracy: 0.5729 - mlp_auc_184: 0.6022\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6532 - siamese_loss: 2.9130 - mlp_loss: 0.6532 - mlp_binary_accuracy: 0.6235 - mlp_auc_184: 0.6620\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6263 - siamese_loss: 3.1861 - mlp_loss: 0.6263 - mlp_binary_accuracy: 0.6641 - mlp_auc_184: 0.7100\n",
      "Epoch 4/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6108 - siamese_loss: 3.0897 - mlp_loss: 0.6108 - mlp_binary_accuracy: 0.6692 - mlp_auc_184: 0.7301\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6041 - siamese_loss: 3.3566 - mlp_loss: 0.6041 - mlp_binary_accuracy: 0.6759 - mlp_auc_184: 0.7379\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5791 - siamese_loss: 3.6694 - mlp_loss: 0.5791 - mlp_binary_accuracy: 0.6972 - mlp_auc_184: 0.7657\n",
      "Epoch 7/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5711 - siamese_loss: 3.5539 - mlp_loss: 0.5711 - mlp_binary_accuracy: 0.7043 - mlp_auc_184: 0.7729\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5601 - siamese_loss: 3.9229 - mlp_loss: 0.5601 - mlp_binary_accuracy: 0.7135 - mlp_auc_184: 0.7839\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5475 - siamese_loss: 4.2907 - mlp_loss: 0.5475 - mlp_binary_accuracy: 0.7294 - mlp_auc_184: 0.7972\n",
      "Epoch 10/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5490 - siamese_loss: 4.3240 - mlp_loss: 0.5490 - mlp_binary_accuracy: 0.7266 - mlp_auc_184: 0.7949\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5358 - siamese_loss: 4.4464 - mlp_loss: 0.5358 - mlp_binary_accuracy: 0.7372 - mlp_auc_184: 0.8078\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5315 - siamese_loss: 4.3526 - mlp_loss: 0.5315 - mlp_binary_accuracy: 0.7427 - mlp_auc_184: 0.8114\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5276 - siamese_loss: 4.2400 - mlp_loss: 0.5276 - mlp_binary_accuracy: 0.7395 - mlp_auc_184: 0.8150\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5212 - siamese_loss: 4.2429 - mlp_loss: 0.5212 - mlp_binary_accuracy: 0.7489 - mlp_auc_184: 0.8197\n",
      "Epoch 15/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5202 - siamese_loss: 4.2347 - mlp_loss: 0.5202 - mlp_binary_accuracy: 0.7416 - mlp_auc_184: 0.8205\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5182 - siamese_loss: 4.2878 - mlp_loss: 0.5182 - mlp_binary_accuracy: 0.7494 - mlp_auc_184: 0.8221\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5094 - siamese_loss: 4.7023 - mlp_loss: 0.5094 - mlp_binary_accuracy: 0.7558 - mlp_auc_184: 0.8290\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5123 - siamese_loss: 4.8139 - mlp_loss: 0.5123 - mlp_binary_accuracy: 0.7560 - mlp_auc_184: 0.8267\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5092 - siamese_loss: 4.8738 - mlp_loss: 0.5092 - mlp_binary_accuracy: 0.7592 - mlp_auc_184: 0.8296\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5034 - siamese_loss: 4.9652 - mlp_loss: 0.5034 - mlp_binary_accuracy: 0.7650 - mlp_auc_184: 0.8333\n",
      "Epoch 21/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5032 - siamese_loss: 5.0328 - mlp_loss: 0.5032 - mlp_binary_accuracy: 0.7632 - mlp_auc_184: 0.8342\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4961 - siamese_loss: 5.0878 - mlp_loss: 0.4961 - mlp_binary_accuracy: 0.7654 - mlp_auc_184: 0.8398\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5051 - siamese_loss: 4.8150 - mlp_loss: 0.5051 - mlp_binary_accuracy: 0.7569 - mlp_auc_184: 0.8320\n",
      "Epoch 24/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4988 - siamese_loss: 4.8068 - mlp_loss: 0.4988 - mlp_binary_accuracy: 0.7588 - mlp_auc_184: 0.8363\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4887 - siamese_loss: 4.9522 - mlp_loss: 0.4887 - mlp_binary_accuracy: 0.7689 - mlp_auc_184: 0.8440\n",
      "Epoch 26/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4912 - siamese_loss: 4.8633 - mlp_loss: 0.4912 - mlp_binary_accuracy: 0.7705 - mlp_auc_184: 0.8421\n",
      "Epoch 27/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4903 - siamese_loss: 5.1846 - mlp_loss: 0.4903 - mlp_binary_accuracy: 0.7673 - mlp_auc_184: 0.8426\n",
      "Epoch 28/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4785 - siamese_loss: 5.0914 - mlp_loss: 0.4785 - mlp_binary_accuracy: 0.7735 - mlp_auc_184: 0.8514\n",
      "Epoch 29/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4854 - siamese_loss: 5.2412 - mlp_loss: 0.4854 - mlp_binary_accuracy: 0.7760 - mlp_auc_184: 0.8465\n",
      "Epoch 30/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4781 - siamese_loss: 5.2030 - mlp_loss: 0.4781 - mlp_binary_accuracy: 0.7749 - mlp_auc_184: 0.8514\n",
      "Epoch 31/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4828 - siamese_loss: 5.3131 - mlp_loss: 0.4828 - mlp_binary_accuracy: 0.7712 - mlp_auc_184: 0.8481\n",
      "Epoch 32/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4758 - siamese_loss: 5.3293 - mlp_loss: 0.4758 - mlp_binary_accuracy: 0.7765 - mlp_auc_184: 0.8536\n",
      "Epoch 33/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4753 - siamese_loss: 5.4725 - mlp_loss: 0.4753 - mlp_binary_accuracy: 0.7804 - mlp_auc_184: 0.8531\n",
      "Epoch 34/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4842 - siamese_loss: 5.4810 - mlp_loss: 0.4842 - mlp_binary_accuracy: 0.7668 - mlp_auc_184: 0.8469\n",
      "Epoch 35/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4764 - siamese_loss: 5.4444 - mlp_loss: 0.4764 - mlp_binary_accuracy: 0.7744 - mlp_auc_184: 0.8531\n",
      "Epoch 36/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4710 - siamese_loss: 5.5053 - mlp_loss: 0.4710 - mlp_binary_accuracy: 0.7802 - mlp_auc_184: 0.8566\n",
      "Epoch 37/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4736 - siamese_loss: 5.4011 - mlp_loss: 0.4736 - mlp_binary_accuracy: 0.7758 - mlp_auc_184: 0.8540\n",
      "Epoch 38/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4701 - siamese_loss: 5.4581 - mlp_loss: 0.4701 - mlp_binary_accuracy: 0.7779 - mlp_auc_184: 0.8563\n",
      "Epoch 39/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4769 - siamese_loss: 5.2564 - mlp_loss: 0.4769 - mlp_binary_accuracy: 0.7728 - mlp_auc_184: 0.8518\n",
      "Epoch 40/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4635 - siamese_loss: 5.1912 - mlp_loss: 0.4635 - mlp_binary_accuracy: 0.7802 - mlp_auc_184: 0.8605\n",
      "Epoch 41/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4622 - siamese_loss: 5.3216 - mlp_loss: 0.4622 - mlp_binary_accuracy: 0.7808 - mlp_auc_184: 0.8618\n",
      "Epoch 42/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4665 - siamese_loss: 5.1731 - mlp_loss: 0.4665 - mlp_binary_accuracy: 0.7765 - mlp_auc_184: 0.8584\n",
      "Epoch 43/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4607 - siamese_loss: 5.2094 - mlp_loss: 0.4607 - mlp_binary_accuracy: 0.7864 - mlp_auc_184: 0.8636\n",
      "Epoch 44/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4566 - siamese_loss: 5.2890 - mlp_loss: 0.4566 - mlp_binary_accuracy: 0.7822 - mlp_auc_184: 0.8645\n",
      "Epoch 45/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4656 - siamese_loss: 5.1511 - mlp_loss: 0.4656 - mlp_binary_accuracy: 0.7831 - mlp_auc_184: 0.8598\n",
      "Epoch 46/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4643 - siamese_loss: 5.2722 - mlp_loss: 0.4643 - mlp_binary_accuracy: 0.7829 - mlp_auc_184: 0.8607\n",
      "Epoch 47/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4579 - siamese_loss: 5.3570 - mlp_loss: 0.4579 - mlp_binary_accuracy: 0.7870 - mlp_auc_184: 0.8643\n",
      "Epoch 48/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4551 - siamese_loss: 5.3244 - mlp_loss: 0.4551 - mlp_binary_accuracy: 0.7907 - mlp_auc_184: 0.8652\n",
      "Epoch 49/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4586 - siamese_loss: 5.4441 - mlp_loss: 0.4586 - mlp_binary_accuracy: 0.7873 - mlp_auc_184: 0.8631\n",
      "Epoch 50/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4538 - siamese_loss: 5.3965 - mlp_loss: 0.4538 - mlp_binary_accuracy: 0.7873 - mlp_auc_184: 0.8665\n",
      "Epoch 51/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4540 - siamese_loss: 5.3888 - mlp_loss: 0.4540 - mlp_binary_accuracy: 0.7900 - mlp_auc_184: 0.8667\n",
      "Epoch 52/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4475 - siamese_loss: 5.4242 - mlp_loss: 0.4475 - mlp_binary_accuracy: 0.7944 - mlp_auc_184: 0.8703\n",
      "Epoch 53/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4551 - siamese_loss: 5.4580 - mlp_loss: 0.4551 - mlp_binary_accuracy: 0.7903 - mlp_auc_184: 0.8653\n",
      "Epoch 54/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4544 - siamese_loss: 5.5654 - mlp_loss: 0.4544 - mlp_binary_accuracy: 0.7891 - mlp_auc_184: 0.8667\n",
      "Epoch 55/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4517 - siamese_loss: 5.4563 - mlp_loss: 0.4517 - mlp_binary_accuracy: 0.7813 - mlp_auc_184: 0.8675\n",
      "Epoch 56/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4498 - siamese_loss: 5.3417 - mlp_loss: 0.4498 - mlp_binary_accuracy: 0.7845 - mlp_auc_184: 0.8687\n",
      "Epoch 57/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4529 - siamese_loss: 5.3217 - mlp_loss: 0.4529 - mlp_binary_accuracy: 0.7859 - mlp_auc_184: 0.8674\n",
      "WARNING:tensorflow:9 out of the last 24 calls to <function Model.make_test_function.<locals>.test_function at 0x0000026146E38C18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5650 - siamese_loss: 5.5038 - mlp_loss: 0.5650 - mlp_binary_accuracy: 0.7267 - mlp_auc_184: 0.8029\n",
      "------ mlp_binary_accuracy: 72.67%\t ----- mlp_auc_184: 80.29%\n",
      "concat_fea的shape： (None, 64)\n",
      "-------------------------------------Kfold: 9 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6885 - siamese_loss: 3.3072 - mlp_loss: 0.6885 - mlp_binary_accuracy: 0.5316 - mlp_auc_185: 0.5507\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6422 - siamese_loss: 3.6947 - mlp_loss: 0.6422 - mlp_binary_accuracy: 0.6384 - mlp_auc_185: 0.6815\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5988 - siamese_loss: 4.0242 - mlp_loss: 0.5988 - mlp_binary_accuracy: 0.6837 - mlp_auc_185: 0.7397\n",
      "Epoch 4/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5847 - siamese_loss: 3.9723 - mlp_loss: 0.5847 - mlp_binary_accuracy: 0.6981 - mlp_auc_185: 0.7584\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5759 - siamese_loss: 4.1934 - mlp_loss: 0.5759 - mlp_binary_accuracy: 0.7032 - mlp_auc_185: 0.7661\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5636 - siamese_loss: 4.2486 - mlp_loss: 0.5636 - mlp_binary_accuracy: 0.7149 - mlp_auc_185: 0.7787\n",
      "Epoch 7/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5604 - siamese_loss: 4.3600 - mlp_loss: 0.5604 - mlp_binary_accuracy: 0.7112 - mlp_auc_185: 0.7803\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5481 - siamese_loss: 4.4269 - mlp_loss: 0.5481 - mlp_binary_accuracy: 0.7324 - mlp_auc_185: 0.7931\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5409 - siamese_loss: 4.5505 - mlp_loss: 0.5409 - mlp_binary_accuracy: 0.7393 - mlp_auc_185: 0.8012\n",
      "Epoch 10/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5360 - siamese_loss: 4.5372 - mlp_loss: 0.5360 - mlp_binary_accuracy: 0.7347 - mlp_auc_185: 0.8057\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5304 - siamese_loss: 4.8581 - mlp_loss: 0.5304 - mlp_binary_accuracy: 0.7457 - mlp_auc_185: 0.8118\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5300 - siamese_loss: 4.7537 - mlp_loss: 0.5300 - mlp_binary_accuracy: 0.7354 - mlp_auc_185: 0.8113\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5233 - siamese_loss: 4.7348 - mlp_loss: 0.5233 - mlp_binary_accuracy: 0.7498 - mlp_auc_185: 0.8172\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5199 - siamese_loss: 4.8261 - mlp_loss: 0.5199 - mlp_binary_accuracy: 0.7517 - mlp_auc_185: 0.8201\n",
      "Epoch 15/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5219 - siamese_loss: 4.7386 - mlp_loss: 0.5219 - mlp_binary_accuracy: 0.7459 - mlp_auc_185: 0.8191\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5150 - siamese_loss: 4.7388 - mlp_loss: 0.5150 - mlp_binary_accuracy: 0.7569 - mlp_auc_185: 0.8249\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5162 - siamese_loss: 4.9414 - mlp_loss: 0.5162 - mlp_binary_accuracy: 0.7514 - mlp_auc_185: 0.8231\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5058 - siamese_loss: 4.9007 - mlp_loss: 0.5058 - mlp_binary_accuracy: 0.7613 - mlp_auc_185: 0.8325\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5012 - siamese_loss: 5.0090 - mlp_loss: 0.5012 - mlp_binary_accuracy: 0.7659 - mlp_auc_185: 0.8364\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5000 - siamese_loss: 5.0310 - mlp_loss: 0.5000 - mlp_binary_accuracy: 0.7645 - mlp_auc_185: 0.8373\n",
      "Epoch 21/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5006 - siamese_loss: 4.9384 - mlp_loss: 0.5006 - mlp_binary_accuracy: 0.7668 - mlp_auc_185: 0.8366\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4976 - siamese_loss: 4.8823 - mlp_loss: 0.4976 - mlp_binary_accuracy: 0.7597 - mlp_auc_185: 0.8384\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4966 - siamese_loss: 5.0046 - mlp_loss: 0.4966 - mlp_binary_accuracy: 0.7581 - mlp_auc_185: 0.8387\n",
      "Epoch 24/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4946 - siamese_loss: 5.1495 - mlp_loss: 0.4946 - mlp_binary_accuracy: 0.7645 - mlp_auc_185: 0.8407\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4926 - siamese_loss: 5.0124 - mlp_loss: 0.4926 - mlp_binary_accuracy: 0.7652 - mlp_auc_185: 0.8424\n",
      "Epoch 26/200\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.4844 - siamese_loss: 5.0791 - mlp_loss: 0.4844 - mlp_binary_accuracy: 0.7741 - mlp_auc_185: 0.848 - 0s 2ms/step - loss: 0.4865 - siamese_loss: 5.0920 - mlp_loss: 0.4865 - mlp_binary_accuracy: 0.7705 - mlp_auc_185: 0.8468\n",
      "Epoch 27/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4777 - siamese_loss: 5.2595 - mlp_loss: 0.4777 - mlp_binary_accuracy: 0.7822 - mlp_auc_185: 0.8535\n",
      "Epoch 28/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4927 - siamese_loss: 5.1437 - mlp_loss: 0.4927 - mlp_binary_accuracy: 0.7673 - mlp_auc_185: 0.8434\n",
      "Epoch 29/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4830 - siamese_loss: 5.1576 - mlp_loss: 0.4830 - mlp_binary_accuracy: 0.7772 - mlp_auc_185: 0.8488\n",
      "Epoch 30/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4773 - siamese_loss: 5.2826 - mlp_loss: 0.4773 - mlp_binary_accuracy: 0.7769 - mlp_auc_185: 0.8534\n",
      "Epoch 31/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4705 - siamese_loss: 5.2233 - mlp_loss: 0.4705 - mlp_binary_accuracy: 0.7811 - mlp_auc_185: 0.8575\n",
      "Epoch 32/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4738 - siamese_loss: 5.3583 - mlp_loss: 0.4738 - mlp_binary_accuracy: 0.7843 - mlp_auc_185: 0.8555\n",
      "Epoch 33/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4769 - siamese_loss: 5.1120 - mlp_loss: 0.4769 - mlp_binary_accuracy: 0.7785 - mlp_auc_185: 0.8529\n",
      "Epoch 34/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4676 - siamese_loss: 5.2519 - mlp_loss: 0.4676 - mlp_binary_accuracy: 0.7781 - mlp_auc_185: 0.8594\n",
      "Epoch 35/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4710 - siamese_loss: 5.1420 - mlp_loss: 0.4710 - mlp_binary_accuracy: 0.7723 - mlp_auc_185: 0.8569\n",
      "Epoch 36/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4653 - siamese_loss: 5.2620 - mlp_loss: 0.4653 - mlp_binary_accuracy: 0.7847 - mlp_auc_185: 0.8608\n",
      "Epoch 37/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4638 - siamese_loss: 5.2023 - mlp_loss: 0.4638 - mlp_binary_accuracy: 0.7866 - mlp_auc_185: 0.8622\n",
      "Epoch 38/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4480 - siamese_loss: 5.2471 - mlp_loss: 0.4480 - mlp_binary_accuracy: 0.7955 - mlp_auc_185: 0.8717\n",
      "Epoch 39/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4634 - siamese_loss: 5.1460 - mlp_loss: 0.4634 - mlp_binary_accuracy: 0.7834 - mlp_auc_185: 0.8622\n",
      "Epoch 40/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4614 - siamese_loss: 5.0767 - mlp_loss: 0.4614 - mlp_binary_accuracy: 0.7903 - mlp_auc_185: 0.8633\n",
      "Epoch 41/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4629 - siamese_loss: 5.1033 - mlp_loss: 0.4629 - mlp_binary_accuracy: 0.7845 - mlp_auc_185: 0.8626\n",
      "Epoch 42/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4645 - siamese_loss: 5.1162 - mlp_loss: 0.4645 - mlp_binary_accuracy: 0.7841 - mlp_auc_185: 0.8614\n",
      "Epoch 43/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4613 - siamese_loss: 5.2333 - mlp_loss: 0.4613 - mlp_binary_accuracy: 0.7909 - mlp_auc_185: 0.8644\n",
      "WARNING:tensorflow:10 out of the last 25 calls to <function Model.make_test_function.<locals>.test_function at 0x000002612586FE58> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6123 - siamese_loss: 5.2972 - mlp_loss: 0.6123 - mlp_binary_accuracy: 0.7246 - mlp_auc_185: 0.7972\n",
      "------ mlp_binary_accuracy: 72.46%\t ----- mlp_auc_185: 79.72%\n",
      "concat_fea的shape： (None, 64)\n",
      "-------------------------------------Kfold: 10 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6841 - siamese_loss: 3.4644 - mlp_loss: 0.6841 - mlp_binary_accuracy: 0.5396 - mlp_auc_186: 0.5884\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6529 - siamese_loss: 3.8997 - mlp_loss: 0.6529 - mlp_binary_accuracy: 0.6278 - mlp_auc_186: 0.6628\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6370 - siamese_loss: 3.9012 - mlp_loss: 0.6370 - mlp_binary_accuracy: 0.6487 - mlp_auc_186: 0.6777\n",
      "Epoch 4/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6189 - siamese_loss: 3.9636 - mlp_loss: 0.6189 - mlp_binary_accuracy: 0.6814 - mlp_auc_186: 0.7152\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6138 - siamese_loss: 4.3632 - mlp_loss: 0.6138 - mlp_binary_accuracy: 0.6685 - mlp_auc_186: 0.7023\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6035 - siamese_loss: 4.2888 - mlp_loss: 0.6035 - mlp_binary_accuracy: 0.6848 - mlp_auc_186: 0.7266\n",
      "Epoch 7/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5954 - siamese_loss: 4.2499 - mlp_loss: 0.5954 - mlp_binary_accuracy: 0.6961 - mlp_auc_186: 0.7420\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5817 - siamese_loss: 4.5321 - mlp_loss: 0.5817 - mlp_binary_accuracy: 0.7062 - mlp_auc_186: 0.7479\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5722 - siamese_loss: 4.6762 - mlp_loss: 0.5722 - mlp_binary_accuracy: 0.7092 - mlp_auc_186: 0.7491\n",
      "Epoch 10/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5744 - siamese_loss: 4.8506 - mlp_loss: 0.5744 - mlp_binary_accuracy: 0.7133 - mlp_auc_186: 0.7508\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.5660 - siamese_loss: 4.7892 - mlp_loss: 0.5660 - mlp_binary_accuracy: 0.7195 - mlp_auc_186: 0.7668\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5557 - siamese_loss: 4.8356 - mlp_loss: 0.5557 - mlp_binary_accuracy: 0.7301 - mlp_auc_186: 0.7721\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5526 - siamese_loss: 4.9024 - mlp_loss: 0.5526 - mlp_binary_accuracy: 0.7326 - mlp_auc_186: 0.7741\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5499 - siamese_loss: 4.9776 - mlp_loss: 0.5499 - mlp_binary_accuracy: 0.7292 - mlp_auc_186: 0.7797\n",
      "Epoch 15/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5472 - siamese_loss: 4.7074 - mlp_loss: 0.5472 - mlp_binary_accuracy: 0.7358 - mlp_auc_186: 0.7860\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5418 - siamese_loss: 4.7452 - mlp_loss: 0.5418 - mlp_binary_accuracy: 0.7434 - mlp_auc_186: 0.7902\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5364 - siamese_loss: 4.7256 - mlp_loss: 0.5364 - mlp_binary_accuracy: 0.7524 - mlp_auc_186: 0.7946\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5390 - siamese_loss: 4.9724 - mlp_loss: 0.5390 - mlp_binary_accuracy: 0.7466 - mlp_auc_186: 0.7949\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5338 - siamese_loss: 4.9036 - mlp_loss: 0.5338 - mlp_binary_accuracy: 0.7480 - mlp_auc_186: 0.7997\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5295 - siamese_loss: 4.7582 - mlp_loss: 0.5295 - mlp_binary_accuracy: 0.7524 - mlp_auc_186: 0.8040\n",
      "Epoch 21/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5290 - siamese_loss: 4.7652 - mlp_loss: 0.5290 - mlp_binary_accuracy: 0.7496 - mlp_auc_186: 0.8058\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5269 - siamese_loss: 4.9190 - mlp_loss: 0.5269 - mlp_binary_accuracy: 0.7549 - mlp_auc_186: 0.8058\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5227 - siamese_loss: 4.9022 - mlp_loss: 0.5227 - mlp_binary_accuracy: 0.7567 - mlp_auc_186: 0.8114\n",
      "Epoch 24/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5199 - siamese_loss: 4.7617 - mlp_loss: 0.5199 - mlp_binary_accuracy: 0.7574 - mlp_auc_186: 0.8151\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5143 - siamese_loss: 5.0641 - mlp_loss: 0.5143 - mlp_binary_accuracy: 0.7615 - mlp_auc_186: 0.8141\n",
      "Epoch 26/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5172 - siamese_loss: 4.8330 - mlp_loss: 0.5172 - mlp_binary_accuracy: 0.7563 - mlp_auc_186: 0.8166\n",
      "Epoch 27/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5099 - siamese_loss: 4.8222 - mlp_loss: 0.5099 - mlp_binary_accuracy: 0.7638 - mlp_auc_186: 0.8236\n",
      "Epoch 28/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5107 - siamese_loss: 4.9380 - mlp_loss: 0.5107 - mlp_binary_accuracy: 0.7611 - mlp_auc_186: 0.8225\n",
      "Epoch 29/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5053 - siamese_loss: 4.9870 - mlp_loss: 0.5053 - mlp_binary_accuracy: 0.7751 - mlp_auc_186: 0.8278\n",
      "Epoch 30/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4986 - siamese_loss: 4.8648 - mlp_loss: 0.4986 - mlp_binary_accuracy: 0.7751 - mlp_auc_186: 0.8330\n",
      "Epoch 31/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5043 - siamese_loss: 4.8492 - mlp_loss: 0.5043 - mlp_binary_accuracy: 0.7673 - mlp_auc_186: 0.8292\n",
      "Epoch 32/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4982 - siamese_loss: 5.0729 - mlp_loss: 0.4982 - mlp_binary_accuracy: 0.7756 - mlp_auc_186: 0.8355\n",
      "Epoch 33/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4950 - siamese_loss: 5.1930 - mlp_loss: 0.4950 - mlp_binary_accuracy: 0.7746 - mlp_auc_186: 0.8390\n",
      "Epoch 34/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4931 - siamese_loss: 5.0800 - mlp_loss: 0.4931 - mlp_binary_accuracy: 0.7802 - mlp_auc_186: 0.8373\n",
      "Epoch 35/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4978 - siamese_loss: 5.0762 - mlp_loss: 0.4978 - mlp_binary_accuracy: 0.7714 - mlp_auc_186: 0.8357\n",
      "Epoch 36/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4947 - siamese_loss: 5.4394 - mlp_loss: 0.4947 - mlp_binary_accuracy: 0.7717 - mlp_auc_186: 0.8374\n",
      "Epoch 37/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4892 - siamese_loss: 5.2914 - mlp_loss: 0.4892 - mlp_binary_accuracy: 0.7785 - mlp_auc_186: 0.8389\n",
      "Epoch 38/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4833 - siamese_loss: 5.3062 - mlp_loss: 0.4833 - mlp_binary_accuracy: 0.7887 - mlp_auc_186: 0.8476\n",
      "Epoch 39/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4885 - siamese_loss: 5.2634 - mlp_loss: 0.4885 - mlp_binary_accuracy: 0.7774 - mlp_auc_186: 0.8404\n",
      "Epoch 40/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4798 - siamese_loss: 5.4303 - mlp_loss: 0.4798 - mlp_binary_accuracy: 0.7824 - mlp_auc_186: 0.8455\n",
      "Epoch 41/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4863 - siamese_loss: 5.2819 - mlp_loss: 0.4863 - mlp_binary_accuracy: 0.7808 - mlp_auc_186: 0.8447\n",
      "Epoch 42/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4871 - siamese_loss: 5.0216 - mlp_loss: 0.4871 - mlp_binary_accuracy: 0.7790 - mlp_auc_186: 0.8463\n",
      "Epoch 43/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4742 - siamese_loss: 5.2788 - mlp_loss: 0.4742 - mlp_binary_accuracy: 0.7903 - mlp_auc_186: 0.8527\n",
      "Epoch 44/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4762 - siamese_loss: 5.1232 - mlp_loss: 0.4762 - mlp_binary_accuracy: 0.7861 - mlp_auc_186: 0.8502\n",
      "Epoch 45/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4734 - siamese_loss: 5.2174 - mlp_loss: 0.4734 - mlp_binary_accuracy: 0.7882 - mlp_auc_186: 0.8528\n",
      "Epoch 46/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4716 - siamese_loss: 5.2025 - mlp_loss: 0.4716 - mlp_binary_accuracy: 0.7843 - mlp_auc_186: 0.8564\n",
      "Epoch 47/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4750 - siamese_loss: 5.2800 - mlp_loss: 0.4750 - mlp_binary_accuracy: 0.7845 - mlp_auc_186: 0.8524\n",
      "Epoch 48/200\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.4760 - siamese_loss: 5.3515 - mlp_loss: 0.4760 - mlp_binary_accuracy: 0.7864 - mlp_auc_186: 0.8488\n",
      "Epoch 49/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4760 - siamese_loss: 5.3614 - mlp_loss: 0.4760 - mlp_binary_accuracy: 0.7891 - mlp_auc_186: 0.8524\n",
      "Epoch 50/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4705 - siamese_loss: 5.4900 - mlp_loss: 0.4705 - mlp_binary_accuracy: 0.7916 - mlp_auc_186: 0.8536\n",
      "Epoch 51/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4720 - siamese_loss: 5.3226 - mlp_loss: 0.4720 - mlp_binary_accuracy: 0.7905 - mlp_auc_186: 0.8539\n",
      "Epoch 52/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4705 - siamese_loss: 5.5118 - mlp_loss: 0.4705 - mlp_binary_accuracy: 0.7887 - mlp_auc_186: 0.8528\n",
      "Epoch 53/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4673 - siamese_loss: 5.4948 - mlp_loss: 0.4673 - mlp_binary_accuracy: 0.7912 - mlp_auc_186: 0.8543\n",
      "Epoch 54/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4695 - siamese_loss: 5.5557 - mlp_loss: 0.4695 - mlp_binary_accuracy: 0.7882 - mlp_auc_186: 0.8548\n",
      "Epoch 55/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4611 - siamese_loss: 5.5655 - mlp_loss: 0.4611 - mlp_binary_accuracy: 0.7930 - mlp_auc_186: 0.8626\n",
      "Epoch 56/200\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.4690 - siamese_loss: 5.5347 - mlp_loss: 0.4690 - mlp_binary_accuracy: 0.7845 - mlp_auc_186: 0.8551\n",
      "Epoch 57/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4591 - siamese_loss: 5.4008 - mlp_loss: 0.4591 - mlp_binary_accuracy: 0.7907 - mlp_auc_186: 0.8624\n",
      "Epoch 58/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4578 - siamese_loss: 5.4891 - mlp_loss: 0.4578 - mlp_binary_accuracy: 0.7967 - mlp_auc_186: 0.8647\n",
      "Epoch 59/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4544 - siamese_loss: 5.5887 - mlp_loss: 0.4544 - mlp_binary_accuracy: 0.7928 - mlp_auc_186: 0.8657\n",
      "Epoch 60/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4530 - siamese_loss: 5.5999 - mlp_loss: 0.4530 - mlp_binary_accuracy: 0.7932 - mlp_auc_186: 0.8682\n",
      "Epoch 61/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4658 - siamese_loss: 5.5580 - mlp_loss: 0.4658 - mlp_binary_accuracy: 0.7884 - mlp_auc_186: 0.8591\n",
      "Epoch 62/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4530 - siamese_loss: 5.5916 - mlp_loss: 0.4530 - mlp_binary_accuracy: 0.7896 - mlp_auc_186: 0.8671\n",
      "Epoch 63/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4497 - siamese_loss: 5.4741 - mlp_loss: 0.4497 - mlp_binary_accuracy: 0.7969 - mlp_auc_186: 0.8695\n",
      "Epoch 64/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4459 - siamese_loss: 5.6173 - mlp_loss: 0.4459 - mlp_binary_accuracy: 0.8086 - mlp_auc_186: 0.8717\n",
      "Epoch 65/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4460 - siamese_loss: 5.4248 - mlp_loss: 0.4460 - mlp_binary_accuracy: 0.8013 - mlp_auc_186: 0.8724\n",
      "Epoch 66/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4462 - siamese_loss: 5.5313 - mlp_loss: 0.4462 - mlp_binary_accuracy: 0.8015 - mlp_auc_186: 0.8712\n",
      "Epoch 67/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4430 - siamese_loss: 5.5636 - mlp_loss: 0.4430 - mlp_binary_accuracy: 0.8027 - mlp_auc_186: 0.8726\n",
      "Epoch 68/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4413 - siamese_loss: 5.7073 - mlp_loss: 0.4413 - mlp_binary_accuracy: 0.8011 - mlp_auc_186: 0.8749\n",
      "Epoch 69/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4386 - siamese_loss: 5.7281 - mlp_loss: 0.4386 - mlp_binary_accuracy: 0.8034 - mlp_auc_186: 0.8764\n",
      "Epoch 70/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4362 - siamese_loss: 5.8471 - mlp_loss: 0.4362 - mlp_binary_accuracy: 0.8038 - mlp_auc_186: 0.8780\n",
      "Epoch 71/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4408 - siamese_loss: 5.7624 - mlp_loss: 0.4408 - mlp_binary_accuracy: 0.8054 - mlp_auc_186: 0.8757\n",
      "Epoch 72/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4400 - siamese_loss: 5.7393 - mlp_loss: 0.4400 - mlp_binary_accuracy: 0.8045 - mlp_auc_186: 0.8727\n",
      "Epoch 73/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4295 - siamese_loss: 5.6234 - mlp_loss: 0.4295 - mlp_binary_accuracy: 0.8137 - mlp_auc_186: 0.8800\n",
      "Epoch 74/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4295 - siamese_loss: 5.6206 - mlp_loss: 0.4295 - mlp_binary_accuracy: 0.8135 - mlp_auc_186: 0.8808\n",
      "Epoch 75/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4251 - siamese_loss: 5.6658 - mlp_loss: 0.4251 - mlp_binary_accuracy: 0.8105 - mlp_auc_186: 0.8830\n",
      "Epoch 76/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4288 - siamese_loss: 5.6111 - mlp_loss: 0.4288 - mlp_binary_accuracy: 0.8089 - mlp_auc_186: 0.8831\n",
      "Epoch 77/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4283 - siamese_loss: 5.7227 - mlp_loss: 0.4283 - mlp_binary_accuracy: 0.8146 - mlp_auc_186: 0.8814\n",
      "Epoch 78/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4265 - siamese_loss: 5.8765 - mlp_loss: 0.4265 - mlp_binary_accuracy: 0.8130 - mlp_auc_186: 0.8835\n",
      "Epoch 79/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4273 - siamese_loss: 5.8898 - mlp_loss: 0.4273 - mlp_binary_accuracy: 0.8130 - mlp_auc_186: 0.8833\n",
      "Epoch 80/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4245 - siamese_loss: 5.8159 - mlp_loss: 0.4245 - mlp_binary_accuracy: 0.8164 - mlp_auc_186: 0.8847\n",
      "Epoch 81/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4270 - siamese_loss: 5.8911 - mlp_loss: 0.4270 - mlp_binary_accuracy: 0.8084 - mlp_auc_186: 0.8827\n",
      "Epoch 82/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4162 - siamese_loss: 5.8818 - mlp_loss: 0.4162 - mlp_binary_accuracy: 0.8162 - mlp_auc_186: 0.8893\n",
      "Epoch 83/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4285 - siamese_loss: 5.8548 - mlp_loss: 0.4285 - mlp_binary_accuracy: 0.8153 - mlp_auc_186: 0.8829\n",
      "Epoch 84/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4337 - siamese_loss: 5.8862 - mlp_loss: 0.4337 - mlp_binary_accuracy: 0.8075 - mlp_auc_186: 0.8792\n",
      "Epoch 85/200\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.4195 - siamese_loss: 5.8936 - mlp_loss: 0.4195 - mlp_binary_accuracy: 0.8169 - mlp_auc_186: 0.8877\n",
      "Epoch 86/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4307 - siamese_loss: 5.8779 - mlp_loss: 0.4307 - mlp_binary_accuracy: 0.8038 - mlp_auc_186: 0.8803\n",
      "Epoch 87/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4250 - siamese_loss: 5.8939 - mlp_loss: 0.4250 - mlp_binary_accuracy: 0.8116 - mlp_auc_186: 0.8846\n",
      "WARNING:tensorflow:11 out of the last 26 calls to <function Model.make_test_function.<locals>.test_function at 0x0000026133A74DC8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5566 - siamese_loss: 6.2392 - mlp_loss: 0.5566 - mlp_binary_accuracy: 0.7516 - mlp_auc_186: 0.8169\n",
      "------ mlp_binary_accuracy: 75.16%\t ----- mlp_auc_186: 81.69%\n",
      "十折性能均值：-------- ave_acc: 74.05% (+/- 1.81%)\t ----- ave_auc_mlp:81.17% (+/- 1.19%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics  import roc_curve,auc,roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "cvacc,cvauc_mlp,cvpre,cvpre_top,cvrec,cvauc_sia=[],[],[],[],[],[]\n",
    "i =1\n",
    "\n",
    "\"\"\"trans to ndarray\"\"\"\n",
    "y=np.array(y)\n",
    "disA_fea_mat=np.array(disA_fea_mat)\n",
    "disB_fea_mat=np.array(disB_fea_mat)\n",
    "\n",
    "\"\"\"random_state=10,11,12 \"\"\"\n",
    "rus = RandomUnderSampler(sampling_strategy=1,random_state=11)\n",
    "rus_dfmA,rus_y = rus.fit_resample(X=disA_fea_mat,y=y)\n",
    "rus_dfmB,rus_y = rus.fit_resample(X=disB_fea_mat,y=y)\n",
    "print('rus_y的长度:',len(rus_y))\n",
    "\n",
    "seed = 201202\n",
    "np.random.seed(seed) # This is necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "python_random.seed(seed)# This is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\"\"\"10 fold\"\"\"\n",
    "Kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\n",
    "for train,test in Kfold.split(X=rus_dfmA,y=rus_y):\n",
    "    ###############################################################################################\n",
    "    \"\"\"create model\"\"\"\n",
    "    input_shape=(322)\n",
    "    base_network = create_base_network(input_shape)\n",
    "    \n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "    # because we re-use the same instance `base_network`,the weights of the network will be shared across the two branches\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "\n",
    "    eucl_model = create_euclLayer(processed_a.shape[-1])\n",
    "    eucl_out = eucl_model([processed_a,processed_b])   \n",
    "    \n",
    "    concat_fea = tf.keras.layers.concatenate([processed_a,processed_b],axis=-1)\n",
    "    print(\"concat_fea的shape：\",concat_fea.shape) #(None,32)\n",
    "    MLP = create_MLP(concat_fea.shape[-1])\n",
    "    MLP_out = MLP(concat_fea)\n",
    "    \n",
    "    model = Model(inputs=[input_a, input_b], outputs=[eucl_out,MLP_out])\n",
    "    \n",
    "    \"\"\"L2\"\"\"\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer,'kernel_regularizer'):\n",
    "            layer.kernel_regularizer = tf.keras.regularizers.l2(0.001)\n",
    "        \n",
    "    #####################################################################################################################\n",
    "    \n",
    "    print(\"-------------------------------------Kfold: {} iter-----------------------------------------\".format(i))\n",
    "    i+=1\n",
    "\n",
    "    rms = RMSprop()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=5)\n",
    "    \n",
    "    my_loss = {\"siamese\":contrastive_loss,\"mlp\":tf.keras.losses.BinaryCrossentropy()}\n",
    "    my_loss_weight = {\"siamese\":0,\"mlp\":1}\n",
    "    my_metrics ={ \"mlp\":[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()]} #tf.keras.metrics.Precision(top_k=len(rus_y)//100)\n",
    "    \n",
    "    model.compile(\n",
    "#         loss= [tfa.losses.ContrastiveLoss(),tf.keras.losses.BinaryCrossentropy()],\n",
    "#         loss_weights=[0.01,1],\n",
    "#         metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()],\n",
    "        \n",
    "        loss= my_loss,\n",
    "        loss_weights=my_loss_weight,\n",
    "        metrics=my_metrics,\n",
    "        \n",
    "        optimizer=rms,\n",
    "    )\n",
    "    \n",
    "    # fit()中 shuffle=True\n",
    "    model.fit([rus_dfmA[train],rus_dfmB[train]],[rus_y[train],rus_y[train]],\n",
    "              batch_size=None,\n",
    "              epochs=200,\n",
    "              callbacks=callback,\n",
    "              shuffle=True)\n",
    "    \n",
    "    scores = model.evaluate([rus_dfmA[test],rus_dfmB[test]],[rus_y[test],rus_y[test]],\n",
    "                            verbose=1,\n",
    "                            batch_size=len(rus_y[test]),\n",
    "                            callbacks=callback)\n",
    "    \n",
    "    cvacc.append(scores[-2] * 100)\n",
    "    cvauc_mlp.append(scores[-1] * 100)\n",
    "    cvauc_sia.append(scores[-3] * 100)\n",
    "    \n",
    "    print(\"------ %s: %.2f%%\\t ----- %s: %.2f%%\" % \n",
    "           (model.metrics_names[-2],scores[-2]*100, model.metrics_names[-1],scores[-1]*100))\n",
    "     \n",
    "#print(\"ave_auc_keras: %.2f%% (+/- %.2f%%)\\t ave_topk_pre: %.2f%% (+/- %.2f%%)\\t ave_auc:%.2f%% (+/- %.2f%%)\" % \n",
    "print(\"十折性能均值：-------- ave_acc: %.2f%% (+/- %.2f%%)\\t ----- ave_auc_mlp:%.2f%% (+/- %.2f%%)\" % \n",
    "      (np.mean(cvacc), np.std(cvacc),np.mean(cvauc_mlp), np.std(cvauc_mlp)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "10-fold avg-------- ave_acc: 74.05% (+/- 1.81%)\t ----- ave_auc_mlp:81.17% (+/- 1.19%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_333\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1042 (InputLayer)         [(None, 322)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1043 (InputLayer)         [(None, 322)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "base_net (Functional)           (None, 32)           52736       input_1042[0][0]                 \n",
      "                                                                 input_1043[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_166 (Concatenate)   (None, 64)           0           base_net[0][0]                   \n",
      "                                                                 base_net[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "siamese (Functional)            (None, 1)            0           base_net[0][0]                   \n",
      "                                                                 base_net[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mlp (Functional)                (None, 1)            6913        concatenate_166[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 59,649\n",
      "Trainable params: 59,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test2:sym+mesh (siamese+mlp)\n",
    "SDSP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mesh（,24）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35245, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abnormalities, multiple</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abnormalities, multiple</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abnormalities, multiple</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abnormalities, multiple</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abnormalities, multiple</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0    1   10   11   12   13   14   15   16   17  \\\n",
       "abnormalities, multiple  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "abnormalities, multiple  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "abnormalities, multiple  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "abnormalities, multiple  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "abnormalities, multiple  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "                         ...   21   22   23    3    4    5    6    7    8    9  \n",
       "abnormalities, multiple  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "abnormalities, multiple  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "abnormalities, multiple  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "abnormalities, multiple  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "abnormalities, multiple  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh_label_A = pd.read_csv(\"concat_mesh_label/mesh_label_A.csv\",index_col=0)\n",
    "print(mesh_label_A.shape)\n",
    "mesh_label_A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_B = pd.read_csv(\"concat_mesh_label/mesh_label_B.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus_y的长度: 4836\n",
      "rus_mesh_B的长度： 4836\n",
      "concat_fea的shape： (None, 112)\n",
      "-------------------------------------Kfold: 1 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6709 - siamese_loss: 2.7976 - mlp_loss: 0.6709 - mlp_binary_accuracy: 0.5770 - mlp_auc_28: 0.6135\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6219 - siamese_loss: 2.7411 - mlp_loss: 0.6219 - mlp_binary_accuracy: 0.6528 - mlp_auc_28: 0.7037\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5949 - siamese_loss: 3.3611 - mlp_loss: 0.5949 - mlp_binary_accuracy: 0.6797 - mlp_auc_28: 0.7422\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5740 - siamese_loss: 3.6960 - mlp_loss: 0.5740 - mlp_binary_accuracy: 0.7089 - mlp_auc_28: 0.7661\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5659 - siamese_loss: 3.7381 - mlp_loss: 0.5659 - mlp_binary_accuracy: 0.7034 - mlp_auc_28: 0.7752\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5496 - siamese_loss: 3.9614 - mlp_loss: 0.5496 - mlp_binary_accuracy: 0.7243 - mlp_auc_28: 0.7934\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5383 - siamese_loss: 3.9899 - mlp_loss: 0.5383 - mlp_binary_accuracy: 0.7376 - mlp_auc_28: 0.8031\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5296 - siamese_loss: 4.0863 - mlp_loss: 0.5296 - mlp_binary_accuracy: 0.7399 - mlp_auc_28: 0.8106\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5202 - siamese_loss: 4.1690 - mlp_loss: 0.5202 - mlp_binary_accuracy: 0.7493 - mlp_auc_28: 0.8198\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5157 - siamese_loss: 4.2403 - mlp_loss: 0.5157 - mlp_binary_accuracy: 0.7530 - mlp_auc_28: 0.8224\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5028 - siamese_loss: 4.3230 - mlp_loss: 0.5028 - mlp_binary_accuracy: 0.7567 - mlp_auc_28: 0.8345\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4951 - siamese_loss: 4.4641 - mlp_loss: 0.4951 - mlp_binary_accuracy: 0.7682 - mlp_auc_28: 0.8391\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4880 - siamese_loss: 4.5621 - mlp_loss: 0.4880 - mlp_binary_accuracy: 0.7691 - mlp_auc_28: 0.8449\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4826 - siamese_loss: 4.6290 - mlp_loss: 0.4826 - mlp_binary_accuracy: 0.7744 - mlp_auc_28: 0.8485\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4737 - siamese_loss: 4.6626 - mlp_loss: 0.4737 - mlp_binary_accuracy: 0.7824 - mlp_auc_28: 0.8548\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4741 - siamese_loss: 4.6601 - mlp_loss: 0.4741 - mlp_binary_accuracy: 0.7711 - mlp_auc_28: 0.8540\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4584 - siamese_loss: 4.8369 - mlp_loss: 0.4584 - mlp_binary_accuracy: 0.7875 - mlp_auc_28: 0.8654\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4579 - siamese_loss: 5.0475 - mlp_loss: 0.4579 - mlp_binary_accuracy: 0.7852 - mlp_auc_28: 0.8658\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4484 - siamese_loss: 4.9988 - mlp_loss: 0.4484 - mlp_binary_accuracy: 0.7962 - mlp_auc_28: 0.8715\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4404 - siamese_loss: 4.9311 - mlp_loss: 0.4404 - mlp_binary_accuracy: 0.8035 - mlp_auc_28: 0.8762\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4367 - siamese_loss: 5.0487 - mlp_loss: 0.4367 - mlp_binary_accuracy: 0.8008 - mlp_auc_28: 0.8789\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4285 - siamese_loss: 4.9508 - mlp_loss: 0.4285 - mlp_binary_accuracy: 0.8033 - mlp_auc_28: 0.8836\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4175 - siamese_loss: 4.9534 - mlp_loss: 0.4175 - mlp_binary_accuracy: 0.8123 - mlp_auc_28: 0.8905\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4173 - siamese_loss: 5.1439 - mlp_loss: 0.4173 - mlp_binary_accuracy: 0.8097 - mlp_auc_28: 0.8906\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4089 - siamese_loss: 5.1919 - mlp_loss: 0.4089 - mlp_binary_accuracy: 0.8153 - mlp_auc_28: 0.8955\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4109 - siamese_loss: 5.1187 - mlp_loss: 0.4109 - mlp_binary_accuracy: 0.8127 - mlp_auc_28: 0.8943\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3989 - siamese_loss: 5.0838 - mlp_loss: 0.3989 - mlp_binary_accuracy: 0.8180 - mlp_auc_28: 0.9008\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3862 - siamese_loss: 5.1658 - mlp_loss: 0.3862 - mlp_binary_accuracy: 0.8265 - mlp_auc_28: 0.9069\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3853 - siamese_loss: 5.2593 - mlp_loss: 0.3853 - mlp_binary_accuracy: 0.8286 - mlp_auc_28: 0.9076\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3708 - siamese_loss: 5.0452 - mlp_loss: 0.3708 - mlp_binary_accuracy: 0.8396 - mlp_auc_28: 0.9154\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3739 - siamese_loss: 4.9271 - mlp_loss: 0.3739 - mlp_binary_accuracy: 0.8387 - mlp_auc_28: 0.9133\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3666 - siamese_loss: 4.9676 - mlp_loss: 0.3666 - mlp_binary_accuracy: 0.8396 - mlp_auc_28: 0.9168\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3647 - siamese_loss: 4.9184 - mlp_loss: 0.3647 - mlp_binary_accuracy: 0.8387 - mlp_auc_28: 0.9176\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3549 - siamese_loss: 5.0526 - mlp_loss: 0.3549 - mlp_binary_accuracy: 0.8454 - mlp_auc_28: 0.9224\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3540 - siamese_loss: 5.1908 - mlp_loss: 0.3540 - mlp_binary_accuracy: 0.8490 - mlp_auc_28: 0.9231\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3490 - siamese_loss: 5.3751 - mlp_loss: 0.3490 - mlp_binary_accuracy: 0.8534 - mlp_auc_28: 0.9246\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3393 - siamese_loss: 5.2825 - mlp_loss: 0.3393 - mlp_binary_accuracy: 0.8568 - mlp_auc_28: 0.9293\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3312 - siamese_loss: 5.2041 - mlp_loss: 0.3312 - mlp_binary_accuracy: 0.8575 - mlp_auc_28: 0.9327\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3298 - siamese_loss: 5.2802 - mlp_loss: 0.3298 - mlp_binary_accuracy: 0.8557 - mlp_auc_28: 0.9332\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3336 - siamese_loss: 5.4097 - mlp_loss: 0.3336 - mlp_binary_accuracy: 0.8552 - mlp_auc_28: 0.9317\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3321 - siamese_loss: 5.3273 - mlp_loss: 0.3321 - mlp_binary_accuracy: 0.8617 - mlp_auc_28: 0.9327\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3171 - siamese_loss: 5.4602 - mlp_loss: 0.3171 - mlp_binary_accuracy: 0.8665 - mlp_auc_28: 0.9383\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3113 - siamese_loss: 5.4138 - mlp_loss: 0.3113 - mlp_binary_accuracy: 0.8670 - mlp_auc_28: 0.9407\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3072 - siamese_loss: 5.3510 - mlp_loss: 0.3072 - mlp_binary_accuracy: 0.8670 - mlp_auc_28: 0.9425\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3087 - siamese_loss: 5.3271 - mlp_loss: 0.3087 - mlp_binary_accuracy: 0.8693 - mlp_auc_28: 0.9420\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3090 - siamese_loss: 5.3490 - mlp_loss: 0.3090 - mlp_binary_accuracy: 0.8704 - mlp_auc_28: 0.9417\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3082 - siamese_loss: 5.4478 - mlp_loss: 0.3082 - mlp_binary_accuracy: 0.8713 - mlp_auc_28: 0.9417\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2983 - siamese_loss: 5.5202 - mlp_loss: 0.2983 - mlp_binary_accuracy: 0.8748 - mlp_auc_28: 0.9457\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2836 - siamese_loss: 5.4892 - mlp_loss: 0.2836 - mlp_binary_accuracy: 0.8789 - mlp_auc_28: 0.9509\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2853 - siamese_loss: 5.4419 - mlp_loss: 0.2853 - mlp_binary_accuracy: 0.8837 - mlp_auc_28: 0.9502\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2874 - siamese_loss: 5.3537 - mlp_loss: 0.2874 - mlp_binary_accuracy: 0.8803 - mlp_auc_28: 0.9494\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2785 - siamese_loss: 5.4310 - mlp_loss: 0.2785 - mlp_binary_accuracy: 0.8824 - mlp_auc_28: 0.9529\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2644 - siamese_loss: 5.5244 - mlp_loss: 0.2644 - mlp_binary_accuracy: 0.8876 - mlp_auc_28: 0.9572\n",
      "Epoch 54/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2712 - siamese_loss: 5.5375 - mlp_loss: 0.2712 - mlp_binary_accuracy: 0.8869 - mlp_auc_28: 0.9550\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2676 - siamese_loss: 5.5349 - mlp_loss: 0.2676 - mlp_binary_accuracy: 0.8922 - mlp_auc_28: 0.9560\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2690 - siamese_loss: 5.4733 - mlp_loss: 0.2690 - mlp_binary_accuracy: 0.8906 - mlp_auc_28: 0.9558\n",
      "Epoch 57/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2575 - siamese_loss: 5.4962 - mlp_loss: 0.2575 - mlp_binary_accuracy: 0.8948 - mlp_auc_28: 0.9594\n",
      "Epoch 58/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2547 - siamese_loss: 5.4787 - mlp_loss: 0.2547 - mlp_binary_accuracy: 0.8952 - mlp_auc_28: 0.9603\n",
      "Epoch 59/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2489 - siamese_loss: 5.4597 - mlp_loss: 0.2489 - mlp_binary_accuracy: 0.8909 - mlp_auc_28: 0.9622\n",
      "Epoch 60/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2618 - siamese_loss: 5.4469 - mlp_loss: 0.2618 - mlp_binary_accuracy: 0.8902 - mlp_auc_28: 0.9581\n",
      "Epoch 61/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2489 - siamese_loss: 5.5319 - mlp_loss: 0.2489 - mlp_binary_accuracy: 0.8961 - mlp_auc_28: 0.9624\n",
      "Epoch 62/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2460 - siamese_loss: 5.5155 - mlp_loss: 0.2460 - mlp_binary_accuracy: 0.8977 - mlp_auc_28: 0.9630\n",
      "Epoch 63/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2378 - siamese_loss: 5.5322 - mlp_loss: 0.2378 - mlp_binary_accuracy: 0.8996 - mlp_auc_28: 0.9655\n",
      "Epoch 64/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2352 - siamese_loss: 5.5519 - mlp_loss: 0.2352 - mlp_binary_accuracy: 0.9035 - mlp_auc_28: 0.9661\n",
      "Epoch 65/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2324 - siamese_loss: 5.5531 - mlp_loss: 0.2324 - mlp_binary_accuracy: 0.9090 - mlp_auc_28: 0.9668\n",
      "Epoch 66/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2333 - siamese_loss: 5.4815 - mlp_loss: 0.2333 - mlp_binary_accuracy: 0.9088 - mlp_auc_28: 0.9669\n",
      "Epoch 67/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2199 - siamese_loss: 5.5500 - mlp_loss: 0.2199 - mlp_binary_accuracy: 0.9134 - mlp_auc_28: 0.9702\n",
      "Epoch 68/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2212 - siamese_loss: 5.6370 - mlp_loss: 0.2212 - mlp_binary_accuracy: 0.9138 - mlp_auc_28: 0.9696\n",
      "Epoch 69/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2189 - siamese_loss: 5.6305 - mlp_loss: 0.2189 - mlp_binary_accuracy: 0.9136 - mlp_auc_28: 0.9704\n",
      "Epoch 70/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2176 - siamese_loss: 5.5154 - mlp_loss: 0.2176 - mlp_binary_accuracy: 0.9111 - mlp_auc_28: 0.9709\n",
      "Epoch 71/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2140 - siamese_loss: 5.4902 - mlp_loss: 0.2140 - mlp_binary_accuracy: 0.9118 - mlp_auc_28: 0.9718\n",
      "Epoch 72/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2180 - siamese_loss: 5.4853 - mlp_loss: 0.2180 - mlp_binary_accuracy: 0.9106 - mlp_auc_28: 0.9709\n",
      "Epoch 73/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2130 - siamese_loss: 5.5555 - mlp_loss: 0.2130 - mlp_binary_accuracy: 0.9145 - mlp_auc_28: 0.9721\n",
      "Epoch 74/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2122 - siamese_loss: 5.4954 - mlp_loss: 0.2122 - mlp_binary_accuracy: 0.9168 - mlp_auc_28: 0.9720\n",
      "Epoch 75/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2031 - siamese_loss: 5.5934 - mlp_loss: 0.2031 - mlp_binary_accuracy: 0.9193 - mlp_auc_28: 0.9744\n",
      "Epoch 76/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2125 - siamese_loss: 5.5955 - mlp_loss: 0.2125 - mlp_binary_accuracy: 0.9203 - mlp_auc_28: 0.9719\n",
      "Epoch 77/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1999 - siamese_loss: 5.5464 - mlp_loss: 0.1999 - mlp_binary_accuracy: 0.9221 - mlp_auc_28: 0.9750\n",
      "Epoch 78/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1975 - siamese_loss: 5.6094 - mlp_loss: 0.1975 - mlp_binary_accuracy: 0.9210 - mlp_auc_28: 0.9761\n",
      "Epoch 79/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1896 - siamese_loss: 5.6373 - mlp_loss: 0.1896 - mlp_binary_accuracy: 0.9233 - mlp_auc_28: 0.9776\n",
      "Epoch 80/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1979 - siamese_loss: 5.5849 - mlp_loss: 0.1979 - mlp_binary_accuracy: 0.9226 - mlp_auc_28: 0.9755\n",
      "Epoch 81/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1919 - siamese_loss: 5.5946 - mlp_loss: 0.1919 - mlp_binary_accuracy: 0.9246 - mlp_auc_28: 0.9771\n",
      "Epoch 82/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1972 - siamese_loss: 5.6331 - mlp_loss: 0.1972 - mlp_binary_accuracy: 0.9210 - mlp_auc_28: 0.9761\n",
      "Epoch 83/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1873 - siamese_loss: 5.6114 - mlp_loss: 0.1873 - mlp_binary_accuracy: 0.9256 - mlp_auc_28: 0.9781\n",
      "Epoch 84/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1930 - siamese_loss: 5.6301 - mlp_loss: 0.1930 - mlp_binary_accuracy: 0.9295 - mlp_auc_28: 0.9767\n",
      "Epoch 85/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1934 - siamese_loss: 5.7246 - mlp_loss: 0.1934 - mlp_binary_accuracy: 0.9230 - mlp_auc_28: 0.9767\n",
      "Epoch 86/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1963 - siamese_loss: 5.6906 - mlp_loss: 0.1963 - mlp_binary_accuracy: 0.9216 - mlp_auc_28: 0.9761\n",
      "Epoch 87/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1834 - siamese_loss: 5.7416 - mlp_loss: 0.1834 - mlp_binary_accuracy: 0.9274 - mlp_auc_28: 0.9790\n",
      "Epoch 88/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1912 - siamese_loss: 5.7904 - mlp_loss: 0.1912 - mlp_binary_accuracy: 0.9246 - mlp_auc_28: 0.9774\n",
      "Epoch 89/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1937 - siamese_loss: 5.7061 - mlp_loss: 0.1937 - mlp_binary_accuracy: 0.9253 - mlp_auc_28: 0.9766\n",
      "Epoch 90/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1879 - siamese_loss: 5.7462 - mlp_loss: 0.1879 - mlp_binary_accuracy: 0.9276 - mlp_auc_28: 0.9781\n",
      "Epoch 91/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1802 - siamese_loss: 5.7298 - mlp_loss: 0.1802 - mlp_binary_accuracy: 0.9311 - mlp_auc_28: 0.9796\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1751 - siamese_loss: 5.4983 - mlp_loss: 0.1751 - mlp_binary_accuracy: 0.9276 - mlp_auc_28: 0.9808\n",
      "Epoch 93/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1817 - siamese_loss: 5.5231 - mlp_loss: 0.1817 - mlp_binary_accuracy: 0.9297 - mlp_auc_28: 0.9794\n",
      "Epoch 94/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1820 - siamese_loss: 5.6906 - mlp_loss: 0.1820 - mlp_binary_accuracy: 0.9306 - mlp_auc_28: 0.9790\n",
      "Epoch 95/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1744 - siamese_loss: 5.7354 - mlp_loss: 0.1744 - mlp_binary_accuracy: 0.9338 - mlp_auc_28: 0.9808\n",
      "Epoch 96/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1832 - siamese_loss: 5.7325 - mlp_loss: 0.1832 - mlp_binary_accuracy: 0.9269 - mlp_auc_28: 0.9788\n",
      "Epoch 97/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1747 - siamese_loss: 5.7263 - mlp_loss: 0.1747 - mlp_binary_accuracy: 0.9306 - mlp_auc_28: 0.9808\n",
      "Epoch 98/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1754 - siamese_loss: 5.7017 - mlp_loss: 0.1754 - mlp_binary_accuracy: 0.9350 - mlp_auc_28: 0.9802\n",
      "Epoch 99/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1749 - siamese_loss: 5.6881 - mlp_loss: 0.1749 - mlp_binary_accuracy: 0.9336 - mlp_auc_28: 0.9807\n",
      "Epoch 100/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1767 - siamese_loss: 5.7461 - mlp_loss: 0.1767 - mlp_binary_accuracy: 0.9290 - mlp_auc_28: 0.9808\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6438 - siamese_loss: 5.3624 - mlp_loss: 0.6438 - mlp_binary_accuracy: 0.8058 - mlp_auc_28: 0.8706    \n",
      "------ mlp_binary_accuracy: 80.58%\t ----- mlp_auc_28: 87.06%\n",
      "concat_fea的shape： (None, 112)\n",
      "-------------------------------------Kfold: 2 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6648 - siamese_loss: 3.1184 - mlp_loss: 0.6648 - mlp_binary_accuracy: 0.5915 - mlp_auc_29: 0.6306\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6168 - siamese_loss: 3.3041 - mlp_loss: 0.6168 - mlp_binary_accuracy: 0.6553 - mlp_auc_29: 0.7166\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5949 - siamese_loss: 3.5539 - mlp_loss: 0.5949 - mlp_binary_accuracy: 0.6797 - mlp_auc_29: 0.7450\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5750 - siamese_loss: 3.6358 - mlp_loss: 0.5750 - mlp_binary_accuracy: 0.7008 - mlp_auc_29: 0.7673\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5594 - siamese_loss: 3.6193 - mlp_loss: 0.5594 - mlp_binary_accuracy: 0.7070 - mlp_auc_29: 0.7825\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5541 - siamese_loss: 3.8086 - mlp_loss: 0.5541 - mlp_binary_accuracy: 0.7190 - mlp_auc_29: 0.7877\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5477 - siamese_loss: 3.8786 - mlp_loss: 0.5477 - mlp_binary_accuracy: 0.7178 - mlp_auc_29: 0.7936\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5382 - siamese_loss: 4.0698 - mlp_loss: 0.5382 - mlp_binary_accuracy: 0.7323 - mlp_auc_29: 0.8034\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5222 - siamese_loss: 4.0269 - mlp_loss: 0.5222 - mlp_binary_accuracy: 0.7456 - mlp_auc_29: 0.8184\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5121 - siamese_loss: 4.1885 - mlp_loss: 0.5121 - mlp_binary_accuracy: 0.7500 - mlp_auc_29: 0.8263\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5112 - siamese_loss: 4.0389 - mlp_loss: 0.5112 - mlp_binary_accuracy: 0.7509 - mlp_auc_29: 0.8267\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5011 - siamese_loss: 4.1419 - mlp_loss: 0.5011 - mlp_binary_accuracy: 0.7534 - mlp_auc_29: 0.8351\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4916 - siamese_loss: 4.2201 - mlp_loss: 0.4916 - mlp_binary_accuracy: 0.7617 - mlp_auc_29: 0.8430\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4814 - siamese_loss: 4.2989 - mlp_loss: 0.4814 - mlp_binary_accuracy: 0.7709 - mlp_auc_29: 0.8499\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4736 - siamese_loss: 4.3251 - mlp_loss: 0.4736 - mlp_binary_accuracy: 0.7762 - mlp_auc_29: 0.8561\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4762 - siamese_loss: 4.1412 - mlp_loss: 0.4762 - mlp_binary_accuracy: 0.7817 - mlp_auc_29: 0.8538\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4623 - siamese_loss: 4.2897 - mlp_loss: 0.4623 - mlp_binary_accuracy: 0.7856 - mlp_auc_29: 0.8635\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4698 - siamese_loss: 4.2680 - mlp_loss: 0.4698 - mlp_binary_accuracy: 0.7787 - mlp_auc_29: 0.8587\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4549 - siamese_loss: 4.6194 - mlp_loss: 0.4549 - mlp_binary_accuracy: 0.7879 - mlp_auc_29: 0.8682\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4394 - siamese_loss: 4.5868 - mlp_loss: 0.4394 - mlp_binary_accuracy: 0.7987 - mlp_auc_29: 0.8778\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4394 - siamese_loss: 4.7018 - mlp_loss: 0.4394 - mlp_binary_accuracy: 0.8010 - mlp_auc_29: 0.8781\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4241 - siamese_loss: 4.8907 - mlp_loss: 0.4241 - mlp_binary_accuracy: 0.8054 - mlp_auc_29: 0.8874\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4177 - siamese_loss: 4.9342 - mlp_loss: 0.4177 - mlp_binary_accuracy: 0.8130 - mlp_auc_29: 0.8911\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4219 - siamese_loss: 5.0431 - mlp_loss: 0.4219 - mlp_binary_accuracy: 0.8088 - mlp_auc_29: 0.8886\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4105 - siamese_loss: 5.0939 - mlp_loss: 0.4105 - mlp_binary_accuracy: 0.8136 - mlp_auc_29: 0.8944\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4024 - siamese_loss: 5.0062 - mlp_loss: 0.4024 - mlp_binary_accuracy: 0.8153 - mlp_auc_29: 0.8987\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4062 - siamese_loss: 4.9905 - mlp_loss: 0.4062 - mlp_binary_accuracy: 0.8155 - mlp_auc_29: 0.8967\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4056 - siamese_loss: 4.9706 - mlp_loss: 0.4056 - mlp_binary_accuracy: 0.8139 - mlp_auc_29: 0.8969\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4020 - siamese_loss: 5.1079 - mlp_loss: 0.4020 - mlp_binary_accuracy: 0.8171 - mlp_auc_29: 0.8992\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3973 - siamese_loss: 5.3274 - mlp_loss: 0.3973 - mlp_binary_accuracy: 0.8228 - mlp_auc_29: 0.9021\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3947 - siamese_loss: 5.2887 - mlp_loss: 0.3947 - mlp_binary_accuracy: 0.8226 - mlp_auc_29: 0.9029\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3926 - siamese_loss: 5.3361 - mlp_loss: 0.3926 - mlp_binary_accuracy: 0.8277 - mlp_auc_29: 0.9041\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3959 - siamese_loss: 5.1557 - mlp_loss: 0.3959 - mlp_binary_accuracy: 0.8235 - mlp_auc_29: 0.9023\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3794 - siamese_loss: 5.2282 - mlp_loss: 0.3794 - mlp_binary_accuracy: 0.8284 - mlp_auc_29: 0.9105\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3782 - siamese_loss: 5.1053 - mlp_loss: 0.3782 - mlp_binary_accuracy: 0.8334 - mlp_auc_29: 0.9110\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3777 - siamese_loss: 5.0675 - mlp_loss: 0.3777 - mlp_binary_accuracy: 0.8316 - mlp_auc_29: 0.9115\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3665 - siamese_loss: 5.1225 - mlp_loss: 0.3665 - mlp_binary_accuracy: 0.8398 - mlp_auc_29: 0.9168\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3612 - siamese_loss: 5.0661 - mlp_loss: 0.3612 - mlp_binary_accuracy: 0.8431 - mlp_auc_29: 0.9196\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3495 - siamese_loss: 5.1764 - mlp_loss: 0.3495 - mlp_binary_accuracy: 0.8479 - mlp_auc_29: 0.9244\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3439 - siamese_loss: 5.2204 - mlp_loss: 0.3439 - mlp_binary_accuracy: 0.8486 - mlp_auc_29: 0.9270\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3434 - siamese_loss: 5.2139 - mlp_loss: 0.3434 - mlp_binary_accuracy: 0.8509 - mlp_auc_29: 0.9273\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3363 - siamese_loss: 5.2496 - mlp_loss: 0.3363 - mlp_binary_accuracy: 0.8582 - mlp_auc_29: 0.9303\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3367 - siamese_loss: 5.1726 - mlp_loss: 0.3367 - mlp_binary_accuracy: 0.8555 - mlp_auc_29: 0.9302\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3347 - siamese_loss: 5.1836 - mlp_loss: 0.3347 - mlp_binary_accuracy: 0.8529 - mlp_auc_29: 0.9310\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3228 - siamese_loss: 5.2036 - mlp_loss: 0.3228 - mlp_binary_accuracy: 0.8628 - mlp_auc_29: 0.9359\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3248 - siamese_loss: 5.0708 - mlp_loss: 0.3248 - mlp_binary_accuracy: 0.8658 - mlp_auc_29: 0.9351\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3214 - siamese_loss: 5.0136 - mlp_loss: 0.3214 - mlp_binary_accuracy: 0.8573 - mlp_auc_29: 0.9365\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3249 - siamese_loss: 5.1780 - mlp_loss: 0.3249 - mlp_binary_accuracy: 0.8605 - mlp_auc_29: 0.9352\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3195 - siamese_loss: 5.2680 - mlp_loss: 0.3195 - mlp_binary_accuracy: 0.8626 - mlp_auc_29: 0.9369\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3088 - siamese_loss: 5.2235 - mlp_loss: 0.3088 - mlp_binary_accuracy: 0.8704 - mlp_auc_29: 0.9414\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3158 - siamese_loss: 5.2333 - mlp_loss: 0.3158 - mlp_binary_accuracy: 0.8640 - mlp_auc_29: 0.9387\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2936 - siamese_loss: 5.3699 - mlp_loss: 0.2936 - mlp_binary_accuracy: 0.8771 - mlp_auc_29: 0.9470\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2954 - siamese_loss: 5.3539 - mlp_loss: 0.2954 - mlp_binary_accuracy: 0.8729 - mlp_auc_29: 0.9466\n",
      "Epoch 54/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2878 - siamese_loss: 5.1233 - mlp_loss: 0.2878 - mlp_binary_accuracy: 0.8801 - mlp_auc_29: 0.9491\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2826 - siamese_loss: 5.2697 - mlp_loss: 0.2826 - mlp_binary_accuracy: 0.8844 - mlp_auc_29: 0.9511\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2854 - siamese_loss: 5.3001 - mlp_loss: 0.2854 - mlp_binary_accuracy: 0.8821 - mlp_auc_29: 0.9500\n",
      "Epoch 57/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2796 - siamese_loss: 5.4581 - mlp_loss: 0.2796 - mlp_binary_accuracy: 0.8858 - mlp_auc_29: 0.9517\n",
      "Epoch 58/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2749 - siamese_loss: 5.3304 - mlp_loss: 0.2749 - mlp_binary_accuracy: 0.8865 - mlp_auc_29: 0.9535\n",
      "Epoch 59/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2643 - siamese_loss: 5.4255 - mlp_loss: 0.2643 - mlp_binary_accuracy: 0.8938 - mlp_auc_29: 0.9569\n",
      "Epoch 60/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2654 - siamese_loss: 5.5138 - mlp_loss: 0.2654 - mlp_binary_accuracy: 0.8902 - mlp_auc_29: 0.9569\n",
      "Epoch 61/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2667 - siamese_loss: 5.4449 - mlp_loss: 0.2667 - mlp_binary_accuracy: 0.8842 - mlp_auc_29: 0.9564\n",
      "Epoch 62/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2664 - siamese_loss: 5.3278 - mlp_loss: 0.2664 - mlp_binary_accuracy: 0.8890 - mlp_auc_29: 0.9565\n",
      "Epoch 63/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2584 - siamese_loss: 5.3667 - mlp_loss: 0.2584 - mlp_binary_accuracy: 0.8938 - mlp_auc_29: 0.9589\n",
      "Epoch 64/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2509 - siamese_loss: 5.4918 - mlp_loss: 0.2509 - mlp_binary_accuracy: 0.8980 - mlp_auc_29: 0.9613\n",
      "Epoch 65/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2544 - siamese_loss: 5.5027 - mlp_loss: 0.2544 - mlp_binary_accuracy: 0.8943 - mlp_auc_29: 0.9600\n",
      "Epoch 66/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2450 - siamese_loss: 5.5259 - mlp_loss: 0.2450 - mlp_binary_accuracy: 0.9051 - mlp_auc_29: 0.9632\n",
      "Epoch 67/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2404 - siamese_loss: 5.4468 - mlp_loss: 0.2404 - mlp_binary_accuracy: 0.9049 - mlp_auc_29: 0.9645\n",
      "Epoch 68/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2591 - siamese_loss: 5.4359 - mlp_loss: 0.2591 - mlp_binary_accuracy: 0.8943 - mlp_auc_29: 0.9587\n",
      "Epoch 69/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2381 - siamese_loss: 5.4917 - mlp_loss: 0.2381 - mlp_binary_accuracy: 0.9028 - mlp_auc_29: 0.9651\n",
      "Epoch 70/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2348 - siamese_loss: 5.5102 - mlp_loss: 0.2348 - mlp_binary_accuracy: 0.9030 - mlp_auc_29: 0.9663\n",
      "Epoch 71/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2331 - siamese_loss: 5.5297 - mlp_loss: 0.2331 - mlp_binary_accuracy: 0.9056 - mlp_auc_29: 0.9668\n",
      "Epoch 72/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2314 - siamese_loss: 5.5377 - mlp_loss: 0.2314 - mlp_binary_accuracy: 0.9051 - mlp_auc_29: 0.9672\n",
      "Epoch 73/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2311 - siamese_loss: 5.6556 - mlp_loss: 0.2311 - mlp_binary_accuracy: 0.9060 - mlp_auc_29: 0.9672\n",
      "Epoch 74/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2279 - siamese_loss: 5.5873 - mlp_loss: 0.2279 - mlp_binary_accuracy: 0.9095 - mlp_auc_29: 0.9679\n",
      "Epoch 75/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2156 - siamese_loss: 5.4490 - mlp_loss: 0.2156 - mlp_binary_accuracy: 0.9127 - mlp_auc_29: 0.9713\n",
      "Epoch 76/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2217 - siamese_loss: 5.4881 - mlp_loss: 0.2217 - mlp_binary_accuracy: 0.9108 - mlp_auc_29: 0.9696\n",
      "Epoch 77/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2113 - siamese_loss: 5.5716 - mlp_loss: 0.2113 - mlp_binary_accuracy: 0.9180 - mlp_auc_29: 0.9724\n",
      "Epoch 78/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2102 - siamese_loss: 5.5584 - mlp_loss: 0.2102 - mlp_binary_accuracy: 0.9157 - mlp_auc_29: 0.9728\n",
      "Epoch 79/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2144 - siamese_loss: 5.6134 - mlp_loss: 0.2144 - mlp_binary_accuracy: 0.9159 - mlp_auc_29: 0.9718\n",
      "Epoch 80/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2131 - siamese_loss: 5.4863 - mlp_loss: 0.2131 - mlp_binary_accuracy: 0.9148 - mlp_auc_29: 0.9720\n",
      "Epoch 81/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2139 - siamese_loss: 5.5782 - mlp_loss: 0.2139 - mlp_binary_accuracy: 0.9150 - mlp_auc_29: 0.9719\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1988 - siamese_loss: 5.6634 - mlp_loss: 0.1988 - mlp_binary_accuracy: 0.9180 - mlp_auc_29: 0.9756\n",
      "Epoch 83/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2011 - siamese_loss: 5.6456 - mlp_loss: 0.2011 - mlp_binary_accuracy: 0.9191 - mlp_auc_29: 0.9753\n",
      "Epoch 84/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2063 - siamese_loss: 5.5725 - mlp_loss: 0.2063 - mlp_binary_accuracy: 0.9193 - mlp_auc_29: 0.9737\n",
      "Epoch 85/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1907 - siamese_loss: 5.5966 - mlp_loss: 0.1907 - mlp_binary_accuracy: 0.9216 - mlp_auc_29: 0.9778\n",
      "Epoch 86/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1972 - siamese_loss: 5.7423 - mlp_loss: 0.1972 - mlp_binary_accuracy: 0.9214 - mlp_auc_29: 0.9761\n",
      "Epoch 87/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1921 - siamese_loss: 5.7732 - mlp_loss: 0.1921 - mlp_binary_accuracy: 0.9242 - mlp_auc_29: 0.9774\n",
      "Epoch 88/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1975 - siamese_loss: 5.7363 - mlp_loss: 0.1975 - mlp_binary_accuracy: 0.9228 - mlp_auc_29: 0.9758\n",
      "Epoch 89/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2042 - siamese_loss: 5.7094 - mlp_loss: 0.2042 - mlp_binary_accuracy: 0.9164 - mlp_auc_29: 0.9744\n",
      "Epoch 90/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1779 - siamese_loss: 5.7322 - mlp_loss: 0.1779 - mlp_binary_accuracy: 0.9297 - mlp_auc_29: 0.9805\n",
      "Epoch 91/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1992 - siamese_loss: 5.8535 - mlp_loss: 0.1992 - mlp_binary_accuracy: 0.9216 - mlp_auc_29: 0.9754\n",
      "Epoch 92/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1903 - siamese_loss: 5.8874 - mlp_loss: 0.1903 - mlp_binary_accuracy: 0.9216 - mlp_auc_29: 0.9776\n",
      "Epoch 93/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1838 - siamese_loss: 5.8342 - mlp_loss: 0.1838 - mlp_binary_accuracy: 0.9242 - mlp_auc_29: 0.9793\n",
      "Epoch 94/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1716 - siamese_loss: 5.7850 - mlp_loss: 0.1716 - mlp_binary_accuracy: 0.9256 - mlp_auc_29: 0.9820\n",
      "Epoch 95/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1794 - siamese_loss: 5.7292 - mlp_loss: 0.1794 - mlp_binary_accuracy: 0.9281 - mlp_auc_29: 0.9802\n",
      "Epoch 96/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1834 - siamese_loss: 5.8800 - mlp_loss: 0.1834 - mlp_binary_accuracy: 0.9258 - mlp_auc_29: 0.9796\n",
      "Epoch 97/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1679 - siamese_loss: 5.7832 - mlp_loss: 0.1679 - mlp_binary_accuracy: 0.9334 - mlp_auc_29: 0.9824\n",
      "Epoch 98/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1718 - siamese_loss: 5.8394 - mlp_loss: 0.1718 - mlp_binary_accuracy: 0.9336 - mlp_auc_29: 0.9816\n",
      "Epoch 99/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1786 - siamese_loss: 5.8788 - mlp_loss: 0.1786 - mlp_binary_accuracy: 0.9295 - mlp_auc_29: 0.9802\n",
      "Epoch 100/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1699 - siamese_loss: 5.9261 - mlp_loss: 0.1699 - mlp_binary_accuracy: 0.9352 - mlp_auc_29: 0.9823\n",
      "Epoch 101/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1599 - siamese_loss: 5.8648 - mlp_loss: 0.1599 - mlp_binary_accuracy: 0.9354 - mlp_auc_29: 0.9842\n",
      "Epoch 102/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1591 - siamese_loss: 5.8373 - mlp_loss: 0.1591 - mlp_binary_accuracy: 0.9361 - mlp_auc_29: 0.9844\n",
      "Epoch 103/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1679 - siamese_loss: 5.7869 - mlp_loss: 0.1679 - mlp_binary_accuracy: 0.9361 - mlp_auc_29: 0.9824\n",
      "Epoch 104/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1657 - siamese_loss: 5.8541 - mlp_loss: 0.1657 - mlp_binary_accuracy: 0.9347 - mlp_auc_29: 0.9830\n",
      "Epoch 105/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1561 - siamese_loss: 5.7778 - mlp_loss: 0.1561 - mlp_binary_accuracy: 0.9352 - mlp_auc_29: 0.9851\n",
      "Epoch 106/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1626 - siamese_loss: 5.8161 - mlp_loss: 0.1626 - mlp_binary_accuracy: 0.9320 - mlp_auc_29: 0.9839\n",
      "Epoch 107/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1608 - siamese_loss: 5.7727 - mlp_loss: 0.1608 - mlp_binary_accuracy: 0.9354 - mlp_auc_29: 0.9841\n",
      "Epoch 108/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1473 - siamese_loss: 5.9174 - mlp_loss: 0.1473 - mlp_binary_accuracy: 0.9430 - mlp_auc_29: 0.9864\n",
      "Epoch 109/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1528 - siamese_loss: 6.0072 - mlp_loss: 0.1528 - mlp_binary_accuracy: 0.9409 - mlp_auc_29: 0.9855\n",
      "Epoch 110/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1617 - siamese_loss: 5.7685 - mlp_loss: 0.1617 - mlp_binary_accuracy: 0.9364 - mlp_auc_29: 0.9840\n",
      "Epoch 111/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1504 - siamese_loss: 5.9904 - mlp_loss: 0.1504 - mlp_binary_accuracy: 0.9416 - mlp_auc_29: 0.9858\n",
      "Epoch 112/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1480 - siamese_loss: 5.9966 - mlp_loss: 0.1480 - mlp_binary_accuracy: 0.9423 - mlp_auc_29: 0.9863\n",
      "Epoch 113/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1372 - siamese_loss: 5.8699 - mlp_loss: 0.1372 - mlp_binary_accuracy: 0.9460 - mlp_auc_29: 0.9879\n",
      "Epoch 114/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1520 - siamese_loss: 5.9456 - mlp_loss: 0.1520 - mlp_binary_accuracy: 0.9437 - mlp_auc_29: 0.9854\n",
      "Epoch 115/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1496 - siamese_loss: 5.8838 - mlp_loss: 0.1496 - mlp_binary_accuracy: 0.9407 - mlp_auc_29: 0.9860\n",
      "Epoch 116/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1432 - siamese_loss: 5.8413 - mlp_loss: 0.1432 - mlp_binary_accuracy: 0.9460 - mlp_auc_29: 0.9869\n",
      "Epoch 117/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1387 - siamese_loss: 5.8684 - mlp_loss: 0.1387 - mlp_binary_accuracy: 0.9476 - mlp_auc_29: 0.9877\n",
      "Epoch 118/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1416 - siamese_loss: 5.8549 - mlp_loss: 0.1416 - mlp_binary_accuracy: 0.9458 - mlp_auc_29: 0.9872\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6858 - siamese_loss: 6.0153 - mlp_loss: 0.6858 - mlp_binary_accuracy: 0.7996 - mlp_auc_29: 0.8846    \n",
      "------ mlp_binary_accuracy: 79.96%\t ----- mlp_auc_29: 88.46%\n",
      "concat_fea的shape： (None, 112)\n",
      "-------------------------------------Kfold: 3 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6638 - siamese_loss: 3.6928 - mlp_loss: 0.6638 - mlp_binary_accuracy: 0.5873 - mlp_auc_30: 0.6279\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6125 - siamese_loss: 3.9982 - mlp_loss: 0.6125 - mlp_binary_accuracy: 0.6627 - mlp_auc_30: 0.7200\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5817 - siamese_loss: 4.2980 - mlp_loss: 0.5817 - mlp_binary_accuracy: 0.6990 - mlp_auc_30: 0.7596\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5637 - siamese_loss: 4.2671 - mlp_loss: 0.5637 - mlp_binary_accuracy: 0.7084 - mlp_auc_30: 0.7789\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5475 - siamese_loss: 4.3467 - mlp_loss: 0.5475 - mlp_binary_accuracy: 0.7206 - mlp_auc_30: 0.7955\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5369 - siamese_loss: 4.4146 - mlp_loss: 0.5369 - mlp_binary_accuracy: 0.7337 - mlp_auc_30: 0.8059\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5252 - siamese_loss: 4.4780 - mlp_loss: 0.5252 - mlp_binary_accuracy: 0.7403 - mlp_auc_30: 0.8150\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5208 - siamese_loss: 4.3078 - mlp_loss: 0.5208 - mlp_binary_accuracy: 0.7408 - mlp_auc_30: 0.8180\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4992 - siamese_loss: 4.3387 - mlp_loss: 0.4992 - mlp_binary_accuracy: 0.7571 - mlp_auc_30: 0.8351\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4984 - siamese_loss: 4.3289 - mlp_loss: 0.4984 - mlp_binary_accuracy: 0.7640 - mlp_auc_30: 0.8380\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4781 - siamese_loss: 4.5291 - mlp_loss: 0.4781 - mlp_binary_accuracy: 0.7730 - mlp_auc_30: 0.8516\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4731 - siamese_loss: 4.5633 - mlp_loss: 0.4731 - mlp_binary_accuracy: 0.7764 - mlp_auc_30: 0.8552\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4667 - siamese_loss: 4.4020 - mlp_loss: 0.4667 - mlp_binary_accuracy: 0.7801 - mlp_auc_30: 0.8598\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4611 - siamese_loss: 4.6407 - mlp_loss: 0.4611 - mlp_binary_accuracy: 0.7840 - mlp_auc_30: 0.8646\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4537 - siamese_loss: 4.7577 - mlp_loss: 0.4537 - mlp_binary_accuracy: 0.7932 - mlp_auc_30: 0.8684\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4441 - siamese_loss: 4.7519 - mlp_loss: 0.4441 - mlp_binary_accuracy: 0.7907 - mlp_auc_30: 0.8750\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4270 - siamese_loss: 4.8668 - mlp_loss: 0.4270 - mlp_binary_accuracy: 0.8074 - mlp_auc_30: 0.8845\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4264 - siamese_loss: 4.7800 - mlp_loss: 0.4264 - mlp_binary_accuracy: 0.8047 - mlp_auc_30: 0.8856\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4180 - siamese_loss: 4.9644 - mlp_loss: 0.4180 - mlp_binary_accuracy: 0.8136 - mlp_auc_30: 0.8907\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4103 - siamese_loss: 4.8594 - mlp_loss: 0.4103 - mlp_binary_accuracy: 0.8182 - mlp_auc_30: 0.8949\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4035 - siamese_loss: 4.8653 - mlp_loss: 0.4035 - mlp_binary_accuracy: 0.8146 - mlp_auc_30: 0.8984\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3963 - siamese_loss: 4.9530 - mlp_loss: 0.3963 - mlp_binary_accuracy: 0.8189 - mlp_auc_30: 0.9019\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3830 - siamese_loss: 4.9882 - mlp_loss: 0.3830 - mlp_binary_accuracy: 0.8334 - mlp_auc_30: 0.9094\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3780 - siamese_loss: 5.0339 - mlp_loss: 0.3780 - mlp_binary_accuracy: 0.8401 - mlp_auc_30: 0.9111\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3869 - siamese_loss: 4.9731 - mlp_loss: 0.3869 - mlp_binary_accuracy: 0.8334 - mlp_auc_30: 0.9068\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3684 - siamese_loss: 4.9947 - mlp_loss: 0.3684 - mlp_binary_accuracy: 0.8396 - mlp_auc_30: 0.9161\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3665 - siamese_loss: 4.9874 - mlp_loss: 0.3665 - mlp_binary_accuracy: 0.8380 - mlp_auc_30: 0.9169\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3601 - siamese_loss: 4.8143 - mlp_loss: 0.3601 - mlp_binary_accuracy: 0.8474 - mlp_auc_30: 0.9201\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3450 - siamese_loss: 4.7957 - mlp_loss: 0.3450 - mlp_binary_accuracy: 0.8529 - mlp_auc_30: 0.9268\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3545 - siamese_loss: 4.9424 - mlp_loss: 0.3545 - mlp_binary_accuracy: 0.8463 - mlp_auc_30: 0.9226\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3469 - siamese_loss: 5.0741 - mlp_loss: 0.3469 - mlp_binary_accuracy: 0.8518 - mlp_auc_30: 0.9260\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3375 - siamese_loss: 5.1469 - mlp_loss: 0.3375 - mlp_binary_accuracy: 0.8523 - mlp_auc_30: 0.9301\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3362 - siamese_loss: 5.2072 - mlp_loss: 0.3362 - mlp_binary_accuracy: 0.8578 - mlp_auc_30: 0.9305\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3288 - siamese_loss: 5.1608 - mlp_loss: 0.3288 - mlp_binary_accuracy: 0.8679 - mlp_auc_30: 0.9334\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3186 - siamese_loss: 5.0058 - mlp_loss: 0.3186 - mlp_binary_accuracy: 0.8601 - mlp_auc_30: 0.9377\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3172 - siamese_loss: 5.0869 - mlp_loss: 0.3172 - mlp_binary_accuracy: 0.8688 - mlp_auc_30: 0.9383\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3147 - siamese_loss: 5.1168 - mlp_loss: 0.3147 - mlp_binary_accuracy: 0.8752 - mlp_auc_30: 0.9390\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3044 - siamese_loss: 5.0659 - mlp_loss: 0.3044 - mlp_binary_accuracy: 0.8745 - mlp_auc_30: 0.9429\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2918 - siamese_loss: 5.1315 - mlp_loss: 0.2918 - mlp_binary_accuracy: 0.8778 - mlp_auc_30: 0.9478\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2830 - siamese_loss: 5.0866 - mlp_loss: 0.2830 - mlp_binary_accuracy: 0.8835 - mlp_auc_30: 0.9509\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2916 - siamese_loss: 5.0309 - mlp_loss: 0.2916 - mlp_binary_accuracy: 0.8803 - mlp_auc_30: 0.9479\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2777 - siamese_loss: 5.1015 - mlp_loss: 0.2777 - mlp_binary_accuracy: 0.8849 - mlp_auc_30: 0.9529\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2824 - siamese_loss: 5.2003 - mlp_loss: 0.2824 - mlp_binary_accuracy: 0.8814 - mlp_auc_30: 0.9516\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2782 - siamese_loss: 5.1900 - mlp_loss: 0.2782 - mlp_binary_accuracy: 0.8849 - mlp_auc_30: 0.9528\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2669 - siamese_loss: 5.1031 - mlp_loss: 0.2669 - mlp_binary_accuracy: 0.8872 - mlp_auc_30: 0.9563\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2645 - siamese_loss: 5.0942 - mlp_loss: 0.2645 - mlp_binary_accuracy: 0.8938 - mlp_auc_30: 0.9574\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2547 - siamese_loss: 5.1563 - mlp_loss: 0.2547 - mlp_binary_accuracy: 0.8984 - mlp_auc_30: 0.9599\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2489 - siamese_loss: 5.1846 - mlp_loss: 0.2489 - mlp_binary_accuracy: 0.8998 - mlp_auc_30: 0.9621\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2531 - siamese_loss: 5.1847 - mlp_loss: 0.2531 - mlp_binary_accuracy: 0.8984 - mlp_auc_30: 0.9606\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2560 - siamese_loss: 5.1607 - mlp_loss: 0.2560 - mlp_binary_accuracy: 0.8925 - mlp_auc_30: 0.9596\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2502 - siamese_loss: 5.1591 - mlp_loss: 0.2502 - mlp_binary_accuracy: 0.8971 - mlp_auc_30: 0.9615\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2532 - siamese_loss: 5.1705 - mlp_loss: 0.2532 - mlp_binary_accuracy: 0.8961 - mlp_auc_30: 0.9608\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2484 - siamese_loss: 5.2246 - mlp_loss: 0.2484 - mlp_binary_accuracy: 0.8955 - mlp_auc_30: 0.9622\n",
      "Epoch 54/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2343 - siamese_loss: 5.2532 - mlp_loss: 0.2343 - mlp_binary_accuracy: 0.9074 - mlp_auc_30: 0.9659\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2324 - siamese_loss: 5.1972 - mlp_loss: 0.2324 - mlp_binary_accuracy: 0.9051 - mlp_auc_30: 0.9669\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2354 - siamese_loss: 5.1732 - mlp_loss: 0.2354 - mlp_binary_accuracy: 0.8998 - mlp_auc_30: 0.9663\n",
      "Epoch 57/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2232 - siamese_loss: 5.2765 - mlp_loss: 0.2232 - mlp_binary_accuracy: 0.9104 - mlp_auc_30: 0.9694\n",
      "Epoch 58/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2150 - siamese_loss: 5.0724 - mlp_loss: 0.2150 - mlp_binary_accuracy: 0.9150 - mlp_auc_30: 0.9716\n",
      "Epoch 59/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2275 - siamese_loss: 5.1042 - mlp_loss: 0.2275 - mlp_binary_accuracy: 0.9058 - mlp_auc_30: 0.9685\n",
      "Epoch 60/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2205 - siamese_loss: 5.0656 - mlp_loss: 0.2205 - mlp_binary_accuracy: 0.9108 - mlp_auc_30: 0.9704\n",
      "Epoch 61/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2122 - siamese_loss: 5.0528 - mlp_loss: 0.2122 - mlp_binary_accuracy: 0.9157 - mlp_auc_30: 0.9719\n",
      "Epoch 62/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2189 - siamese_loss: 5.0622 - mlp_loss: 0.2189 - mlp_binary_accuracy: 0.9113 - mlp_auc_30: 0.9707\n",
      "Epoch 63/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2073 - siamese_loss: 5.0543 - mlp_loss: 0.2073 - mlp_binary_accuracy: 0.9187 - mlp_auc_30: 0.9737\n",
      "Epoch 64/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1967 - siamese_loss: 5.0939 - mlp_loss: 0.1967 - mlp_binary_accuracy: 0.9196 - mlp_auc_30: 0.9760\n",
      "Epoch 65/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1997 - siamese_loss: 4.9956 - mlp_loss: 0.1997 - mlp_binary_accuracy: 0.9214 - mlp_auc_30: 0.9752\n",
      "Epoch 66/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2018 - siamese_loss: 5.0481 - mlp_loss: 0.2018 - mlp_binary_accuracy: 0.9182 - mlp_auc_30: 0.9751\n",
      "Epoch 67/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1847 - siamese_loss: 5.1134 - mlp_loss: 0.1847 - mlp_binary_accuracy: 0.9258 - mlp_auc_30: 0.9788\n",
      "Epoch 68/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1826 - siamese_loss: 5.1093 - mlp_loss: 0.1826 - mlp_binary_accuracy: 0.9269 - mlp_auc_30: 0.9794\n",
      "Epoch 69/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1994 - siamese_loss: 5.1161 - mlp_loss: 0.1994 - mlp_binary_accuracy: 0.9267 - mlp_auc_30: 0.9748\n",
      "Epoch 70/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1931 - siamese_loss: 5.1584 - mlp_loss: 0.1931 - mlp_binary_accuracy: 0.9267 - mlp_auc_30: 0.9770\n",
      "Epoch 71/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1824 - siamese_loss: 5.1066 - mlp_loss: 0.1824 - mlp_binary_accuracy: 0.9324 - mlp_auc_30: 0.9793\n",
      "Epoch 72/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1954 - siamese_loss: 5.0544 - mlp_loss: 0.1954 - mlp_binary_accuracy: 0.9244 - mlp_auc_30: 0.9762\n",
      "Epoch 73/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2013 - siamese_loss: 5.1063 - mlp_loss: 0.2013 - mlp_binary_accuracy: 0.9235 - mlp_auc_30: 0.9747\n",
      "Epoch 74/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1946 - siamese_loss: 5.1278 - mlp_loss: 0.1946 - mlp_binary_accuracy: 0.9233 - mlp_auc_30: 0.9767\n",
      "Epoch 75/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1908 - siamese_loss: 5.0473 - mlp_loss: 0.1908 - mlp_binary_accuracy: 0.9272 - mlp_auc_30: 0.9772\n",
      "Epoch 76/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1866 - siamese_loss: 5.1717 - mlp_loss: 0.1866 - mlp_binary_accuracy: 0.9292 - mlp_auc_30: 0.9785\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6709 - siamese_loss: 5.0733 - mlp_loss: 0.6709 - mlp_binary_accuracy: 0.7996 - mlp_auc_30: 0.8682    \n",
      "------ mlp_binary_accuracy: 79.96%\t ----- mlp_auc_30: 86.82%\n",
      "concat_fea的shape： (None, 112)\n",
      "-------------------------------------Kfold: 4 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6721 - siamese_loss: 2.9426 - mlp_loss: 0.6721 - mlp_binary_accuracy: 0.5857 - mlp_auc_31: 0.6179\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6241 - siamese_loss: 3.4944 - mlp_loss: 0.6241 - mlp_binary_accuracy: 0.6530 - mlp_auc_31: 0.7066\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5908 - siamese_loss: 3.5324 - mlp_loss: 0.5908 - mlp_binary_accuracy: 0.6877 - mlp_auc_31: 0.7506\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5779 - siamese_loss: 3.4513 - mlp_loss: 0.5779 - mlp_binary_accuracy: 0.7001 - mlp_auc_31: 0.7649\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5688 - siamese_loss: 3.5808 - mlp_loss: 0.5688 - mlp_binary_accuracy: 0.7027 - mlp_auc_31: 0.7743\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5549 - siamese_loss: 3.6027 - mlp_loss: 0.5549 - mlp_binary_accuracy: 0.7213 - mlp_auc_31: 0.7875\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5372 - siamese_loss: 3.5582 - mlp_loss: 0.5372 - mlp_binary_accuracy: 0.7314 - mlp_auc_31: 0.8049\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5342 - siamese_loss: 3.5661 - mlp_loss: 0.5342 - mlp_binary_accuracy: 0.7399 - mlp_auc_31: 0.8080\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5210 - siamese_loss: 3.7300 - mlp_loss: 0.5210 - mlp_binary_accuracy: 0.7484 - mlp_auc_31: 0.8196\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5183 - siamese_loss: 3.8617 - mlp_loss: 0.5183 - mlp_binary_accuracy: 0.7461 - mlp_auc_31: 0.8216\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5063 - siamese_loss: 3.8986 - mlp_loss: 0.5063 - mlp_binary_accuracy: 0.7592 - mlp_auc_31: 0.8320\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4967 - siamese_loss: 3.9998 - mlp_loss: 0.4967 - mlp_binary_accuracy: 0.7652 - mlp_auc_31: 0.8392\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4889 - siamese_loss: 4.3255 - mlp_loss: 0.4889 - mlp_binary_accuracy: 0.7698 - mlp_auc_31: 0.8451\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4749 - siamese_loss: 4.4379 - mlp_loss: 0.4749 - mlp_binary_accuracy: 0.7778 - mlp_auc_31: 0.8548\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4678 - siamese_loss: 4.4126 - mlp_loss: 0.4678 - mlp_binary_accuracy: 0.7831 - mlp_auc_31: 0.8597\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4571 - siamese_loss: 4.4641 - mlp_loss: 0.4571 - mlp_binary_accuracy: 0.7875 - mlp_auc_31: 0.8670\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4545 - siamese_loss: 4.4543 - mlp_loss: 0.4545 - mlp_binary_accuracy: 0.7930 - mlp_auc_31: 0.8687\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4510 - siamese_loss: 4.4909 - mlp_loss: 0.4510 - mlp_binary_accuracy: 0.7902 - mlp_auc_31: 0.8707\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4414 - siamese_loss: 4.3545 - mlp_loss: 0.4414 - mlp_binary_accuracy: 0.8001 - mlp_auc_31: 0.8770\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4325 - siamese_loss: 4.4776 - mlp_loss: 0.4325 - mlp_binary_accuracy: 0.8081 - mlp_auc_31: 0.8821\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4293 - siamese_loss: 4.4828 - mlp_loss: 0.4293 - mlp_binary_accuracy: 0.8100 - mlp_auc_31: 0.8842\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4302 - siamese_loss: 4.6007 - mlp_loss: 0.4302 - mlp_binary_accuracy: 0.8033 - mlp_auc_31: 0.8835\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4213 - siamese_loss: 4.5880 - mlp_loss: 0.4213 - mlp_binary_accuracy: 0.8116 - mlp_auc_31: 0.8886\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4080 - siamese_loss: 4.7806 - mlp_loss: 0.4080 - mlp_binary_accuracy: 0.8203 - mlp_auc_31: 0.8961\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4077 - siamese_loss: 4.8150 - mlp_loss: 0.4077 - mlp_binary_accuracy: 0.8192 - mlp_auc_31: 0.8967\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4028 - siamese_loss: 4.7680 - mlp_loss: 0.4028 - mlp_binary_accuracy: 0.8187 - mlp_auc_31: 0.8987\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3871 - siamese_loss: 4.8550 - mlp_loss: 0.3871 - mlp_binary_accuracy: 0.8311 - mlp_auc_31: 0.9070\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3946 - siamese_loss: 4.9174 - mlp_loss: 0.3946 - mlp_binary_accuracy: 0.8212 - mlp_auc_31: 0.9034\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3974 - siamese_loss: 4.9750 - mlp_loss: 0.3974 - mlp_binary_accuracy: 0.8249 - mlp_auc_31: 0.9018\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3756 - siamese_loss: 4.9130 - mlp_loss: 0.3756 - mlp_binary_accuracy: 0.8355 - mlp_auc_31: 0.9127\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3781 - siamese_loss: 5.1113 - mlp_loss: 0.3781 - mlp_binary_accuracy: 0.8359 - mlp_auc_31: 0.9116\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3665 - siamese_loss: 5.1296 - mlp_loss: 0.3665 - mlp_binary_accuracy: 0.8392 - mlp_auc_31: 0.9173\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3711 - siamese_loss: 5.1156 - mlp_loss: 0.3711 - mlp_binary_accuracy: 0.8410 - mlp_auc_31: 0.9146\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3527 - siamese_loss: 5.2384 - mlp_loss: 0.3527 - mlp_binary_accuracy: 0.8465 - mlp_auc_31: 0.9231\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3491 - siamese_loss: 5.2984 - mlp_loss: 0.3491 - mlp_binary_accuracy: 0.8493 - mlp_auc_31: 0.9246\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3451 - siamese_loss: 5.0966 - mlp_loss: 0.3451 - mlp_binary_accuracy: 0.8500 - mlp_auc_31: 0.9266\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3369 - siamese_loss: 5.1335 - mlp_loss: 0.3369 - mlp_binary_accuracy: 0.8559 - mlp_auc_31: 0.9300\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3425 - siamese_loss: 5.1299 - mlp_loss: 0.3425 - mlp_binary_accuracy: 0.8536 - mlp_auc_31: 0.9278\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3379 - siamese_loss: 5.1504 - mlp_loss: 0.3379 - mlp_binary_accuracy: 0.8571 - mlp_auc_31: 0.9296\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3254 - siamese_loss: 5.2998 - mlp_loss: 0.3254 - mlp_binary_accuracy: 0.8640 - mlp_auc_31: 0.9349\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3283 - siamese_loss: 5.2985 - mlp_loss: 0.3283 - mlp_binary_accuracy: 0.8575 - mlp_auc_31: 0.9338\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3175 - siamese_loss: 5.3488 - mlp_loss: 0.3175 - mlp_binary_accuracy: 0.8631 - mlp_auc_31: 0.9383\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3203 - siamese_loss: 5.2490 - mlp_loss: 0.3203 - mlp_binary_accuracy: 0.8711 - mlp_auc_31: 0.9367\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3125 - siamese_loss: 5.2183 - mlp_loss: 0.3125 - mlp_binary_accuracy: 0.8693 - mlp_auc_31: 0.9400\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3107 - siamese_loss: 5.2936 - mlp_loss: 0.3107 - mlp_binary_accuracy: 0.8709 - mlp_auc_31: 0.9401\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3020 - siamese_loss: 5.3095 - mlp_loss: 0.3020 - mlp_binary_accuracy: 0.8736 - mlp_auc_31: 0.9442\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2964 - siamese_loss: 5.3926 - mlp_loss: 0.2964 - mlp_binary_accuracy: 0.8741 - mlp_auc_31: 0.9461\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2945 - siamese_loss: 5.4536 - mlp_loss: 0.2945 - mlp_binary_accuracy: 0.8798 - mlp_auc_31: 0.9467\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2958 - siamese_loss: 5.4197 - mlp_loss: 0.2958 - mlp_binary_accuracy: 0.8787 - mlp_auc_31: 0.9459\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2880 - siamese_loss: 5.3208 - mlp_loss: 0.2880 - mlp_binary_accuracy: 0.8798 - mlp_auc_31: 0.9484\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2884 - siamese_loss: 5.4212 - mlp_loss: 0.2884 - mlp_binary_accuracy: 0.8821 - mlp_auc_31: 0.9487\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2842 - siamese_loss: 5.4226 - mlp_loss: 0.2842 - mlp_binary_accuracy: 0.8833 - mlp_auc_31: 0.9503\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2752 - siamese_loss: 5.2595 - mlp_loss: 0.2752 - mlp_binary_accuracy: 0.8872 - mlp_auc_31: 0.9533\n",
      "Epoch 54/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2818 - siamese_loss: 5.1793 - mlp_loss: 0.2818 - mlp_binary_accuracy: 0.8814 - mlp_auc_31: 0.9510\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2821 - siamese_loss: 5.2292 - mlp_loss: 0.2821 - mlp_binary_accuracy: 0.8844 - mlp_auc_31: 0.9509\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2716 - siamese_loss: 5.2574 - mlp_loss: 0.2716 - mlp_binary_accuracy: 0.8892 - mlp_auc_31: 0.9542\n",
      "Epoch 57/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2596 - siamese_loss: 5.3915 - mlp_loss: 0.2596 - mlp_binary_accuracy: 0.8987 - mlp_auc_31: 0.9580\n",
      "Epoch 58/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2718 - siamese_loss: 5.4265 - mlp_loss: 0.2718 - mlp_binary_accuracy: 0.8911 - mlp_auc_31: 0.9543\n",
      "Epoch 59/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2572 - siamese_loss: 5.4681 - mlp_loss: 0.2572 - mlp_binary_accuracy: 0.8955 - mlp_auc_31: 0.9592\n",
      "Epoch 60/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2512 - siamese_loss: 5.4566 - mlp_loss: 0.2512 - mlp_binary_accuracy: 0.8996 - mlp_auc_31: 0.9603\n",
      "Epoch 61/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2518 - siamese_loss: 5.3214 - mlp_loss: 0.2518 - mlp_binary_accuracy: 0.8982 - mlp_auc_31: 0.9603\n",
      "Epoch 62/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2495 - siamese_loss: 5.3563 - mlp_loss: 0.2495 - mlp_binary_accuracy: 0.8980 - mlp_auc_31: 0.9611\n",
      "Epoch 63/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2482 - siamese_loss: 5.4155 - mlp_loss: 0.2482 - mlp_binary_accuracy: 0.9012 - mlp_auc_31: 0.9618\n",
      "Epoch 64/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2451 - siamese_loss: 5.4797 - mlp_loss: 0.2451 - mlp_binary_accuracy: 0.9051 - mlp_auc_31: 0.9627\n",
      "Epoch 65/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2370 - siamese_loss: 5.4619 - mlp_loss: 0.2370 - mlp_binary_accuracy: 0.9079 - mlp_auc_31: 0.9648\n",
      "Epoch 66/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2442 - siamese_loss: 5.3782 - mlp_loss: 0.2442 - mlp_binary_accuracy: 0.9005 - mlp_auc_31: 0.9627\n",
      "Epoch 67/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2483 - siamese_loss: 5.5602 - mlp_loss: 0.2483 - mlp_binary_accuracy: 0.9012 - mlp_auc_31: 0.9612\n",
      "Epoch 68/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2511 - siamese_loss: 5.5255 - mlp_loss: 0.2511 - mlp_binary_accuracy: 0.9007 - mlp_auc_31: 0.9610\n",
      "Epoch 69/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2393 - siamese_loss: 5.5284 - mlp_loss: 0.2393 - mlp_binary_accuracy: 0.9053 - mlp_auc_31: 0.9642\n",
      "Epoch 70/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2402 - siamese_loss: 5.5064 - mlp_loss: 0.2402 - mlp_binary_accuracy: 0.9060 - mlp_auc_31: 0.9635\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5333 - siamese_loss: 5.6822 - mlp_loss: 0.5333 - mlp_binary_accuracy: 0.7789 - mlp_auc_31: 0.8739    \n",
      "------ mlp_binary_accuracy: 77.89%\t ----- mlp_auc_31: 87.39%\n",
      "concat_fea的shape： (None, 112)\n",
      "-------------------------------------Kfold: 5 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6773 - siamese_loss: 3.1640 - mlp_loss: 0.6773 - mlp_binary_accuracy: 0.5742 - mlp_auc_32: 0.6146\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6364 - siamese_loss: 3.0908 - mlp_loss: 0.6364 - mlp_binary_accuracy: 0.6388 - mlp_auc_32: 0.6931\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6066 - siamese_loss: 3.1473 - mlp_loss: 0.6066 - mlp_binary_accuracy: 0.6719 - mlp_auc_32: 0.7346\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5847 - siamese_loss: 3.4249 - mlp_loss: 0.5847 - mlp_binary_accuracy: 0.6893 - mlp_auc_32: 0.7581\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5647 - siamese_loss: 3.6878 - mlp_loss: 0.5647 - mlp_binary_accuracy: 0.7158 - mlp_auc_32: 0.7802\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5599 - siamese_loss: 3.8117 - mlp_loss: 0.5599 - mlp_binary_accuracy: 0.7102 - mlp_auc_32: 0.7845\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5445 - siamese_loss: 3.8556 - mlp_loss: 0.5445 - mlp_binary_accuracy: 0.7263 - mlp_auc_32: 0.7989\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5328 - siamese_loss: 3.9615 - mlp_loss: 0.5328 - mlp_binary_accuracy: 0.7378 - mlp_auc_32: 0.8096\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5184 - siamese_loss: 3.9042 - mlp_loss: 0.5184 - mlp_binary_accuracy: 0.7466 - mlp_auc_32: 0.8215\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5102 - siamese_loss: 4.0376 - mlp_loss: 0.5102 - mlp_binary_accuracy: 0.7500 - mlp_auc_32: 0.8278\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5044 - siamese_loss: 4.0409 - mlp_loss: 0.5044 - mlp_binary_accuracy: 0.7578 - mlp_auc_32: 0.8329\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4899 - siamese_loss: 4.2143 - mlp_loss: 0.4899 - mlp_binary_accuracy: 0.7615 - mlp_auc_32: 0.8434\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4884 - siamese_loss: 4.1628 - mlp_loss: 0.4884 - mlp_binary_accuracy: 0.7679 - mlp_auc_32: 0.8447\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4796 - siamese_loss: 4.2088 - mlp_loss: 0.4796 - mlp_binary_accuracy: 0.7654 - mlp_auc_32: 0.8502\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4715 - siamese_loss: 4.0061 - mlp_loss: 0.4715 - mlp_binary_accuracy: 0.7748 - mlp_auc_32: 0.8571\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4680 - siamese_loss: 4.1271 - mlp_loss: 0.4680 - mlp_binary_accuracy: 0.7757 - mlp_auc_32: 0.8586\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4622 - siamese_loss: 4.2371 - mlp_loss: 0.4622 - mlp_binary_accuracy: 0.7815 - mlp_auc_32: 0.8637\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4423 - siamese_loss: 4.1396 - mlp_loss: 0.4423 - mlp_binary_accuracy: 0.7916 - mlp_auc_32: 0.8751\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4488 - siamese_loss: 4.2498 - mlp_loss: 0.4488 - mlp_binary_accuracy: 0.7870 - mlp_auc_32: 0.8712\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4299 - siamese_loss: 4.3223 - mlp_loss: 0.4299 - mlp_binary_accuracy: 0.8072 - mlp_auc_32: 0.8833\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4298 - siamese_loss: 4.3164 - mlp_loss: 0.4298 - mlp_binary_accuracy: 0.8022 - mlp_auc_32: 0.8831\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4289 - siamese_loss: 4.3332 - mlp_loss: 0.4289 - mlp_binary_accuracy: 0.8045 - mlp_auc_32: 0.8837\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4106 - siamese_loss: 4.4629 - mlp_loss: 0.4106 - mlp_binary_accuracy: 0.8141 - mlp_auc_32: 0.8944\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4129 - siamese_loss: 4.4096 - mlp_loss: 0.4129 - mlp_binary_accuracy: 0.8134 - mlp_auc_32: 0.8931\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4038 - siamese_loss: 4.4473 - mlp_loss: 0.4038 - mlp_binary_accuracy: 0.8261 - mlp_auc_32: 0.8983\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3924 - siamese_loss: 4.4044 - mlp_loss: 0.3924 - mlp_binary_accuracy: 0.8277 - mlp_auc_32: 0.9042\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3911 - siamese_loss: 4.4274 - mlp_loss: 0.3911 - mlp_binary_accuracy: 0.8272 - mlp_auc_32: 0.9046\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3872 - siamese_loss: 4.3767 - mlp_loss: 0.3872 - mlp_binary_accuracy: 0.8277 - mlp_auc_32: 0.9069\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3811 - siamese_loss: 4.4495 - mlp_loss: 0.3811 - mlp_binary_accuracy: 0.8316 - mlp_auc_32: 0.9090\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3673 - siamese_loss: 4.4458 - mlp_loss: 0.3673 - mlp_binary_accuracy: 0.8378 - mlp_auc_32: 0.9160\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3588 - siamese_loss: 4.3636 - mlp_loss: 0.3588 - mlp_binary_accuracy: 0.8463 - mlp_auc_32: 0.9198\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3535 - siamese_loss: 4.4238 - mlp_loss: 0.3535 - mlp_binary_accuracy: 0.8456 - mlp_auc_32: 0.9226\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3519 - siamese_loss: 4.3615 - mlp_loss: 0.3519 - mlp_binary_accuracy: 0.8490 - mlp_auc_32: 0.9229\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3478 - siamese_loss: 4.2976 - mlp_loss: 0.3478 - mlp_binary_accuracy: 0.8412 - mlp_auc_32: 0.9256\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3431 - siamese_loss: 4.5477 - mlp_loss: 0.3431 - mlp_binary_accuracy: 0.8504 - mlp_auc_32: 0.9270\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3444 - siamese_loss: 4.5619 - mlp_loss: 0.3444 - mlp_binary_accuracy: 0.8545 - mlp_auc_32: 0.9264\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3330 - siamese_loss: 4.5380 - mlp_loss: 0.3330 - mlp_binary_accuracy: 0.8596 - mlp_auc_32: 0.9314\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3306 - siamese_loss: 4.5042 - mlp_loss: 0.3306 - mlp_binary_accuracy: 0.8587 - mlp_auc_32: 0.9326\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3194 - siamese_loss: 4.5020 - mlp_loss: 0.3194 - mlp_binary_accuracy: 0.8566 - mlp_auc_32: 0.9369\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3111 - siamese_loss: 4.5667 - mlp_loss: 0.3111 - mlp_binary_accuracy: 0.8651 - mlp_auc_32: 0.9403\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3131 - siamese_loss: 4.5280 - mlp_loss: 0.3131 - mlp_binary_accuracy: 0.8697 - mlp_auc_32: 0.9395\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3053 - siamese_loss: 4.5785 - mlp_loss: 0.3053 - mlp_binary_accuracy: 0.8690 - mlp_auc_32: 0.9427\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3104 - siamese_loss: 4.5458 - mlp_loss: 0.3104 - mlp_binary_accuracy: 0.8741 - mlp_auc_32: 0.9410\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2998 - siamese_loss: 4.4330 - mlp_loss: 0.2998 - mlp_binary_accuracy: 0.8713 - mlp_auc_32: 0.9451\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2911 - siamese_loss: 4.4981 - mlp_loss: 0.2911 - mlp_binary_accuracy: 0.8773 - mlp_auc_32: 0.9480\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2988 - siamese_loss: 4.6013 - mlp_loss: 0.2988 - mlp_binary_accuracy: 0.8734 - mlp_auc_32: 0.9450\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2929 - siamese_loss: 4.7419 - mlp_loss: 0.2929 - mlp_binary_accuracy: 0.8782 - mlp_auc_32: 0.9476\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2886 - siamese_loss: 4.5767 - mlp_loss: 0.2886 - mlp_binary_accuracy: 0.8704 - mlp_auc_32: 0.9489\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2758 - siamese_loss: 4.6274 - mlp_loss: 0.2758 - mlp_binary_accuracy: 0.8851 - mlp_auc_32: 0.9533\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2736 - siamese_loss: 4.6975 - mlp_loss: 0.2736 - mlp_binary_accuracy: 0.8899 - mlp_auc_32: 0.9543\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2743 - siamese_loss: 4.6935 - mlp_loss: 0.2743 - mlp_binary_accuracy: 0.8867 - mlp_auc_32: 0.9540\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2720 - siamese_loss: 4.7725 - mlp_loss: 0.2720 - mlp_binary_accuracy: 0.8844 - mlp_auc_32: 0.9548\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2670 - siamese_loss: 4.6312 - mlp_loss: 0.2670 - mlp_binary_accuracy: 0.8904 - mlp_auc_32: 0.9562\n",
      "Epoch 54/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2621 - siamese_loss: 4.7156 - mlp_loss: 0.2621 - mlp_binary_accuracy: 0.8911 - mlp_auc_32: 0.9578\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2686 - siamese_loss: 4.7734 - mlp_loss: 0.2686 - mlp_binary_accuracy: 0.8911 - mlp_auc_32: 0.9552\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2636 - siamese_loss: 4.8506 - mlp_loss: 0.2636 - mlp_binary_accuracy: 0.8883 - mlp_auc_32: 0.9576\n",
      "Epoch 57/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2572 - siamese_loss: 4.9302 - mlp_loss: 0.2572 - mlp_binary_accuracy: 0.8950 - mlp_auc_32: 0.9594\n",
      "Epoch 58/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2647 - siamese_loss: 4.9198 - mlp_loss: 0.2647 - mlp_binary_accuracy: 0.8911 - mlp_auc_32: 0.9571\n",
      "Epoch 59/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2593 - siamese_loss: 4.9241 - mlp_loss: 0.2593 - mlp_binary_accuracy: 0.8874 - mlp_auc_32: 0.9589\n",
      "Epoch 60/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2478 - siamese_loss: 4.8734 - mlp_loss: 0.2478 - mlp_binary_accuracy: 0.8984 - mlp_auc_32: 0.9623\n",
      "Epoch 61/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2331 - siamese_loss: 4.9982 - mlp_loss: 0.2331 - mlp_binary_accuracy: 0.8998 - mlp_auc_32: 0.9666\n",
      "Epoch 62/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2358 - siamese_loss: 4.9677 - mlp_loss: 0.2358 - mlp_binary_accuracy: 0.9051 - mlp_auc_32: 0.9656\n",
      "Epoch 63/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2209 - siamese_loss: 4.9463 - mlp_loss: 0.2209 - mlp_binary_accuracy: 0.9085 - mlp_auc_32: 0.9697\n",
      "Epoch 64/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2335 - siamese_loss: 4.9461 - mlp_loss: 0.2335 - mlp_binary_accuracy: 0.9035 - mlp_auc_32: 0.9667\n",
      "Epoch 65/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2281 - siamese_loss: 4.8679 - mlp_loss: 0.2281 - mlp_binary_accuracy: 0.9067 - mlp_auc_32: 0.9680\n",
      "Epoch 66/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2192 - siamese_loss: 4.9277 - mlp_loss: 0.2192 - mlp_binary_accuracy: 0.9085 - mlp_auc_32: 0.9708\n",
      "Epoch 67/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2262 - siamese_loss: 4.9552 - mlp_loss: 0.2262 - mlp_binary_accuracy: 0.9085 - mlp_auc_32: 0.9686\n",
      "Epoch 68/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2170 - siamese_loss: 5.0404 - mlp_loss: 0.2170 - mlp_binary_accuracy: 0.9072 - mlp_auc_32: 0.9712\n",
      "Epoch 69/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2164 - siamese_loss: 4.9703 - mlp_loss: 0.2164 - mlp_binary_accuracy: 0.9049 - mlp_auc_32: 0.9714\n",
      "Epoch 70/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2044 - siamese_loss: 4.9576 - mlp_loss: 0.2044 - mlp_binary_accuracy: 0.9166 - mlp_auc_32: 0.9739\n",
      "Epoch 71/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2082 - siamese_loss: 4.8243 - mlp_loss: 0.2082 - mlp_binary_accuracy: 0.9111 - mlp_auc_32: 0.9735\n",
      "Epoch 72/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2092 - siamese_loss: 4.9027 - mlp_loss: 0.2092 - mlp_binary_accuracy: 0.9166 - mlp_auc_32: 0.9731\n",
      "Epoch 73/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1935 - siamese_loss: 4.8692 - mlp_loss: 0.1935 - mlp_binary_accuracy: 0.9182 - mlp_auc_32: 0.9769\n",
      "Epoch 74/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2055 - siamese_loss: 5.0064 - mlp_loss: 0.2055 - mlp_binary_accuracy: 0.9148 - mlp_auc_32: 0.9742\n",
      "Epoch 75/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2007 - siamese_loss: 5.0821 - mlp_loss: 0.2007 - mlp_binary_accuracy: 0.9203 - mlp_auc_32: 0.9750\n",
      "Epoch 76/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1979 - siamese_loss: 5.1355 - mlp_loss: 0.1979 - mlp_binary_accuracy: 0.9207 - mlp_auc_32: 0.9761\n",
      "Epoch 77/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1926 - siamese_loss: 4.9658 - mlp_loss: 0.1926 - mlp_binary_accuracy: 0.9216 - mlp_auc_32: 0.9770\n",
      "Epoch 78/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1902 - siamese_loss: 4.9287 - mlp_loss: 0.1902 - mlp_binary_accuracy: 0.9265 - mlp_auc_32: 0.9772\n",
      "Epoch 79/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1934 - siamese_loss: 5.0003 - mlp_loss: 0.1934 - mlp_binary_accuracy: 0.9235 - mlp_auc_32: 0.9770\n",
      "Epoch 80/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1825 - siamese_loss: 5.0067 - mlp_loss: 0.1825 - mlp_binary_accuracy: 0.9246 - mlp_auc_32: 0.9795\n",
      "Epoch 81/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1848 - siamese_loss: 5.0096 - mlp_loss: 0.1848 - mlp_binary_accuracy: 0.9297 - mlp_auc_32: 0.9787\n",
      "Epoch 82/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1891 - siamese_loss: 4.9819 - mlp_loss: 0.1891 - mlp_binary_accuracy: 0.9258 - mlp_auc_32: 0.9781\n",
      "Epoch 83/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1780 - siamese_loss: 5.0637 - mlp_loss: 0.1780 - mlp_binary_accuracy: 0.9281 - mlp_auc_32: 0.9802\n",
      "Epoch 84/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1793 - siamese_loss: 5.1057 - mlp_loss: 0.1793 - mlp_binary_accuracy: 0.9308 - mlp_auc_32: 0.9796\n",
      "Epoch 85/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1678 - siamese_loss: 5.1157 - mlp_loss: 0.1678 - mlp_binary_accuracy: 0.9361 - mlp_auc_32: 0.9826\n",
      "Epoch 86/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1677 - siamese_loss: 5.1377 - mlp_loss: 0.1677 - mlp_binary_accuracy: 0.9299 - mlp_auc_32: 0.9825\n",
      "Epoch 87/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1711 - siamese_loss: 5.0802 - mlp_loss: 0.1711 - mlp_binary_accuracy: 0.9322 - mlp_auc_32: 0.9820\n",
      "Epoch 88/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1673 - siamese_loss: 4.9936 - mlp_loss: 0.1673 - mlp_binary_accuracy: 0.9341 - mlp_auc_32: 0.9826\n",
      "Epoch 89/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1475 - siamese_loss: 5.0945 - mlp_loss: 0.1475 - mlp_binary_accuracy: 0.9407 - mlp_auc_32: 0.9865\n",
      "Epoch 90/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1572 - siamese_loss: 5.2353 - mlp_loss: 0.1572 - mlp_binary_accuracy: 0.9391 - mlp_auc_32: 0.9842\n",
      "Epoch 91/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1644 - siamese_loss: 5.2240 - mlp_loss: 0.1644 - mlp_binary_accuracy: 0.9368 - mlp_auc_32: 0.9833\n",
      "Epoch 92/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1628 - siamese_loss: 5.1961 - mlp_loss: 0.1628 - mlp_binary_accuracy: 0.9368 - mlp_auc_32: 0.9835\n",
      "Epoch 93/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1619 - siamese_loss: 5.1981 - mlp_loss: 0.1619 - mlp_binary_accuracy: 0.9318 - mlp_auc_32: 0.9838\n",
      "Epoch 94/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1535 - siamese_loss: 5.0956 - mlp_loss: 0.1535 - mlp_binary_accuracy: 0.9426 - mlp_auc_32: 0.9849\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6246 - siamese_loss: 5.0842 - mlp_loss: 0.6246 - mlp_binary_accuracy: 0.8079 - mlp_auc_32: 0.8860    \n",
      "------ mlp_binary_accuracy: 80.79%\t ----- mlp_auc_32: 88.60%\n",
      "concat_fea的shape： (None, 112)\n",
      "-------------------------------------Kfold: 6 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6726 - siamese_loss: 3.0719 - mlp_loss: 0.6726 - mlp_binary_accuracy: 0.5790 - mlp_auc_33: 0.6125\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6226 - siamese_loss: 3.3505 - mlp_loss: 0.6226 - mlp_binary_accuracy: 0.6487 - mlp_auc_33: 0.7107\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5937 - siamese_loss: 3.6292 - mlp_loss: 0.5937 - mlp_binary_accuracy: 0.6820 - mlp_auc_33: 0.7477\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5753 - siamese_loss: 3.7707 - mlp_loss: 0.5753 - mlp_binary_accuracy: 0.7061 - mlp_auc_33: 0.7685\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5609 - siamese_loss: 3.8381 - mlp_loss: 0.5609 - mlp_binary_accuracy: 0.7100 - mlp_auc_33: 0.7827\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5505 - siamese_loss: 4.1178 - mlp_loss: 0.5505 - mlp_binary_accuracy: 0.7222 - mlp_auc_33: 0.7930\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5417 - siamese_loss: 4.0907 - mlp_loss: 0.5417 - mlp_binary_accuracy: 0.7330 - mlp_auc_33: 0.8012\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5351 - siamese_loss: 3.9267 - mlp_loss: 0.5351 - mlp_binary_accuracy: 0.7390 - mlp_auc_33: 0.8071\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5202 - siamese_loss: 4.0859 - mlp_loss: 0.5202 - mlp_binary_accuracy: 0.7433 - mlp_auc_33: 0.8199\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5163 - siamese_loss: 4.3972 - mlp_loss: 0.5163 - mlp_binary_accuracy: 0.7468 - mlp_auc_33: 0.8240\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5069 - siamese_loss: 4.4707 - mlp_loss: 0.5069 - mlp_binary_accuracy: 0.7518 - mlp_auc_33: 0.8302\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4972 - siamese_loss: 4.3537 - mlp_loss: 0.4972 - mlp_binary_accuracy: 0.7562 - mlp_auc_33: 0.8371\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4934 - siamese_loss: 4.5182 - mlp_loss: 0.4934 - mlp_binary_accuracy: 0.7597 - mlp_auc_33: 0.8409\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4739 - siamese_loss: 4.5542 - mlp_loss: 0.4739 - mlp_binary_accuracy: 0.7727 - mlp_auc_33: 0.8546\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4666 - siamese_loss: 4.6074 - mlp_loss: 0.4666 - mlp_binary_accuracy: 0.7716 - mlp_auc_33: 0.8589\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4562 - siamese_loss: 4.5046 - mlp_loss: 0.4562 - mlp_binary_accuracy: 0.7898 - mlp_auc_33: 0.8657\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4531 - siamese_loss: 4.5345 - mlp_loss: 0.4531 - mlp_binary_accuracy: 0.7900 - mlp_auc_33: 0.8686\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4486 - siamese_loss: 4.5366 - mlp_loss: 0.4486 - mlp_binary_accuracy: 0.7835 - mlp_auc_33: 0.8702\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4385 - siamese_loss: 4.5002 - mlp_loss: 0.4385 - mlp_binary_accuracy: 0.7971 - mlp_auc_33: 0.8776\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4359 - siamese_loss: 4.6270 - mlp_loss: 0.4359 - mlp_binary_accuracy: 0.7976 - mlp_auc_33: 0.8791\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4294 - siamese_loss: 4.5220 - mlp_loss: 0.4294 - mlp_binary_accuracy: 0.8006 - mlp_auc_33: 0.8829\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4239 - siamese_loss: 4.6139 - mlp_loss: 0.4239 - mlp_binary_accuracy: 0.8040 - mlp_auc_33: 0.8857\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4168 - siamese_loss: 4.6992 - mlp_loss: 0.4168 - mlp_binary_accuracy: 0.8047 - mlp_auc_33: 0.8899\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.4218 - siamese_loss: 5.0235 - mlp_loss: 0.4218 - mlp_binary_accuracy: 0.8049 - mlp_auc_33: 0.8869\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.4059 - siamese_loss: 4.9701 - mlp_loss: 0.4059 - mlp_binary_accuracy: 0.8136 - mlp_auc_33: 0.8957\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3990 - siamese_loss: 4.8037 - mlp_loss: 0.3990 - mlp_binary_accuracy: 0.8194 - mlp_auc_33: 0.8992\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3976 - siamese_loss: 4.8357 - mlp_loss: 0.3976 - mlp_binary_accuracy: 0.8228 - mlp_auc_33: 0.9009\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3847 - siamese_loss: 4.8895 - mlp_loss: 0.3847 - mlp_binary_accuracy: 0.8270 - mlp_auc_33: 0.9076\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3846 - siamese_loss: 4.7898 - mlp_loss: 0.3846 - mlp_binary_accuracy: 0.8281 - mlp_auc_33: 0.9071\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3767 - siamese_loss: 4.7260 - mlp_loss: 0.3767 - mlp_binary_accuracy: 0.8313 - mlp_auc_33: 0.9119\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3764 - siamese_loss: 4.8144 - mlp_loss: 0.3764 - mlp_binary_accuracy: 0.8279 - mlp_auc_33: 0.9117\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3667 - siamese_loss: 4.9329 - mlp_loss: 0.3667 - mlp_binary_accuracy: 0.8359 - mlp_auc_33: 0.9163\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3559 - siamese_loss: 4.8942 - mlp_loss: 0.3559 - mlp_binary_accuracy: 0.8428 - mlp_auc_33: 0.9213\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3606 - siamese_loss: 4.8939 - mlp_loss: 0.3606 - mlp_binary_accuracy: 0.8431 - mlp_auc_33: 0.9193\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3441 - siamese_loss: 4.9483 - mlp_loss: 0.3441 - mlp_binary_accuracy: 0.8493 - mlp_auc_33: 0.9267\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3292 - siamese_loss: 4.8759 - mlp_loss: 0.3292 - mlp_binary_accuracy: 0.8575 - mlp_auc_33: 0.9329\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3353 - siamese_loss: 4.7971 - mlp_loss: 0.3353 - mlp_binary_accuracy: 0.8562 - mlp_auc_33: 0.9305\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3217 - siamese_loss: 4.7396 - mlp_loss: 0.3217 - mlp_binary_accuracy: 0.8635 - mlp_auc_33: 0.9356\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3330 - siamese_loss: 4.7327 - mlp_loss: 0.3330 - mlp_binary_accuracy: 0.8573 - mlp_auc_33: 0.9309\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3229 - siamese_loss: 4.7972 - mlp_loss: 0.3229 - mlp_binary_accuracy: 0.8619 - mlp_auc_33: 0.9353\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3145 - siamese_loss: 4.9062 - mlp_loss: 0.3145 - mlp_binary_accuracy: 0.8697 - mlp_auc_33: 0.9389\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2916 - siamese_loss: 4.9056 - mlp_loss: 0.2916 - mlp_binary_accuracy: 0.8750 - mlp_auc_33: 0.9476\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3122 - siamese_loss: 4.8656 - mlp_loss: 0.3122 - mlp_binary_accuracy: 0.8676 - mlp_auc_33: 0.9400\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3013 - siamese_loss: 4.7412 - mlp_loss: 0.3013 - mlp_binary_accuracy: 0.8741 - mlp_auc_33: 0.9445\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2982 - siamese_loss: 4.9698 - mlp_loss: 0.2982 - mlp_binary_accuracy: 0.8690 - mlp_auc_33: 0.9450\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2967 - siamese_loss: 4.9347 - mlp_loss: 0.2967 - mlp_binary_accuracy: 0.8752 - mlp_auc_33: 0.9455\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2795 - siamese_loss: 4.9226 - mlp_loss: 0.2795 - mlp_binary_accuracy: 0.8895 - mlp_auc_33: 0.9520\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2873 - siamese_loss: 4.8518 - mlp_loss: 0.2873 - mlp_binary_accuracy: 0.8787 - mlp_auc_33: 0.9494\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2831 - siamese_loss: 4.9648 - mlp_loss: 0.2831 - mlp_binary_accuracy: 0.8824 - mlp_auc_33: 0.9507\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2739 - siamese_loss: 4.9691 - mlp_loss: 0.2739 - mlp_binary_accuracy: 0.8858 - mlp_auc_33: 0.9541\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2748 - siamese_loss: 4.9727 - mlp_loss: 0.2748 - mlp_binary_accuracy: 0.8922 - mlp_auc_33: 0.9536\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2708 - siamese_loss: 4.9270 - mlp_loss: 0.2708 - mlp_binary_accuracy: 0.8899 - mlp_auc_33: 0.9547\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2675 - siamese_loss: 4.9936 - mlp_loss: 0.2675 - mlp_binary_accuracy: 0.8851 - mlp_auc_33: 0.9566\n",
      "Epoch 54/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2575 - siamese_loss: 4.9922 - mlp_loss: 0.2575 - mlp_binary_accuracy: 0.8982 - mlp_auc_33: 0.9588\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2652 - siamese_loss: 5.0189 - mlp_loss: 0.2652 - mlp_binary_accuracy: 0.8911 - mlp_auc_33: 0.9568\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2612 - siamese_loss: 5.0463 - mlp_loss: 0.2612 - mlp_binary_accuracy: 0.8945 - mlp_auc_33: 0.9578\n",
      "Epoch 57/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2483 - siamese_loss: 5.0345 - mlp_loss: 0.2483 - mlp_binary_accuracy: 0.8975 - mlp_auc_33: 0.9620\n",
      "Epoch 58/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2463 - siamese_loss: 4.9476 - mlp_loss: 0.2463 - mlp_binary_accuracy: 0.9040 - mlp_auc_33: 0.9625\n",
      "Epoch 59/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2497 - siamese_loss: 4.9065 - mlp_loss: 0.2497 - mlp_binary_accuracy: 0.8998 - mlp_auc_33: 0.9611\n",
      "Epoch 60/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2404 - siamese_loss: 5.1690 - mlp_loss: 0.2404 - mlp_binary_accuracy: 0.9010 - mlp_auc_33: 0.9643\n",
      "Epoch 61/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2440 - siamese_loss: 5.1088 - mlp_loss: 0.2440 - mlp_binary_accuracy: 0.8982 - mlp_auc_33: 0.9636\n",
      "Epoch 62/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2393 - siamese_loss: 5.1203 - mlp_loss: 0.2393 - mlp_binary_accuracy: 0.9005 - mlp_auc_33: 0.9645\n",
      "Epoch 63/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2402 - siamese_loss: 5.2412 - mlp_loss: 0.2402 - mlp_binary_accuracy: 0.9040 - mlp_auc_33: 0.9639\n",
      "Epoch 64/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2310 - siamese_loss: 5.0822 - mlp_loss: 0.2310 - mlp_binary_accuracy: 0.9104 - mlp_auc_33: 0.9666\n",
      "Epoch 65/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2326 - siamese_loss: 5.2122 - mlp_loss: 0.2326 - mlp_binary_accuracy: 0.9056 - mlp_auc_33: 0.9664\n",
      "Epoch 66/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2319 - siamese_loss: 5.1579 - mlp_loss: 0.2319 - mlp_binary_accuracy: 0.9097 - mlp_auc_33: 0.9666\n",
      "Epoch 67/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2128 - siamese_loss: 5.2740 - mlp_loss: 0.2128 - mlp_binary_accuracy: 0.9187 - mlp_auc_33: 0.9710\n",
      "Epoch 68/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2217 - siamese_loss: 5.1566 - mlp_loss: 0.2217 - mlp_binary_accuracy: 0.9106 - mlp_auc_33: 0.9696\n",
      "Epoch 69/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2112 - siamese_loss: 5.1336 - mlp_loss: 0.2112 - mlp_binary_accuracy: 0.9184 - mlp_auc_33: 0.9722\n",
      "Epoch 70/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2035 - siamese_loss: 5.0807 - mlp_loss: 0.2035 - mlp_binary_accuracy: 0.9170 - mlp_auc_33: 0.9749\n",
      "Epoch 71/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2012 - siamese_loss: 5.1359 - mlp_loss: 0.2012 - mlp_binary_accuracy: 0.9237 - mlp_auc_33: 0.9745\n",
      "Epoch 72/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2073 - siamese_loss: 5.1881 - mlp_loss: 0.2073 - mlp_binary_accuracy: 0.9189 - mlp_auc_33: 0.9734\n",
      "Epoch 73/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1907 - siamese_loss: 5.1181 - mlp_loss: 0.1907 - mlp_binary_accuracy: 0.9278 - mlp_auc_33: 0.9773\n",
      "Epoch 74/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2025 - siamese_loss: 5.1874 - mlp_loss: 0.2025 - mlp_binary_accuracy: 0.9216 - mlp_auc_33: 0.9743\n",
      "Epoch 75/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2025 - siamese_loss: 5.0516 - mlp_loss: 0.2025 - mlp_binary_accuracy: 0.9184 - mlp_auc_33: 0.9739\n",
      "Epoch 76/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1969 - siamese_loss: 5.1404 - mlp_loss: 0.1969 - mlp_binary_accuracy: 0.9253 - mlp_auc_33: 0.9751\n",
      "Epoch 77/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1940 - siamese_loss: 5.1607 - mlp_loss: 0.1940 - mlp_binary_accuracy: 0.9221 - mlp_auc_33: 0.9764\n",
      "Epoch 78/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.2005 - siamese_loss: 5.2248 - mlp_loss: 0.2005 - mlp_binary_accuracy: 0.9292 - mlp_auc_33: 0.9752\n",
      "16/16 [==============================] - 0s 454us/step - loss: 0.4968 - siamese_loss: 5.4923 - mlp_loss: 0.4968 - mlp_binary_accuracy: 0.8202 - mlp_auc_33: 0.8974  \n",
      "------ mlp_binary_accuracy: 82.02%\t ----- mlp_auc_33: 89.74%\n",
      "concat_fea的shape： (None, 112)\n",
      "-------------------------------------Kfold: 7 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6686 - siamese_loss: 3.0286 - mlp_loss: 0.6686 - mlp_binary_accuracy: 0.5886 - mlp_auc_34: 0.6337\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6243 - siamese_loss: 3.1919 - mlp_loss: 0.6243 - mlp_binary_accuracy: 0.6595 - mlp_auc_34: 0.7119\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5978 - siamese_loss: 3.2390 - mlp_loss: 0.5978 - mlp_binary_accuracy: 0.6754 - mlp_auc_34: 0.7436\n",
      "Epoch 4/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5758 - siamese_loss: 3.3178 - mlp_loss: 0.5758 - mlp_binary_accuracy: 0.6952 - mlp_auc_34: 0.7685\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5602 - siamese_loss: 3.4514 - mlp_loss: 0.5602 - mlp_binary_accuracy: 0.7140 - mlp_auc_34: 0.7836\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5427 - siamese_loss: 3.8450 - mlp_loss: 0.5427 - mlp_binary_accuracy: 0.7264 - mlp_auc_34: 0.7998\n",
      "Epoch 7/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5354 - siamese_loss: 4.0985 - mlp_loss: 0.5354 - mlp_binary_accuracy: 0.7310 - mlp_auc_34: 0.8070\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5264 - siamese_loss: 4.3390 - mlp_loss: 0.5264 - mlp_binary_accuracy: 0.7335 - mlp_auc_34: 0.8137\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5107 - siamese_loss: 4.2755 - mlp_loss: 0.5107 - mlp_binary_accuracy: 0.7478 - mlp_auc_34: 0.8267\n",
      "Epoch 10/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5081 - siamese_loss: 4.4291 - mlp_loss: 0.5081 - mlp_binary_accuracy: 0.7528 - mlp_auc_34: 0.8303\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4895 - siamese_loss: 4.5030 - mlp_loss: 0.4895 - mlp_binary_accuracy: 0.7659 - mlp_auc_34: 0.8443\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4861 - siamese_loss: 4.3988 - mlp_loss: 0.4861 - mlp_binary_accuracy: 0.7675 - mlp_auc_34: 0.8465\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4734 - siamese_loss: 4.4717 - mlp_loss: 0.4734 - mlp_binary_accuracy: 0.7797 - mlp_auc_34: 0.8561\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4647 - siamese_loss: 4.4071 - mlp_loss: 0.4647 - mlp_binary_accuracy: 0.7847 - mlp_auc_34: 0.8615\n",
      "Epoch 15/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4484 - siamese_loss: 4.4533 - mlp_loss: 0.4484 - mlp_binary_accuracy: 0.7983 - mlp_auc_34: 0.8723\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4424 - siamese_loss: 4.3950 - mlp_loss: 0.4424 - mlp_binary_accuracy: 0.7930 - mlp_auc_34: 0.8757\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4419 - siamese_loss: 4.4479 - mlp_loss: 0.4419 - mlp_binary_accuracy: 0.8013 - mlp_auc_34: 0.8766\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4433 - siamese_loss: 4.7618 - mlp_loss: 0.4433 - mlp_binary_accuracy: 0.7992 - mlp_auc_34: 0.8755\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4301 - siamese_loss: 4.6742 - mlp_loss: 0.4301 - mlp_binary_accuracy: 0.8038 - mlp_auc_34: 0.8832\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4206 - siamese_loss: 4.4649 - mlp_loss: 0.4206 - mlp_binary_accuracy: 0.8086 - mlp_auc_34: 0.8886\n",
      "Epoch 21/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4187 - siamese_loss: 4.4572 - mlp_loss: 0.4187 - mlp_binary_accuracy: 0.8084 - mlp_auc_34: 0.8900\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4076 - siamese_loss: 4.6115 - mlp_loss: 0.4076 - mlp_binary_accuracy: 0.8137 - mlp_auc_34: 0.8961\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4114 - siamese_loss: 4.3340 - mlp_loss: 0.4114 - mlp_binary_accuracy: 0.8197 - mlp_auc_34: 0.8939\n",
      "Epoch 24/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3922 - siamese_loss: 4.4001 - mlp_loss: 0.3922 - mlp_binary_accuracy: 0.8233 - mlp_auc_34: 0.9043\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3906 - siamese_loss: 4.4650 - mlp_loss: 0.3906 - mlp_binary_accuracy: 0.8224 - mlp_auc_34: 0.9046\n",
      "Epoch 26/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3804 - siamese_loss: 4.6224 - mlp_loss: 0.3804 - mlp_binary_accuracy: 0.8364 - mlp_auc_34: 0.9102\n",
      "Epoch 27/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3769 - siamese_loss: 4.5263 - mlp_loss: 0.3769 - mlp_binary_accuracy: 0.8362 - mlp_auc_34: 0.9117\n",
      "Epoch 28/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3745 - siamese_loss: 4.5684 - mlp_loss: 0.3745 - mlp_binary_accuracy: 0.8318 - mlp_auc_34: 0.9132\n",
      "Epoch 29/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3634 - siamese_loss: 4.5913 - mlp_loss: 0.3634 - mlp_binary_accuracy: 0.8415 - mlp_auc_34: 0.9185\n",
      "Epoch 30/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3613 - siamese_loss: 4.5452 - mlp_loss: 0.3613 - mlp_binary_accuracy: 0.8429 - mlp_auc_34: 0.9193\n",
      "Epoch 31/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3537 - siamese_loss: 4.5260 - mlp_loss: 0.3537 - mlp_binary_accuracy: 0.8465 - mlp_auc_34: 0.9228\n",
      "Epoch 32/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3530 - siamese_loss: 4.5649 - mlp_loss: 0.3530 - mlp_binary_accuracy: 0.8449 - mlp_auc_34: 0.9226\n",
      "Epoch 33/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3519 - siamese_loss: 4.8080 - mlp_loss: 0.3519 - mlp_binary_accuracy: 0.8454 - mlp_auc_34: 0.9235\n",
      "Epoch 34/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3415 - siamese_loss: 4.9417 - mlp_loss: 0.3415 - mlp_binary_accuracy: 0.8472 - mlp_auc_34: 0.9281\n",
      "Epoch 35/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3382 - siamese_loss: 4.8647 - mlp_loss: 0.3382 - mlp_binary_accuracy: 0.8573 - mlp_auc_34: 0.9299\n",
      "Epoch 36/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3419 - siamese_loss: 5.0131 - mlp_loss: 0.3419 - mlp_binary_accuracy: 0.8560 - mlp_auc_34: 0.9284\n",
      "Epoch 37/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3223 - siamese_loss: 5.0114 - mlp_loss: 0.3223 - mlp_binary_accuracy: 0.8603 - mlp_auc_34: 0.9361\n",
      "Epoch 38/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3248 - siamese_loss: 4.8280 - mlp_loss: 0.3248 - mlp_binary_accuracy: 0.8608 - mlp_auc_34: 0.9350\n",
      "Epoch 39/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3165 - siamese_loss: 4.8075 - mlp_loss: 0.3165 - mlp_binary_accuracy: 0.8684 - mlp_auc_34: 0.9384\n",
      "Epoch 40/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3218 - siamese_loss: 4.8129 - mlp_loss: 0.3218 - mlp_binary_accuracy: 0.8619 - mlp_auc_34: 0.9362\n",
      "Epoch 41/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3156 - siamese_loss: 4.7722 - mlp_loss: 0.3156 - mlp_binary_accuracy: 0.8617 - mlp_auc_34: 0.9383\n",
      "Epoch 42/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3075 - siamese_loss: 4.9057 - mlp_loss: 0.3075 - mlp_binary_accuracy: 0.8730 - mlp_auc_34: 0.9421\n",
      "Epoch 43/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2939 - siamese_loss: 4.9668 - mlp_loss: 0.2939 - mlp_binary_accuracy: 0.8746 - mlp_auc_34: 0.9471\n",
      "Epoch 44/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2916 - siamese_loss: 4.9816 - mlp_loss: 0.2916 - mlp_binary_accuracy: 0.8743 - mlp_auc_34: 0.9480\n",
      "Epoch 45/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2931 - siamese_loss: 4.9544 - mlp_loss: 0.2931 - mlp_binary_accuracy: 0.8720 - mlp_auc_34: 0.9476\n",
      "Epoch 46/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3017 - siamese_loss: 4.8938 - mlp_loss: 0.3017 - mlp_binary_accuracy: 0.8725 - mlp_auc_34: 0.9440\n",
      "Epoch 47/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2893 - siamese_loss: 4.8129 - mlp_loss: 0.2893 - mlp_binary_accuracy: 0.8794 - mlp_auc_34: 0.9487\n",
      "Epoch 48/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2773 - siamese_loss: 4.8510 - mlp_loss: 0.2773 - mlp_binary_accuracy: 0.8828 - mlp_auc_34: 0.9530\n",
      "Epoch 49/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2828 - siamese_loss: 5.0178 - mlp_loss: 0.2828 - mlp_binary_accuracy: 0.8782 - mlp_auc_34: 0.9513\n",
      "Epoch 50/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2900 - siamese_loss: 5.0360 - mlp_loss: 0.2900 - mlp_binary_accuracy: 0.8771 - mlp_auc_34: 0.9484\n",
      "Epoch 51/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2793 - siamese_loss: 5.0313 - mlp_loss: 0.2793 - mlp_binary_accuracy: 0.8819 - mlp_auc_34: 0.9519\n",
      "Epoch 52/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2765 - siamese_loss: 5.0023 - mlp_loss: 0.2765 - mlp_binary_accuracy: 0.8780 - mlp_auc_34: 0.9532\n",
      "Epoch 53/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2752 - siamese_loss: 5.0271 - mlp_loss: 0.2752 - mlp_binary_accuracy: 0.8801 - mlp_auc_34: 0.9537\n",
      "Epoch 54/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2680 - siamese_loss: 5.0730 - mlp_loss: 0.2680 - mlp_binary_accuracy: 0.8867 - mlp_auc_34: 0.9560\n",
      "Epoch 55/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2715 - siamese_loss: 5.1174 - mlp_loss: 0.2715 - mlp_binary_accuracy: 0.8861 - mlp_auc_34: 0.9547\n",
      "Epoch 56/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2662 - siamese_loss: 5.0068 - mlp_loss: 0.2662 - mlp_binary_accuracy: 0.8879 - mlp_auc_34: 0.9565\n",
      "Epoch 57/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2616 - siamese_loss: 5.0267 - mlp_loss: 0.2616 - mlp_binary_accuracy: 0.8911 - mlp_auc_34: 0.9584\n",
      "Epoch 58/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2613 - siamese_loss: 5.0319 - mlp_loss: 0.2613 - mlp_binary_accuracy: 0.8907 - mlp_auc_34: 0.9587\n",
      "Epoch 59/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2512 - siamese_loss: 5.0419 - mlp_loss: 0.2512 - mlp_binary_accuracy: 0.8959 - mlp_auc_34: 0.9613\n",
      "Epoch 60/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2604 - siamese_loss: 5.0271 - mlp_loss: 0.2604 - mlp_binary_accuracy: 0.8897 - mlp_auc_34: 0.9588\n",
      "Epoch 61/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2364 - siamese_loss: 5.0275 - mlp_loss: 0.2364 - mlp_binary_accuracy: 0.9024 - mlp_auc_34: 0.9658\n",
      "Epoch 62/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2339 - siamese_loss: 5.0994 - mlp_loss: 0.2339 - mlp_binary_accuracy: 0.9026 - mlp_auc_34: 0.9666\n",
      "Epoch 63/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2366 - siamese_loss: 5.0403 - mlp_loss: 0.2366 - mlp_binary_accuracy: 0.9019 - mlp_auc_34: 0.9658\n",
      "Epoch 64/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2394 - siamese_loss: 5.0568 - mlp_loss: 0.2394 - mlp_binary_accuracy: 0.9024 - mlp_auc_34: 0.9651\n",
      "Epoch 65/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2274 - siamese_loss: 5.0973 - mlp_loss: 0.2274 - mlp_binary_accuracy: 0.9095 - mlp_auc_34: 0.9684\n",
      "Epoch 66/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2255 - siamese_loss: 5.1364 - mlp_loss: 0.2255 - mlp_binary_accuracy: 0.9102 - mlp_auc_34: 0.9686\n",
      "Epoch 67/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2204 - siamese_loss: 5.0983 - mlp_loss: 0.2204 - mlp_binary_accuracy: 0.9111 - mlp_auc_34: 0.9696\n",
      "Epoch 68/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2174 - siamese_loss: 5.1517 - mlp_loss: 0.2174 - mlp_binary_accuracy: 0.9111 - mlp_auc_34: 0.9705\n",
      "Epoch 69/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2127 - siamese_loss: 5.2146 - mlp_loss: 0.2127 - mlp_binary_accuracy: 0.9155 - mlp_auc_34: 0.9724\n",
      "Epoch 70/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2089 - siamese_loss: 5.2246 - mlp_loss: 0.2089 - mlp_binary_accuracy: 0.9134 - mlp_auc_34: 0.9731\n",
      "Epoch 71/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2132 - siamese_loss: 5.1150 - mlp_loss: 0.2132 - mlp_binary_accuracy: 0.9141 - mlp_auc_34: 0.9720\n",
      "Epoch 72/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2098 - siamese_loss: 5.0194 - mlp_loss: 0.2098 - mlp_binary_accuracy: 0.9113 - mlp_auc_34: 0.9722\n",
      "Epoch 73/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2038 - siamese_loss: 5.0217 - mlp_loss: 0.2038 - mlp_binary_accuracy: 0.9166 - mlp_auc_34: 0.9744\n",
      "Epoch 74/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1941 - siamese_loss: 5.0548 - mlp_loss: 0.1941 - mlp_binary_accuracy: 0.9242 - mlp_auc_34: 0.9765\n",
      "Epoch 75/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1974 - siamese_loss: 5.0733 - mlp_loss: 0.1974 - mlp_binary_accuracy: 0.9210 - mlp_auc_34: 0.9756\n",
      "Epoch 76/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1958 - siamese_loss: 5.1701 - mlp_loss: 0.1958 - mlp_binary_accuracy: 0.9214 - mlp_auc_34: 0.9767\n",
      "Epoch 77/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2015 - siamese_loss: 5.1684 - mlp_loss: 0.2015 - mlp_binary_accuracy: 0.9180 - mlp_auc_34: 0.9748\n",
      "Epoch 78/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2005 - siamese_loss: 5.2157 - mlp_loss: 0.2005 - mlp_binary_accuracy: 0.9175 - mlp_auc_34: 0.9750\n",
      "Epoch 79/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1991 - siamese_loss: 5.2656 - mlp_loss: 0.1991 - mlp_binary_accuracy: 0.9196 - mlp_auc_34: 0.9754\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5582 - siamese_loss: 5.4056 - mlp_loss: 0.5582 - mlp_binary_accuracy: 0.7867 - mlp_auc_34: 0.8717    \n",
      "------ mlp_binary_accuracy: 78.67%\t ----- mlp_auc_34: 87.17%\n",
      "concat_fea的shape： (None, 112)\n",
      "-------------------------------------Kfold: 8 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6758 - siamese_loss: 2.7207 - mlp_loss: 0.6758 - mlp_binary_accuracy: 0.5796 - mlp_auc_35: 0.6293\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6327 - siamese_loss: 2.6776 - mlp_loss: 0.6327 - mlp_binary_accuracy: 0.6499 - mlp_auc_35: 0.7009\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5996 - siamese_loss: 2.9580 - mlp_loss: 0.5996 - mlp_binary_accuracy: 0.6800 - mlp_auc_35: 0.7429\n",
      "Epoch 4/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5787 - siamese_loss: 3.3777 - mlp_loss: 0.5787 - mlp_binary_accuracy: 0.7025 - mlp_auc_35: 0.7656\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5632 - siamese_loss: 3.2534 - mlp_loss: 0.5632 - mlp_binary_accuracy: 0.7110 - mlp_auc_35: 0.7824\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5501 - siamese_loss: 3.3911 - mlp_loss: 0.5501 - mlp_binary_accuracy: 0.7209 - mlp_auc_35: 0.7938\n",
      "Epoch 7/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5397 - siamese_loss: 3.5738 - mlp_loss: 0.5397 - mlp_binary_accuracy: 0.7308 - mlp_auc_35: 0.8031\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5307 - siamese_loss: 3.5985 - mlp_loss: 0.5307 - mlp_binary_accuracy: 0.7370 - mlp_auc_35: 0.8111\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5196 - siamese_loss: 3.7497 - mlp_loss: 0.5196 - mlp_binary_accuracy: 0.7496 - mlp_auc_35: 0.8208\n",
      "Epoch 10/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5157 - siamese_loss: 3.8621 - mlp_loss: 0.5157 - mlp_binary_accuracy: 0.7501 - mlp_auc_35: 0.8246\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5032 - siamese_loss: 3.8711 - mlp_loss: 0.5032 - mlp_binary_accuracy: 0.7512 - mlp_auc_35: 0.8340\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4921 - siamese_loss: 3.8266 - mlp_loss: 0.4921 - mlp_binary_accuracy: 0.7634 - mlp_auc_35: 0.8422\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4920 - siamese_loss: 4.0450 - mlp_loss: 0.4920 - mlp_binary_accuracy: 0.7629 - mlp_auc_35: 0.8418\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4773 - siamese_loss: 4.2703 - mlp_loss: 0.4773 - mlp_binary_accuracy: 0.7689 - mlp_auc_35: 0.8523\n",
      "Epoch 15/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4739 - siamese_loss: 4.1351 - mlp_loss: 0.4739 - mlp_binary_accuracy: 0.7756 - mlp_auc_35: 0.8564\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4683 - siamese_loss: 3.8455 - mlp_loss: 0.4683 - mlp_binary_accuracy: 0.7785 - mlp_auc_35: 0.8586\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4575 - siamese_loss: 3.9969 - mlp_loss: 0.4575 - mlp_binary_accuracy: 0.7829 - mlp_auc_35: 0.8657\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4541 - siamese_loss: 4.2309 - mlp_loss: 0.4541 - mlp_binary_accuracy: 0.7859 - mlp_auc_35: 0.8681\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4445 - siamese_loss: 4.3536 - mlp_loss: 0.4445 - mlp_binary_accuracy: 0.7907 - mlp_auc_35: 0.8747\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4375 - siamese_loss: 4.4213 - mlp_loss: 0.4375 - mlp_binary_accuracy: 0.8011 - mlp_auc_35: 0.8794\n",
      "Epoch 21/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4298 - siamese_loss: 4.3833 - mlp_loss: 0.4298 - mlp_binary_accuracy: 0.8057 - mlp_auc_35: 0.8844\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4307 - siamese_loss: 4.4290 - mlp_loss: 0.4307 - mlp_binary_accuracy: 0.8020 - mlp_auc_35: 0.8821\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4135 - siamese_loss: 4.4096 - mlp_loss: 0.4135 - mlp_binary_accuracy: 0.8119 - mlp_auc_35: 0.8933\n",
      "Epoch 24/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4112 - siamese_loss: 4.3967 - mlp_loss: 0.4112 - mlp_binary_accuracy: 0.8171 - mlp_auc_35: 0.8943\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4047 - siamese_loss: 4.3721 - mlp_loss: 0.4047 - mlp_binary_accuracy: 0.8210 - mlp_auc_35: 0.8975\n",
      "Epoch 26/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3945 - siamese_loss: 4.3617 - mlp_loss: 0.3945 - mlp_binary_accuracy: 0.8224 - mlp_auc_35: 0.9030\n",
      "Epoch 27/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3985 - siamese_loss: 4.6142 - mlp_loss: 0.3985 - mlp_binary_accuracy: 0.8213 - mlp_auc_35: 0.9010\n",
      "Epoch 28/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3888 - siamese_loss: 4.7242 - mlp_loss: 0.3888 - mlp_binary_accuracy: 0.8249 - mlp_auc_35: 0.9058\n",
      "Epoch 29/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3761 - siamese_loss: 4.6601 - mlp_loss: 0.3761 - mlp_binary_accuracy: 0.8380 - mlp_auc_35: 0.9120\n",
      "Epoch 30/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3698 - siamese_loss: 4.4476 - mlp_loss: 0.3698 - mlp_binary_accuracy: 0.8362 - mlp_auc_35: 0.9151\n",
      "Epoch 31/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3659 - siamese_loss: 4.4671 - mlp_loss: 0.3659 - mlp_binary_accuracy: 0.8380 - mlp_auc_35: 0.9166\n",
      "Epoch 32/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3609 - siamese_loss: 4.5428 - mlp_loss: 0.3609 - mlp_binary_accuracy: 0.8406 - mlp_auc_35: 0.9196\n",
      "Epoch 33/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3613 - siamese_loss: 4.6666 - mlp_loss: 0.3613 - mlp_binary_accuracy: 0.8390 - mlp_auc_35: 0.9188\n",
      "Epoch 34/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3517 - siamese_loss: 4.6889 - mlp_loss: 0.3517 - mlp_binary_accuracy: 0.8452 - mlp_auc_35: 0.9236\n",
      "Epoch 35/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3476 - siamese_loss: 4.7020 - mlp_loss: 0.3476 - mlp_binary_accuracy: 0.8447 - mlp_auc_35: 0.9251\n",
      "Epoch 36/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3494 - siamese_loss: 4.8502 - mlp_loss: 0.3494 - mlp_binary_accuracy: 0.8424 - mlp_auc_35: 0.9247\n",
      "Epoch 37/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3302 - siamese_loss: 4.8434 - mlp_loss: 0.3302 - mlp_binary_accuracy: 0.8612 - mlp_auc_35: 0.9329\n",
      "Epoch 38/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3331 - siamese_loss: 4.8212 - mlp_loss: 0.3331 - mlp_binary_accuracy: 0.8587 - mlp_auc_35: 0.9316\n",
      "Epoch 39/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3292 - siamese_loss: 4.9393 - mlp_loss: 0.3292 - mlp_binary_accuracy: 0.8626 - mlp_auc_35: 0.9330\n",
      "Epoch 40/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3272 - siamese_loss: 5.1383 - mlp_loss: 0.3272 - mlp_binary_accuracy: 0.8626 - mlp_auc_35: 0.9343\n",
      "Epoch 41/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3263 - siamese_loss: 5.0780 - mlp_loss: 0.3263 - mlp_binary_accuracy: 0.8612 - mlp_auc_35: 0.9342\n",
      "Epoch 42/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3235 - siamese_loss: 4.9634 - mlp_loss: 0.3235 - mlp_binary_accuracy: 0.8610 - mlp_auc_35: 0.9352\n",
      "Epoch 43/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3166 - siamese_loss: 4.9805 - mlp_loss: 0.3166 - mlp_binary_accuracy: 0.8670 - mlp_auc_35: 0.9377\n",
      "Epoch 44/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3105 - siamese_loss: 5.0457 - mlp_loss: 0.3105 - mlp_binary_accuracy: 0.8688 - mlp_auc_35: 0.9405\n",
      "Epoch 45/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2989 - siamese_loss: 4.9775 - mlp_loss: 0.2989 - mlp_binary_accuracy: 0.8732 - mlp_auc_35: 0.9450\n",
      "Epoch 46/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2978 - siamese_loss: 5.0278 - mlp_loss: 0.2978 - mlp_binary_accuracy: 0.8741 - mlp_auc_35: 0.9454\n",
      "Epoch 47/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3012 - siamese_loss: 4.9267 - mlp_loss: 0.3012 - mlp_binary_accuracy: 0.8697 - mlp_auc_35: 0.9442\n",
      "Epoch 48/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3050 - siamese_loss: 5.1196 - mlp_loss: 0.3050 - mlp_binary_accuracy: 0.8707 - mlp_auc_35: 0.9426\n",
      "Epoch 49/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2919 - siamese_loss: 5.0950 - mlp_loss: 0.2919 - mlp_binary_accuracy: 0.8776 - mlp_auc_35: 0.9472\n",
      "Epoch 50/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2921 - siamese_loss: 5.1381 - mlp_loss: 0.2921 - mlp_binary_accuracy: 0.8743 - mlp_auc_35: 0.9473\n",
      "Epoch 51/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2849 - siamese_loss: 5.1611 - mlp_loss: 0.2849 - mlp_binary_accuracy: 0.8792 - mlp_auc_35: 0.9504\n",
      "Epoch 52/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2826 - siamese_loss: 5.1175 - mlp_loss: 0.2826 - mlp_binary_accuracy: 0.8815 - mlp_auc_35: 0.9509\n",
      "Epoch 53/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2774 - siamese_loss: 5.2207 - mlp_loss: 0.2774 - mlp_binary_accuracy: 0.8799 - mlp_auc_35: 0.9524\n",
      "Epoch 54/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2756 - siamese_loss: 5.1971 - mlp_loss: 0.2756 - mlp_binary_accuracy: 0.8833 - mlp_auc_35: 0.9529\n",
      "Epoch 55/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2603 - siamese_loss: 5.1045 - mlp_loss: 0.2603 - mlp_binary_accuracy: 0.8909 - mlp_auc_35: 0.9583\n",
      "Epoch 56/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2667 - siamese_loss: 5.1817 - mlp_loss: 0.2667 - mlp_binary_accuracy: 0.8867 - mlp_auc_35: 0.9555\n",
      "Epoch 57/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2550 - siamese_loss: 5.2419 - mlp_loss: 0.2550 - mlp_binary_accuracy: 0.8950 - mlp_auc_35: 0.9596\n",
      "Epoch 58/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2566 - siamese_loss: 5.2771 - mlp_loss: 0.2566 - mlp_binary_accuracy: 0.8971 - mlp_auc_35: 0.9592\n",
      "Epoch 59/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2659 - siamese_loss: 5.3330 - mlp_loss: 0.2659 - mlp_binary_accuracy: 0.8936 - mlp_auc_35: 0.9565\n",
      "Epoch 60/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2598 - siamese_loss: 5.2028 - mlp_loss: 0.2598 - mlp_binary_accuracy: 0.8890 - mlp_auc_35: 0.9591\n",
      "Epoch 61/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2491 - siamese_loss: 5.3048 - mlp_loss: 0.2491 - mlp_binary_accuracy: 0.8952 - mlp_auc_35: 0.9615\n",
      "Epoch 62/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2565 - siamese_loss: 5.4814 - mlp_loss: 0.2565 - mlp_binary_accuracy: 0.8925 - mlp_auc_35: 0.9594\n",
      "Epoch 63/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2417 - siamese_loss: 5.5556 - mlp_loss: 0.2417 - mlp_binary_accuracy: 0.9035 - mlp_auc_35: 0.9635\n",
      "Epoch 64/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2376 - siamese_loss: 5.4124 - mlp_loss: 0.2376 - mlp_binary_accuracy: 0.9037 - mlp_auc_35: 0.9651\n",
      "Epoch 65/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2262 - siamese_loss: 5.5223 - mlp_loss: 0.2262 - mlp_binary_accuracy: 0.9072 - mlp_auc_35: 0.9684\n",
      "Epoch 66/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2418 - siamese_loss: 5.5473 - mlp_loss: 0.2418 - mlp_binary_accuracy: 0.9019 - mlp_auc_35: 0.9642\n",
      "Epoch 67/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2226 - siamese_loss: 5.5478 - mlp_loss: 0.2226 - mlp_binary_accuracy: 0.9111 - mlp_auc_35: 0.9694\n",
      "Epoch 68/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2226 - siamese_loss: 5.6278 - mlp_loss: 0.2226 - mlp_binary_accuracy: 0.9095 - mlp_auc_35: 0.9693\n",
      "Epoch 69/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2349 - siamese_loss: 5.5955 - mlp_loss: 0.2349 - mlp_binary_accuracy: 0.9081 - mlp_auc_35: 0.9663\n",
      "Epoch 70/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2323 - siamese_loss: 5.5919 - mlp_loss: 0.2323 - mlp_binary_accuracy: 0.9058 - mlp_auc_35: 0.9669\n",
      "Epoch 71/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2164 - siamese_loss: 5.5750 - mlp_loss: 0.2164 - mlp_binary_accuracy: 0.9122 - mlp_auc_35: 0.9707\n",
      "Epoch 72/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2139 - siamese_loss: 5.6426 - mlp_loss: 0.2139 - mlp_binary_accuracy: 0.9141 - mlp_auc_35: 0.9715\n",
      "Epoch 73/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2057 - siamese_loss: 5.5224 - mlp_loss: 0.2057 - mlp_binary_accuracy: 0.9141 - mlp_auc_35: 0.9736\n",
      "Epoch 74/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2035 - siamese_loss: 5.6172 - mlp_loss: 0.2035 - mlp_binary_accuracy: 0.9187 - mlp_auc_35: 0.9746\n",
      "Epoch 75/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2078 - siamese_loss: 5.6019 - mlp_loss: 0.2078 - mlp_binary_accuracy: 0.9157 - mlp_auc_35: 0.9725\n",
      "Epoch 76/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2101 - siamese_loss: 5.5862 - mlp_loss: 0.2101 - mlp_binary_accuracy: 0.9175 - mlp_auc_35: 0.9730\n",
      "Epoch 77/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2112 - siamese_loss: 5.6287 - mlp_loss: 0.2112 - mlp_binary_accuracy: 0.9178 - mlp_auc_35: 0.9718\n",
      "Epoch 78/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2036 - siamese_loss: 5.7198 - mlp_loss: 0.2036 - mlp_binary_accuracy: 0.9191 - mlp_auc_35: 0.9741\n",
      "Epoch 79/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1904 - siamese_loss: 5.6961 - mlp_loss: 0.1904 - mlp_binary_accuracy: 0.9256 - mlp_auc_35: 0.9778\n",
      "Epoch 80/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1982 - siamese_loss: 5.6468 - mlp_loss: 0.1982 - mlp_binary_accuracy: 0.9212 - mlp_auc_35: 0.9755\n",
      "Epoch 81/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1873 - siamese_loss: 5.6236 - mlp_loss: 0.1873 - mlp_binary_accuracy: 0.9253 - mlp_auc_35: 0.9782\n",
      "Epoch 82/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1888 - siamese_loss: 5.6275 - mlp_loss: 0.1888 - mlp_binary_accuracy: 0.9240 - mlp_auc_35: 0.9777\n",
      "Epoch 83/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1836 - siamese_loss: 5.6146 - mlp_loss: 0.1836 - mlp_binary_accuracy: 0.9269 - mlp_auc_35: 0.9792\n",
      "Epoch 84/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1926 - siamese_loss: 5.6472 - mlp_loss: 0.1926 - mlp_binary_accuracy: 0.9235 - mlp_auc_35: 0.9769\n",
      "Epoch 85/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1880 - siamese_loss: 5.6205 - mlp_loss: 0.1880 - mlp_binary_accuracy: 0.9249 - mlp_auc_35: 0.9776\n",
      "Epoch 86/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1777 - siamese_loss: 5.6680 - mlp_loss: 0.1777 - mlp_binary_accuracy: 0.9302 - mlp_auc_35: 0.9802\n",
      "Epoch 87/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1855 - siamese_loss: 5.5964 - mlp_loss: 0.1855 - mlp_binary_accuracy: 0.9274 - mlp_auc_35: 0.9783\n",
      "Epoch 88/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1856 - siamese_loss: 5.6514 - mlp_loss: 0.1856 - mlp_binary_accuracy: 0.9258 - mlp_auc_35: 0.9786\n",
      "Epoch 89/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1797 - siamese_loss: 5.7048 - mlp_loss: 0.1797 - mlp_binary_accuracy: 0.9286 - mlp_auc_35: 0.9801\n",
      "Epoch 90/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1724 - siamese_loss: 5.7513 - mlp_loss: 0.1724 - mlp_binary_accuracy: 0.9334 - mlp_auc_35: 0.9815\n",
      "Epoch 91/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1715 - siamese_loss: 5.7367 - mlp_loss: 0.1715 - mlp_binary_accuracy: 0.9279 - mlp_auc_35: 0.9820\n",
      "Epoch 92/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1728 - siamese_loss: 5.6643 - mlp_loss: 0.1728 - mlp_binary_accuracy: 0.9338 - mlp_auc_35: 0.9813\n",
      "Epoch 93/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1795 - siamese_loss: 5.6166 - mlp_loss: 0.1795 - mlp_binary_accuracy: 0.9290 - mlp_auc_35: 0.9796\n",
      "Epoch 94/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1640 - siamese_loss: 5.5832 - mlp_loss: 0.1640 - mlp_binary_accuracy: 0.9350 - mlp_auc_35: 0.9837\n",
      "Epoch 95/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1730 - siamese_loss: 5.7410 - mlp_loss: 0.1730 - mlp_binary_accuracy: 0.9286 - mlp_auc_35: 0.9821\n",
      "Epoch 96/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1712 - siamese_loss: 5.7126 - mlp_loss: 0.1712 - mlp_binary_accuracy: 0.9334 - mlp_auc_35: 0.9817\n",
      "Epoch 97/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1633 - siamese_loss: 5.6594 - mlp_loss: 0.1633 - mlp_binary_accuracy: 0.9354 - mlp_auc_35: 0.9833\n",
      "Epoch 98/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1611 - siamese_loss: 5.7507 - mlp_loss: 0.1611 - mlp_binary_accuracy: 0.9416 - mlp_auc_35: 0.9834\n",
      "Epoch 99/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1544 - siamese_loss: 5.7628 - mlp_loss: 0.1544 - mlp_binary_accuracy: 0.9421 - mlp_auc_35: 0.9848\n",
      "Epoch 100/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1640 - siamese_loss: 5.7728 - mlp_loss: 0.1640 - mlp_binary_accuracy: 0.9341 - mlp_auc_35: 0.9833\n",
      "Epoch 101/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1545 - siamese_loss: 5.8123 - mlp_loss: 0.1545 - mlp_binary_accuracy: 0.9403 - mlp_auc_35: 0.9851\n",
      "Epoch 102/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1521 - siamese_loss: 5.7532 - mlp_loss: 0.1521 - mlp_binary_accuracy: 0.9414 - mlp_auc_35: 0.9854\n",
      "Epoch 103/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1571 - siamese_loss: 5.7794 - mlp_loss: 0.1571 - mlp_binary_accuracy: 0.9391 - mlp_auc_35: 0.9845\n",
      "Epoch 104/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1568 - siamese_loss: 5.6651 - mlp_loss: 0.1568 - mlp_binary_accuracy: 0.9377 - mlp_auc_35: 0.9842\n",
      "Epoch 105/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1495 - siamese_loss: 5.6048 - mlp_loss: 0.1495 - mlp_binary_accuracy: 0.9373 - mlp_auc_35: 0.9860\n",
      "Epoch 106/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1530 - siamese_loss: 5.6644 - mlp_loss: 0.1530 - mlp_binary_accuracy: 0.9419 - mlp_auc_35: 0.9854\n",
      "Epoch 107/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1532 - siamese_loss: 5.7128 - mlp_loss: 0.1532 - mlp_binary_accuracy: 0.9428 - mlp_auc_35: 0.9854\n",
      "Epoch 108/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1430 - siamese_loss: 5.7319 - mlp_loss: 0.1430 - mlp_binary_accuracy: 0.9479 - mlp_auc_35: 0.9867\n",
      "Epoch 109/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1530 - siamese_loss: 5.7386 - mlp_loss: 0.1530 - mlp_binary_accuracy: 0.9410 - mlp_auc_35: 0.9854\n",
      "Epoch 110/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1382 - siamese_loss: 5.7434 - mlp_loss: 0.1382 - mlp_binary_accuracy: 0.9444 - mlp_auc_35: 0.9877\n",
      "Epoch 111/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1409 - siamese_loss: 5.6515 - mlp_loss: 0.1409 - mlp_binary_accuracy: 0.9439 - mlp_auc_35: 0.9875\n",
      "Epoch 112/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1415 - siamese_loss: 5.6767 - mlp_loss: 0.1415 - mlp_binary_accuracy: 0.9449 - mlp_auc_35: 0.9873\n",
      "Epoch 113/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1347 - siamese_loss: 5.6972 - mlp_loss: 0.1347 - mlp_binary_accuracy: 0.9472 - mlp_auc_35: 0.9884\n",
      "Epoch 114/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1440 - siamese_loss: 5.7335 - mlp_loss: 0.1440 - mlp_binary_accuracy: 0.9453 - mlp_auc_35: 0.9867\n",
      "Epoch 115/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1447 - siamese_loss: 5.8141 - mlp_loss: 0.1447 - mlp_binary_accuracy: 0.9458 - mlp_auc_35: 0.9869\n",
      "Epoch 116/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1411 - siamese_loss: 5.8030 - mlp_loss: 0.1411 - mlp_binary_accuracy: 0.9449 - mlp_auc_35: 0.9874\n",
      "Epoch 117/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1278 - siamese_loss: 5.7803 - mlp_loss: 0.1278 - mlp_binary_accuracy: 0.9488 - mlp_auc_35: 0.9893\n",
      "Epoch 118/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1315 - siamese_loss: 5.7770 - mlp_loss: 0.1315 - mlp_binary_accuracy: 0.9504 - mlp_auc_35: 0.9889\n",
      "Epoch 119/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1186 - siamese_loss: 5.8576 - mlp_loss: 0.1186 - mlp_binary_accuracy: 0.9577 - mlp_auc_35: 0.9910\n",
      "Epoch 120/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1260 - siamese_loss: 5.7811 - mlp_loss: 0.1260 - mlp_binary_accuracy: 0.9518 - mlp_auc_35: 0.9896\n",
      "Epoch 121/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1312 - siamese_loss: 5.8037 - mlp_loss: 0.1312 - mlp_binary_accuracy: 0.9508 - mlp_auc_35: 0.9890\n",
      "Epoch 122/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1336 - siamese_loss: 5.8182 - mlp_loss: 0.1336 - mlp_binary_accuracy: 0.9529 - mlp_auc_35: 0.9883\n",
      "Epoch 123/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1229 - siamese_loss: 5.7756 - mlp_loss: 0.1229 - mlp_binary_accuracy: 0.9552 - mlp_auc_35: 0.9902\n",
      "Epoch 124/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1271 - siamese_loss: 5.7794 - mlp_loss: 0.1271 - mlp_binary_accuracy: 0.9552 - mlp_auc_35: 0.9897\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7025 - siamese_loss: 5.8504 - mlp_loss: 0.7025 - mlp_binary_accuracy: 0.7992 - mlp_auc_35: 0.8806    \n",
      "------ mlp_binary_accuracy: 79.92%\t ----- mlp_auc_35: 88.06%\n",
      "concat_fea的shape： (None, 112)\n",
      "-------------------------------------Kfold: 9 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6790 - siamese_loss: 3.2829 - mlp_loss: 0.6790 - mlp_binary_accuracy: 0.5642 - mlp_auc_36: 0.6050\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6175 - siamese_loss: 3.4411 - mlp_loss: 0.6175 - mlp_binary_accuracy: 0.6572 - mlp_auc_36: 0.7178\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5888 - siamese_loss: 3.3596 - mlp_loss: 0.5888 - mlp_binary_accuracy: 0.6855 - mlp_auc_36: 0.7515\n",
      "Epoch 4/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5692 - siamese_loss: 3.7457 - mlp_loss: 0.5692 - mlp_binary_accuracy: 0.7032 - mlp_auc_36: 0.7739\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5535 - siamese_loss: 3.8088 - mlp_loss: 0.5535 - mlp_binary_accuracy: 0.7229 - mlp_auc_36: 0.7893\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5471 - siamese_loss: 3.6908 - mlp_loss: 0.5471 - mlp_binary_accuracy: 0.7190 - mlp_auc_36: 0.7955\n",
      "Epoch 7/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5349 - siamese_loss: 3.8247 - mlp_loss: 0.5349 - mlp_binary_accuracy: 0.7241 - mlp_auc_36: 0.8053\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5208 - siamese_loss: 3.6493 - mlp_loss: 0.5208 - mlp_binary_accuracy: 0.7374 - mlp_auc_36: 0.8187\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5164 - siamese_loss: 3.6015 - mlp_loss: 0.5164 - mlp_binary_accuracy: 0.7434 - mlp_auc_36: 0.8220\n",
      "Epoch 10/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5123 - siamese_loss: 3.7225 - mlp_loss: 0.5123 - mlp_binary_accuracy: 0.7429 - mlp_auc_36: 0.8264\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5047 - siamese_loss: 3.8045 - mlp_loss: 0.5047 - mlp_binary_accuracy: 0.7526 - mlp_auc_36: 0.8315\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4945 - siamese_loss: 3.9296 - mlp_loss: 0.4945 - mlp_binary_accuracy: 0.7625 - mlp_auc_36: 0.8408\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4861 - siamese_loss: 3.9988 - mlp_loss: 0.4861 - mlp_binary_accuracy: 0.7636 - mlp_auc_36: 0.8466\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4802 - siamese_loss: 3.9430 - mlp_loss: 0.4802 - mlp_binary_accuracy: 0.7744 - mlp_auc_36: 0.8507\n",
      "Epoch 15/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4829 - siamese_loss: 3.8552 - mlp_loss: 0.4829 - mlp_binary_accuracy: 0.7664 - mlp_auc_36: 0.8481\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4726 - siamese_loss: 4.2553 - mlp_loss: 0.4726 - mlp_binary_accuracy: 0.7781 - mlp_auc_36: 0.8571\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4700 - siamese_loss: 4.2838 - mlp_loss: 0.4700 - mlp_binary_accuracy: 0.7760 - mlp_auc_36: 0.8571\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4597 - siamese_loss: 4.0996 - mlp_loss: 0.4597 - mlp_binary_accuracy: 0.7808 - mlp_auc_36: 0.8644\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4538 - siamese_loss: 4.3024 - mlp_loss: 0.4538 - mlp_binary_accuracy: 0.7868 - mlp_auc_36: 0.8687\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4499 - siamese_loss: 3.9810 - mlp_loss: 0.4499 - mlp_binary_accuracy: 0.7912 - mlp_auc_36: 0.8704\n",
      "Epoch 21/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4382 - siamese_loss: 4.1999 - mlp_loss: 0.4382 - mlp_binary_accuracy: 0.7919 - mlp_auc_36: 0.8777\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4326 - siamese_loss: 4.2046 - mlp_loss: 0.4326 - mlp_binary_accuracy: 0.8020 - mlp_auc_36: 0.8815\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4267 - siamese_loss: 4.1454 - mlp_loss: 0.4267 - mlp_binary_accuracy: 0.8059 - mlp_auc_36: 0.8843\n",
      "Epoch 24/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4233 - siamese_loss: 4.3223 - mlp_loss: 0.4233 - mlp_binary_accuracy: 0.8031 - mlp_auc_36: 0.8864\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4190 - siamese_loss: 4.1640 - mlp_loss: 0.4190 - mlp_binary_accuracy: 0.8036 - mlp_auc_36: 0.8882\n",
      "Epoch 26/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4096 - siamese_loss: 4.2688 - mlp_loss: 0.4096 - mlp_binary_accuracy: 0.8123 - mlp_auc_36: 0.8943\n",
      "Epoch 27/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4045 - siamese_loss: 4.3501 - mlp_loss: 0.4045 - mlp_binary_accuracy: 0.8178 - mlp_auc_36: 0.8969\n",
      "Epoch 28/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4005 - siamese_loss: 4.4277 - mlp_loss: 0.4005 - mlp_binary_accuracy: 0.8176 - mlp_auc_36: 0.8996\n",
      "Epoch 29/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3941 - siamese_loss: 4.4698 - mlp_loss: 0.3941 - mlp_binary_accuracy: 0.8206 - mlp_auc_36: 0.9026\n",
      "Epoch 30/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3982 - siamese_loss: 4.5344 - mlp_loss: 0.3982 - mlp_binary_accuracy: 0.8229 - mlp_auc_36: 0.9003\n",
      "Epoch 31/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3793 - siamese_loss: 4.5476 - mlp_loss: 0.3793 - mlp_binary_accuracy: 0.8341 - mlp_auc_36: 0.9104\n",
      "Epoch 32/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3784 - siamese_loss: 4.5776 - mlp_loss: 0.3784 - mlp_binary_accuracy: 0.8376 - mlp_auc_36: 0.9106\n",
      "Epoch 33/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3783 - siamese_loss: 4.5075 - mlp_loss: 0.3783 - mlp_binary_accuracy: 0.8360 - mlp_auc_36: 0.9106\n",
      "Epoch 34/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3624 - siamese_loss: 4.3339 - mlp_loss: 0.3624 - mlp_binary_accuracy: 0.8431 - mlp_auc_36: 0.9181\n",
      "Epoch 35/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3707 - siamese_loss: 4.2803 - mlp_loss: 0.3707 - mlp_binary_accuracy: 0.8341 - mlp_auc_36: 0.9147\n",
      "Epoch 36/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3604 - siamese_loss: 4.5813 - mlp_loss: 0.3604 - mlp_binary_accuracy: 0.8422 - mlp_auc_36: 0.9196\n",
      "Epoch 37/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3518 - siamese_loss: 4.5776 - mlp_loss: 0.3518 - mlp_binary_accuracy: 0.8438 - mlp_auc_36: 0.9231\n",
      "Epoch 38/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3488 - siamese_loss: 4.5919 - mlp_loss: 0.3488 - mlp_binary_accuracy: 0.8500 - mlp_auc_36: 0.9246\n",
      "Epoch 39/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3457 - siamese_loss: 4.6358 - mlp_loss: 0.3457 - mlp_binary_accuracy: 0.8537 - mlp_auc_36: 0.9260\n",
      "Epoch 40/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3384 - siamese_loss: 4.5678 - mlp_loss: 0.3384 - mlp_binary_accuracy: 0.8539 - mlp_auc_36: 0.9286\n",
      "Epoch 41/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3437 - siamese_loss: 4.4658 - mlp_loss: 0.3437 - mlp_binary_accuracy: 0.8569 - mlp_auc_36: 0.9264\n",
      "Epoch 42/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3293 - siamese_loss: 4.6459 - mlp_loss: 0.3293 - mlp_binary_accuracy: 0.8594 - mlp_auc_36: 0.9328\n",
      "Epoch 43/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3281 - siamese_loss: 4.5480 - mlp_loss: 0.3281 - mlp_binary_accuracy: 0.8622 - mlp_auc_36: 0.9335\n",
      "Epoch 44/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3235 - siamese_loss: 4.5237 - mlp_loss: 0.3235 - mlp_binary_accuracy: 0.8603 - mlp_auc_36: 0.9349\n",
      "Epoch 45/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3176 - siamese_loss: 4.6412 - mlp_loss: 0.3176 - mlp_binary_accuracy: 0.8693 - mlp_auc_36: 0.9379\n",
      "Epoch 46/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3104 - siamese_loss: 4.6182 - mlp_loss: 0.3104 - mlp_binary_accuracy: 0.8663 - mlp_auc_36: 0.9403\n",
      "Epoch 47/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3105 - siamese_loss: 4.5936 - mlp_loss: 0.3105 - mlp_binary_accuracy: 0.8677 - mlp_auc_36: 0.9406\n",
      "Epoch 48/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3189 - siamese_loss: 4.5075 - mlp_loss: 0.3189 - mlp_binary_accuracy: 0.8677 - mlp_auc_36: 0.9369\n",
      "Epoch 49/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3018 - siamese_loss: 4.6282 - mlp_loss: 0.3018 - mlp_binary_accuracy: 0.8741 - mlp_auc_36: 0.9435\n",
      "Epoch 50/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3018 - siamese_loss: 4.8062 - mlp_loss: 0.3018 - mlp_binary_accuracy: 0.8670 - mlp_auc_36: 0.9438\n",
      "Epoch 51/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2946 - siamese_loss: 4.7244 - mlp_loss: 0.2946 - mlp_binary_accuracy: 0.8794 - mlp_auc_36: 0.9462\n",
      "Epoch 52/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2895 - siamese_loss: 4.8734 - mlp_loss: 0.2895 - mlp_binary_accuracy: 0.8785 - mlp_auc_36: 0.9482\n",
      "Epoch 53/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2954 - siamese_loss: 4.7608 - mlp_loss: 0.2954 - mlp_binary_accuracy: 0.8741 - mlp_auc_36: 0.9458\n",
      "Epoch 54/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2928 - siamese_loss: 4.7243 - mlp_loss: 0.2928 - mlp_binary_accuracy: 0.8764 - mlp_auc_36: 0.9466\n",
      "Epoch 55/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2853 - siamese_loss: 4.8460 - mlp_loss: 0.2853 - mlp_binary_accuracy: 0.8803 - mlp_auc_36: 0.9498\n",
      "Epoch 56/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2807 - siamese_loss: 4.8268 - mlp_loss: 0.2807 - mlp_binary_accuracy: 0.8842 - mlp_auc_36: 0.9514\n",
      "Epoch 57/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2800 - siamese_loss: 5.0666 - mlp_loss: 0.2800 - mlp_binary_accuracy: 0.8808 - mlp_auc_36: 0.9519\n",
      "Epoch 58/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2752 - siamese_loss: 4.9746 - mlp_loss: 0.2752 - mlp_binary_accuracy: 0.8856 - mlp_auc_36: 0.9536\n",
      "Epoch 59/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2605 - siamese_loss: 4.9142 - mlp_loss: 0.2605 - mlp_binary_accuracy: 0.8941 - mlp_auc_36: 0.9577\n",
      "Epoch 60/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2663 - siamese_loss: 4.9436 - mlp_loss: 0.2663 - mlp_binary_accuracy: 0.8900 - mlp_auc_36: 0.9561\n",
      "Epoch 61/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2603 - siamese_loss: 4.9393 - mlp_loss: 0.2603 - mlp_binary_accuracy: 0.8923 - mlp_auc_36: 0.9578\n",
      "Epoch 62/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2518 - siamese_loss: 5.0353 - mlp_loss: 0.2518 - mlp_binary_accuracy: 0.8973 - mlp_auc_36: 0.9601\n",
      "Epoch 63/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2520 - siamese_loss: 4.9945 - mlp_loss: 0.2520 - mlp_binary_accuracy: 0.8941 - mlp_auc_36: 0.9607\n",
      "Epoch 64/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2565 - siamese_loss: 5.0530 - mlp_loss: 0.2565 - mlp_binary_accuracy: 0.8980 - mlp_auc_36: 0.9589\n",
      "Epoch 65/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2636 - siamese_loss: 5.1247 - mlp_loss: 0.2636 - mlp_binary_accuracy: 0.8939 - mlp_auc_36: 0.9570\n",
      "Epoch 66/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2467 - siamese_loss: 5.0684 - mlp_loss: 0.2467 - mlp_binary_accuracy: 0.8992 - mlp_auc_36: 0.9622\n",
      "Epoch 67/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2423 - siamese_loss: 5.0123 - mlp_loss: 0.2423 - mlp_binary_accuracy: 0.9035 - mlp_auc_36: 0.9632\n",
      "Epoch 68/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2412 - siamese_loss: 4.9991 - mlp_loss: 0.2412 - mlp_binary_accuracy: 0.9031 - mlp_auc_36: 0.9639\n",
      "Epoch 69/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2347 - siamese_loss: 4.8539 - mlp_loss: 0.2347 - mlp_binary_accuracy: 0.9054 - mlp_auc_36: 0.9655\n",
      "Epoch 70/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2350 - siamese_loss: 4.8418 - mlp_loss: 0.2350 - mlp_binary_accuracy: 0.9054 - mlp_auc_36: 0.9656\n",
      "Epoch 71/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2213 - siamese_loss: 4.9091 - mlp_loss: 0.2213 - mlp_binary_accuracy: 0.9125 - mlp_auc_36: 0.9694\n",
      "Epoch 72/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2247 - siamese_loss: 4.9687 - mlp_loss: 0.2247 - mlp_binary_accuracy: 0.9063 - mlp_auc_36: 0.9688\n",
      "Epoch 73/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2263 - siamese_loss: 5.0286 - mlp_loss: 0.2263 - mlp_binary_accuracy: 0.9088 - mlp_auc_36: 0.9681\n",
      "Epoch 74/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2242 - siamese_loss: 5.0602 - mlp_loss: 0.2242 - mlp_binary_accuracy: 0.9109 - mlp_auc_36: 0.9693\n",
      "Epoch 75/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2213 - siamese_loss: 4.9574 - mlp_loss: 0.2213 - mlp_binary_accuracy: 0.9120 - mlp_auc_36: 0.9693\n",
      "Epoch 76/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2102 - siamese_loss: 5.0227 - mlp_loss: 0.2102 - mlp_binary_accuracy: 0.9155 - mlp_auc_36: 0.9728\n",
      "Epoch 77/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2205 - siamese_loss: 4.9361 - mlp_loss: 0.2205 - mlp_binary_accuracy: 0.9129 - mlp_auc_36: 0.9695\n",
      "Epoch 78/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2231 - siamese_loss: 4.8678 - mlp_loss: 0.2231 - mlp_binary_accuracy: 0.9072 - mlp_auc_36: 0.9691\n",
      "Epoch 79/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2232 - siamese_loss: 5.0470 - mlp_loss: 0.2232 - mlp_binary_accuracy: 0.9145 - mlp_auc_36: 0.9688\n",
      "Epoch 80/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2103 - siamese_loss: 5.0651 - mlp_loss: 0.2103 - mlp_binary_accuracy: 0.9143 - mlp_auc_36: 0.9724\n",
      "Epoch 81/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2046 - siamese_loss: 4.9912 - mlp_loss: 0.2046 - mlp_binary_accuracy: 0.9168 - mlp_auc_36: 0.9737\n",
      "Epoch 82/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1911 - siamese_loss: 5.0489 - mlp_loss: 0.1911 - mlp_binary_accuracy: 0.9306 - mlp_auc_36: 0.9771\n",
      "Epoch 83/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2065 - siamese_loss: 5.0036 - mlp_loss: 0.2065 - mlp_binary_accuracy: 0.9159 - mlp_auc_36: 0.9735\n",
      "Epoch 84/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1899 - siamese_loss: 5.0386 - mlp_loss: 0.1899 - mlp_binary_accuracy: 0.9242 - mlp_auc_36: 0.9777\n",
      "Epoch 85/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1902 - siamese_loss: 5.1653 - mlp_loss: 0.1902 - mlp_binary_accuracy: 0.9198 - mlp_auc_36: 0.9776\n",
      "Epoch 86/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1796 - siamese_loss: 5.1617 - mlp_loss: 0.1796 - mlp_binary_accuracy: 0.9290 - mlp_auc_36: 0.9794\n",
      "Epoch 87/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1829 - siamese_loss: 5.1327 - mlp_loss: 0.1829 - mlp_binary_accuracy: 0.9279 - mlp_auc_36: 0.9790\n",
      "Epoch 88/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1858 - siamese_loss: 5.1111 - mlp_loss: 0.1858 - mlp_binary_accuracy: 0.9309 - mlp_auc_36: 0.9782\n",
      "Epoch 89/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1785 - siamese_loss: 5.1997 - mlp_loss: 0.1785 - mlp_binary_accuracy: 0.9286 - mlp_auc_36: 0.9799\n",
      "Epoch 90/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1828 - siamese_loss: 5.0371 - mlp_loss: 0.1828 - mlp_binary_accuracy: 0.9237 - mlp_auc_36: 0.9797\n",
      "Epoch 91/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1814 - siamese_loss: 5.0811 - mlp_loss: 0.1814 - mlp_binary_accuracy: 0.9297 - mlp_auc_36: 0.9794\n",
      "Epoch 92/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1785 - siamese_loss: 5.1014 - mlp_loss: 0.1785 - mlp_binary_accuracy: 0.9286 - mlp_auc_36: 0.9799\n",
      "Epoch 93/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1793 - siamese_loss: 5.1916 - mlp_loss: 0.1793 - mlp_binary_accuracy: 0.9258 - mlp_auc_36: 0.9802\n",
      "Epoch 94/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1904 - siamese_loss: 5.1626 - mlp_loss: 0.1904 - mlp_binary_accuracy: 0.9279 - mlp_auc_36: 0.9774\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5867 - siamese_loss: 5.2233 - mlp_loss: 0.5867 - mlp_binary_accuracy: 0.7950 - mlp_auc_36: 0.8733    \n",
      "------ mlp_binary_accuracy: 79.50%\t ----- mlp_auc_36: 87.33%\n",
      "concat_fea的shape： (None, 112)\n",
      "-------------------------------------Kfold: 10 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "  1/137 [..............................] - ETA: 0s - loss: 0.7162 - siamese_loss: 3.3193 - mlp_loss: 0.7162 - mlp_binary_accuracy: 0.4375 - mlp_auc_37: 0.3869WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6810 - siamese_loss: 3.2426 - mlp_loss: 0.6810 - mlp_binary_accuracy: 0.5415 - mlp_auc_37: 0.6114\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6440 - siamese_loss: 3.6223 - mlp_loss: 0.6440 - mlp_binary_accuracy: 0.6402 - mlp_auc_37: 0.6802\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6051 - siamese_loss: 3.6463 - mlp_loss: 0.6051 - mlp_binary_accuracy: 0.6823 - mlp_auc_37: 0.7179\n",
      "Epoch 4/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5870 - siamese_loss: 3.7701 - mlp_loss: 0.5870 - mlp_binary_accuracy: 0.6912 - mlp_auc_37: 0.7455\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5691 - siamese_loss: 3.8339 - mlp_loss: 0.5691 - mlp_binary_accuracy: 0.7078 - mlp_auc_37: 0.7638\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5570 - siamese_loss: 4.0497 - mlp_loss: 0.5570 - mlp_binary_accuracy: 0.7177 - mlp_auc_37: 0.7806\n",
      "Epoch 7/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5477 - siamese_loss: 4.0340 - mlp_loss: 0.5477 - mlp_binary_accuracy: 0.7257 - mlp_auc_37: 0.7915\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5338 - siamese_loss: 4.3114 - mlp_loss: 0.5338 - mlp_binary_accuracy: 0.7365 - mlp_auc_37: 0.8052\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5240 - siamese_loss: 4.4460 - mlp_loss: 0.5240 - mlp_binary_accuracy: 0.7482 - mlp_auc_37: 0.8152\n",
      "Epoch 10/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5190 - siamese_loss: 4.4444 - mlp_loss: 0.5190 - mlp_binary_accuracy: 0.7466 - mlp_auc_37: 0.8184\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4983 - siamese_loss: 4.4838 - mlp_loss: 0.4983 - mlp_binary_accuracy: 0.7549 - mlp_auc_37: 0.8367\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4954 - siamese_loss: 4.6026 - mlp_loss: 0.4954 - mlp_binary_accuracy: 0.7634 - mlp_auc_37: 0.8379\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4905 - siamese_loss: 4.5449 - mlp_loss: 0.4905 - mlp_binary_accuracy: 0.7677 - mlp_auc_37: 0.8438\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4843 - siamese_loss: 4.5160 - mlp_loss: 0.4843 - mlp_binary_accuracy: 0.7710 - mlp_auc_37: 0.8486\n",
      "Epoch 15/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4708 - siamese_loss: 4.7808 - mlp_loss: 0.4708 - mlp_binary_accuracy: 0.7806 - mlp_auc_37: 0.8572\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4679 - siamese_loss: 4.8479 - mlp_loss: 0.4679 - mlp_binary_accuracy: 0.7799 - mlp_auc_37: 0.8590\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4651 - siamese_loss: 4.9101 - mlp_loss: 0.4651 - mlp_binary_accuracy: 0.7795 - mlp_auc_37: 0.8603\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4473 - siamese_loss: 4.8072 - mlp_loss: 0.4473 - mlp_binary_accuracy: 0.7907 - mlp_auc_37: 0.8732\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4375 - siamese_loss: 4.8969 - mlp_loss: 0.4375 - mlp_binary_accuracy: 0.7997 - mlp_auc_37: 0.8789\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4381 - siamese_loss: 5.0220 - mlp_loss: 0.4381 - mlp_binary_accuracy: 0.8004 - mlp_auc_37: 0.8787\n",
      "Epoch 21/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4321 - siamese_loss: 4.9908 - mlp_loss: 0.4321 - mlp_binary_accuracy: 0.8070 - mlp_auc_37: 0.8829\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4386 - siamese_loss: 4.8440 - mlp_loss: 0.4386 - mlp_binary_accuracy: 0.7978 - mlp_auc_37: 0.8780\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4321 - siamese_loss: 4.7949 - mlp_loss: 0.4321 - mlp_binary_accuracy: 0.8013 - mlp_auc_37: 0.8822\n",
      "Epoch 24/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4178 - siamese_loss: 4.8359 - mlp_loss: 0.4178 - mlp_binary_accuracy: 0.8142 - mlp_auc_37: 0.8908\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4095 - siamese_loss: 4.9506 - mlp_loss: 0.4095 - mlp_binary_accuracy: 0.8142 - mlp_auc_37: 0.8955\n",
      "Epoch 26/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4020 - siamese_loss: 4.9237 - mlp_loss: 0.4020 - mlp_binary_accuracy: 0.8187 - mlp_auc_37: 0.8989\n",
      "Epoch 27/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4019 - siamese_loss: 4.8971 - mlp_loss: 0.4019 - mlp_binary_accuracy: 0.8178 - mlp_auc_37: 0.8995\n",
      "Epoch 28/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3943 - siamese_loss: 4.9038 - mlp_loss: 0.3943 - mlp_binary_accuracy: 0.8231 - mlp_auc_37: 0.9034\n",
      "Epoch 29/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3886 - siamese_loss: 5.0530 - mlp_loss: 0.3886 - mlp_binary_accuracy: 0.8275 - mlp_auc_37: 0.9059\n",
      "Epoch 30/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3896 - siamese_loss: 4.8345 - mlp_loss: 0.3896 - mlp_binary_accuracy: 0.8270 - mlp_auc_37: 0.9057\n",
      "Epoch 31/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3741 - siamese_loss: 4.9081 - mlp_loss: 0.3741 - mlp_binary_accuracy: 0.8376 - mlp_auc_37: 0.9135\n",
      "Epoch 32/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3671 - siamese_loss: 4.9714 - mlp_loss: 0.3671 - mlp_binary_accuracy: 0.8392 - mlp_auc_37: 0.9166\n",
      "Epoch 33/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3715 - siamese_loss: 5.1066 - mlp_loss: 0.3715 - mlp_binary_accuracy: 0.8348 - mlp_auc_37: 0.9147\n",
      "Epoch 34/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3565 - siamese_loss: 5.0243 - mlp_loss: 0.3565 - mlp_binary_accuracy: 0.8429 - mlp_auc_37: 0.9220\n",
      "Epoch 35/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3476 - siamese_loss: 4.8817 - mlp_loss: 0.3476 - mlp_binary_accuracy: 0.8516 - mlp_auc_37: 0.9260\n",
      "Epoch 36/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3497 - siamese_loss: 4.9298 - mlp_loss: 0.3497 - mlp_binary_accuracy: 0.8472 - mlp_auc_37: 0.9250\n",
      "Epoch 37/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3454 - siamese_loss: 4.8148 - mlp_loss: 0.3454 - mlp_binary_accuracy: 0.8511 - mlp_auc_37: 0.9267\n",
      "Epoch 38/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3424 - siamese_loss: 4.8653 - mlp_loss: 0.3424 - mlp_binary_accuracy: 0.8546 - mlp_auc_37: 0.9279\n",
      "Epoch 39/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3443 - siamese_loss: 4.8471 - mlp_loss: 0.3443 - mlp_binary_accuracy: 0.8498 - mlp_auc_37: 0.9270\n",
      "Epoch 40/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3371 - siamese_loss: 4.9371 - mlp_loss: 0.3371 - mlp_binary_accuracy: 0.8562 - mlp_auc_37: 0.9303\n",
      "Epoch 41/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3268 - siamese_loss: 4.9064 - mlp_loss: 0.3268 - mlp_binary_accuracy: 0.8622 - mlp_auc_37: 0.9348\n",
      "Epoch 42/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3253 - siamese_loss: 4.9690 - mlp_loss: 0.3253 - mlp_binary_accuracy: 0.8603 - mlp_auc_37: 0.9353\n",
      "Epoch 43/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3203 - siamese_loss: 5.0949 - mlp_loss: 0.3203 - mlp_binary_accuracy: 0.8663 - mlp_auc_37: 0.9372\n",
      "Epoch 44/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3293 - siamese_loss: 4.7934 - mlp_loss: 0.3293 - mlp_binary_accuracy: 0.8583 - mlp_auc_37: 0.9336\n",
      "Epoch 45/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3245 - siamese_loss: 4.6912 - mlp_loss: 0.3245 - mlp_binary_accuracy: 0.8576 - mlp_auc_37: 0.9359\n",
      "Epoch 46/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3053 - siamese_loss: 4.8879 - mlp_loss: 0.3053 - mlp_binary_accuracy: 0.8693 - mlp_auc_37: 0.9429\n",
      "Epoch 47/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3021 - siamese_loss: 4.9308 - mlp_loss: 0.3021 - mlp_binary_accuracy: 0.8755 - mlp_auc_37: 0.9441\n",
      "Epoch 48/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2965 - siamese_loss: 5.0883 - mlp_loss: 0.2965 - mlp_binary_accuracy: 0.8748 - mlp_auc_37: 0.9463\n",
      "Epoch 49/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2958 - siamese_loss: 5.2421 - mlp_loss: 0.2958 - mlp_binary_accuracy: 0.8787 - mlp_auc_37: 0.9462\n",
      "Epoch 50/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3080 - siamese_loss: 5.2111 - mlp_loss: 0.3080 - mlp_binary_accuracy: 0.8702 - mlp_auc_37: 0.9419\n",
      "Epoch 51/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2903 - siamese_loss: 5.2672 - mlp_loss: 0.2903 - mlp_binary_accuracy: 0.8746 - mlp_auc_37: 0.9487\n",
      "Epoch 52/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2902 - siamese_loss: 5.3868 - mlp_loss: 0.2902 - mlp_binary_accuracy: 0.8833 - mlp_auc_37: 0.9486\n",
      "Epoch 53/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2802 - siamese_loss: 5.2978 - mlp_loss: 0.2802 - mlp_binary_accuracy: 0.8812 - mlp_auc_37: 0.9521\n",
      "Epoch 54/200\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.2854 - siamese_loss: 5.3540 - mlp_loss: 0.2854 - mlp_binary_accuracy: 0.8782 - mlp_auc_37: 0.9502\n",
      "Epoch 55/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2725 - siamese_loss: 5.1963 - mlp_loss: 0.2725 - mlp_binary_accuracy: 0.8877 - mlp_auc_37: 0.9545\n",
      "Epoch 56/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2805 - siamese_loss: 5.3462 - mlp_loss: 0.2805 - mlp_binary_accuracy: 0.8810 - mlp_auc_37: 0.9518\n",
      "Epoch 57/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2826 - siamese_loss: 5.4784 - mlp_loss: 0.2826 - mlp_binary_accuracy: 0.8819 - mlp_auc_37: 0.9510\n",
      "Epoch 58/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2776 - siamese_loss: 5.5866 - mlp_loss: 0.2776 - mlp_binary_accuracy: 0.8874 - mlp_auc_37: 0.9528\n",
      "Epoch 59/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2666 - siamese_loss: 5.3502 - mlp_loss: 0.2666 - mlp_binary_accuracy: 0.8916 - mlp_auc_37: 0.9565\n",
      "Epoch 60/200\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.2614 - siamese_loss: 5.2125 - mlp_loss: 0.2614 - mlp_binary_accuracy: 0.8900 - mlp_auc_37: 0.9584\n",
      "Epoch 61/200\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.2550 - siamese_loss: 5.3814 - mlp_loss: 0.2550 - mlp_binary_accuracy: 0.8971 - mlp_auc_37: 0.9597\n",
      "Epoch 62/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2513 - siamese_loss: 5.3023 - mlp_loss: 0.2513 - mlp_binary_accuracy: 0.8943 - mlp_auc_37: 0.9614\n",
      "Epoch 63/200\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.2609 - siamese_loss: 5.2530 - mlp_loss: 0.2609 - mlp_binary_accuracy: 0.8918 - mlp_auc_37: 0.9583\n",
      "Epoch 64/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2523 - siamese_loss: 5.3098 - mlp_loss: 0.2523 - mlp_binary_accuracy: 0.8978 - mlp_auc_37: 0.9609\n",
      "Epoch 65/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2536 - siamese_loss: 5.6110 - mlp_loss: 0.2536 - mlp_binary_accuracy: 0.8982 - mlp_auc_37: 0.9603\n",
      "Epoch 66/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2537 - siamese_loss: 5.4292 - mlp_loss: 0.2537 - mlp_binary_accuracy: 0.9003 - mlp_auc_37: 0.9603\n",
      "Epoch 67/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2443 - siamese_loss: 5.2923 - mlp_loss: 0.2443 - mlp_binary_accuracy: 0.9008 - mlp_auc_37: 0.9632\n",
      "Epoch 68/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2440 - siamese_loss: 5.3979 - mlp_loss: 0.2440 - mlp_binary_accuracy: 0.9003 - mlp_auc_37: 0.9637\n",
      "Epoch 69/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2438 - siamese_loss: 5.2863 - mlp_loss: 0.2438 - mlp_binary_accuracy: 0.9001 - mlp_auc_37: 0.9636\n",
      "Epoch 70/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2420 - siamese_loss: 5.3397 - mlp_loss: 0.2420 - mlp_binary_accuracy: 0.9017 - mlp_auc_37: 0.9640\n",
      "Epoch 71/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2464 - siamese_loss: 5.4755 - mlp_loss: 0.2464 - mlp_binary_accuracy: 0.9033 - mlp_auc_37: 0.9627\n",
      "Epoch 72/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2332 - siamese_loss: 5.3362 - mlp_loss: 0.2332 - mlp_binary_accuracy: 0.9035 - mlp_auc_37: 0.9666\n",
      "Epoch 73/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2287 - siamese_loss: 5.4467 - mlp_loss: 0.2287 - mlp_binary_accuracy: 0.9097 - mlp_auc_37: 0.9679\n",
      "Epoch 74/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2158 - siamese_loss: 5.3729 - mlp_loss: 0.2158 - mlp_binary_accuracy: 0.9122 - mlp_auc_37: 0.9715\n",
      "Epoch 75/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2306 - siamese_loss: 5.4348 - mlp_loss: 0.2306 - mlp_binary_accuracy: 0.9049 - mlp_auc_37: 0.9673\n",
      "Epoch 76/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2133 - siamese_loss: 5.3849 - mlp_loss: 0.2133 - mlp_binary_accuracy: 0.9148 - mlp_auc_37: 0.9719\n",
      "Epoch 77/200\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.2234 - siamese_loss: 5.4232 - mlp_loss: 0.2234 - mlp_binary_accuracy: 0.9109 - mlp_auc_37: 0.9695\n",
      "Epoch 78/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2029 - siamese_loss: 5.3953 - mlp_loss: 0.2029 - mlp_binary_accuracy: 0.9175 - mlp_auc_37: 0.9746\n",
      "Epoch 79/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2272 - siamese_loss: 5.4633 - mlp_loss: 0.2272 - mlp_binary_accuracy: 0.9026 - mlp_auc_37: 0.9685\n",
      "Epoch 80/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2004 - siamese_loss: 5.4674 - mlp_loss: 0.2004 - mlp_binary_accuracy: 0.9201 - mlp_auc_37: 0.9753\n",
      "Epoch 81/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2095 - siamese_loss: 5.4838 - mlp_loss: 0.2095 - mlp_binary_accuracy: 0.9173 - mlp_auc_37: 0.9729\n",
      "Epoch 82/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2026 - siamese_loss: 5.6013 - mlp_loss: 0.2026 - mlp_binary_accuracy: 0.9217 - mlp_auc_37: 0.9745\n",
      "Epoch 83/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2141 - siamese_loss: 5.6009 - mlp_loss: 0.2141 - mlp_binary_accuracy: 0.9136 - mlp_auc_37: 0.9717\n",
      "Epoch 84/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2087 - siamese_loss: 5.4949 - mlp_loss: 0.2087 - mlp_binary_accuracy: 0.9155 - mlp_auc_37: 0.9732\n",
      "Epoch 85/200\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.1980 - siamese_loss: 5.4829 - mlp_loss: 0.1980 - mlp_binary_accuracy: 0.9191 - mlp_auc_37: 0.9759\n",
      "Epoch 86/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1967 - siamese_loss: 5.5825 - mlp_loss: 0.1967 - mlp_binary_accuracy: 0.9221 - mlp_auc_37: 0.9761\n",
      "Epoch 87/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2041 - siamese_loss: 5.6441 - mlp_loss: 0.2041 - mlp_binary_accuracy: 0.9180 - mlp_auc_37: 0.9744\n",
      "Epoch 88/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1921 - siamese_loss: 5.6088 - mlp_loss: 0.1921 - mlp_binary_accuracy: 0.9242 - mlp_auc_37: 0.9773\n",
      "Epoch 89/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1957 - siamese_loss: 5.5354 - mlp_loss: 0.1957 - mlp_binary_accuracy: 0.9253 - mlp_auc_37: 0.9762\n",
      "Epoch 90/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1802 - siamese_loss: 5.4477 - mlp_loss: 0.1802 - mlp_binary_accuracy: 0.9279 - mlp_auc_37: 0.9800\n",
      "Epoch 91/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1908 - siamese_loss: 5.5092 - mlp_loss: 0.1908 - mlp_binary_accuracy: 0.9251 - mlp_auc_37: 0.9774\n",
      "Epoch 92/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1901 - siamese_loss: 5.4746 - mlp_loss: 0.1901 - mlp_binary_accuracy: 0.9233 - mlp_auc_37: 0.9778\n",
      "Epoch 93/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1890 - siamese_loss: 5.6036 - mlp_loss: 0.1890 - mlp_binary_accuracy: 0.9258 - mlp_auc_37: 0.9777\n",
      "Epoch 94/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1931 - siamese_loss: 5.5951 - mlp_loss: 0.1931 - mlp_binary_accuracy: 0.9251 - mlp_auc_37: 0.9767\n",
      "Epoch 95/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1850 - siamese_loss: 5.6419 - mlp_loss: 0.1850 - mlp_binary_accuracy: 0.9258 - mlp_auc_37: 0.9786\n",
      "16/16 [==============================] - 0s 984us/step - loss: 0.5902 - siamese_loss: 5.6321 - mlp_loss: 0.5902 - mlp_binary_accuracy: 0.8199 - mlp_auc_37: 0.8840  \n",
      "------ mlp_binary_accuracy: 81.99%\t ----- mlp_auc_37: 88.40%\n",
      "十折性能均值：-------- ave_acc: 80.13% (+/- 1.24%)\t ----- ave_auc_mlp:87.90% (+/- 0.86%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics  import roc_curve,auc,roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "cvacc,cvauc_mlp,cvpre,cvpre_top,cvrec,cvauc_sia=[],[],[],[],[],[]\n",
    "i =1\n",
    "\n",
    "y=np.array(y)\n",
    "disA_fea_mat=np.array(disA_fea_mat)\n",
    "disB_fea_mat=np.array(disB_fea_mat)\n",
    "\n",
    "mesh_label_A = np.array(mesh_label_A)\n",
    "mesh_label_B = np.array(mesh_label_B)\n",
    "\n",
    "neg_to_pos=1 #2418/32827\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy=neg_to_pos,random_state=11)\n",
    "rus_dfmA,rus_y = rus.fit_resample(X=disA_fea_mat,y=y)\n",
    "rus_dfmB,rus_y = rus.fit_resample(X=disB_fea_mat,y=y)\n",
    "print('rus_y的长度:',len(rus_y))\n",
    "rus_mesh_A,rus_y3= rus.fit_resample(X=mesh_label_A,y=y)\n",
    "rus_mesh_B,rus_y4 = rus.fit_resample(X=mesh_label_B,y=y)\n",
    "print(\"rus_mesh_B的长度：\",len(rus_mesh_B))\n",
    "\n",
    "\n",
    "seed = 201202\n",
    "#!PYTHONHASHSEED=0 # disable？？？\n",
    "np.random.seed(seed) # This is necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "python_random.seed(seed)# This is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "Kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\n",
    "for train,test in Kfold.split(X=rus_dfmA,y=rus_y):\n",
    "    ###############################################################################################\n",
    "    \"\"\"create model\"\"\"\n",
    "    input_shape=(322)\n",
    "    base_network = create_base_network(input_shape)\n",
    "    \n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "    input_mesh_a = Input(shape=24)\n",
    "    input_mesh_b = Input(shape=24)\n",
    "    \n",
    "    # because we re-use the same instance `base_network`,the weights of the network will be shared across the two branches\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    \n",
    "    eucl_model = create_euclLayer(processed_a.shape[-1])\n",
    "    eucl_out = eucl_model([processed_a,processed_b]) \n",
    "    \n",
    "    \n",
    "    concat_fea = tf.keras.layers.concatenate([processed_a,input_mesh_a,processed_b,input_mesh_b],axis=-1)\n",
    "    print(\"concat_fea的shape：\",concat_fea.shape) #(None,32)\n",
    "    MLP = create_MLP(concat_fea.shape[-1])\n",
    "    MLP_out = MLP(concat_fea)\n",
    "    \n",
    "    model = Model(inputs=[input_a, input_b,input_mesh_a,input_mesh_b], outputs=[eucl_out,MLP_out])\n",
    "    \n",
    "    \"\"\"L2\"\"\"\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer,'kernel_regularizer'):\n",
    "            layer.kernel_regularizer = tf.keras.regularizers.l2(0.001)\n",
    "        \n",
    "    #####################################################################################################################\n",
    "\n",
    "    print(\"-------------------------------------Kfold: {} iter-----------------------------------------\".format(i))\n",
    "    i+=1\n",
    "\n",
    "    rms = RMSprop()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=5)\n",
    "    \n",
    "    my_loss = {\"siamese\":contrastive_loss,\"mlp\":tf.keras.losses.BinaryCrossentropy()}\n",
    "    my_loss_weight = {\"siamese\":0,\"mlp\":1}\n",
    "    my_metrics ={ \"mlp\":[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()]} #tf.keras.metrics.Precision(top_k=len(rus_y)//100)\n",
    "    \n",
    "    model.compile(\n",
    "#         loss= [tfa.losses.ContrastiveLoss(),tf.keras.losses.BinaryCrossentropy()],\n",
    "#         loss_weights=[0.01,1],\n",
    "#         metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()],\n",
    "        \n",
    "        loss= my_loss,\n",
    "        loss_weights=my_loss_weight,\n",
    "        metrics=my_metrics,\n",
    "        \n",
    "        optimizer=rms,\n",
    "    )\n",
    "    \n",
    "    # fit()中 shuffle=True\n",
    "    model.fit([rus_dfmA[train],rus_dfmB[train],rus_mesh_A[train],rus_mesh_B[train]],\n",
    "              [rus_y[train],rus_y[train]],\n",
    "              \n",
    "              batch_size=None,#默认32\n",
    "              epochs=200,\n",
    "              callbacks=callback,\n",
    "              shuffle=True,\n",
    "             )\n",
    "    \n",
    "    scores = model.evaluate([rus_dfmA[test],rus_dfmB[test],rus_mesh_A[test],rus_mesh_B[test]],\n",
    "                            [rus_y[test],rus_y[test]],\n",
    "                            verbose=1,\n",
    "                            batch_size=32,\n",
    "                            callbacks=callback)\n",
    "    \n",
    "    cvacc.append(scores[-2] * 100)\n",
    "    cvauc_mlp.append(scores[-1] * 100)\n",
    "    cvauc_sia.append(scores[-3] * 100)\n",
    "    \n",
    "    print(\"------ %s: %.2f%%\\t ----- %s: %.2f%%\" % \n",
    "           (model.metrics_names[-2],scores[-2]*100, model.metrics_names[-1],scores[-1]*100))\n",
    "     \n",
    "print(\"十折性能均值：-------- ave_acc: %.2f%% (+/- %.2f%%)\\t ----- ave_auc_mlp:%.2f%% (+/- %.2f%%)\" % \n",
    "      (np.mean(cvacc), np.std(cvacc),np.mean(cvauc_mlp), np.std(cvauc_mlp)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------- ave_acc: 80.13% (+/- 1.24%)\t ----- ave_auc_mlp:87.90% (+/- 0.86%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly divide 20% as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus_y的长度: 4836\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold,train_test_split\n",
    "from sklearn.metrics  import roc_curve,auc,roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed) # This is necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "python_random.seed(seed)# This is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "neg_to_pos= 1\n",
    "rus = RandomUnderSampler(sampling_strategy=neg_to_pos,random_state=seed) \n",
    "rus_dfmA,rus_y = rus.fit_resample(X=disA_fea_mat,y=y)\n",
    "rus_dfmB,rus_y = rus.fit_resample(X=disB_fea_mat,y=y)\n",
    "\n",
    "rus_mesh_A,rus_y = rus.fit_resample(X=mesh_label_A,y=y)\n",
    "rus_mesh_B,rus_y = rus.fit_resample(X=mesh_label_B,y=y)\n",
    "\n",
    "print('rus_y的长度:',len(rus_y))\n",
    "\n",
    "\"\"\"split 20%\"\"\"\n",
    "symA_train,symA_test,symB_train,symB_test,meshA_train,meshA_test,meshB_train,meshB_test,y_train,y_test=train_test_split(\n",
    "                rus_dfmA,rus_dfmB,\n",
    "                rus_mesh_A,rus_mesh_B,#mesh_label_A[-5515:-1],mesh_label_B[-5515:-1],\n",
    "                rus_y,\n",
    "                test_size=0.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus_y的长度: 4836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'PYTHONHASHSEED' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat_fea的shape： (None, 112)\n",
      "-------------------------------------Train start-----------------------------------------\n",
      "Epoch 1/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.6712 - siamese_loss: 3.3297 - mlp_loss: 0.6712 - mlp_binary_accuracy: 0.5752 - mlp_auc_41: 0.6148\n",
      "Epoch 2/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.6203 - siamese_loss: 3.7938 - mlp_loss: 0.6203 - mlp_binary_accuracy: 0.6569 - mlp_auc_41: 0.7136\n",
      "Epoch 3/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.5791 - siamese_loss: 3.8642 - mlp_loss: 0.5791 - mlp_binary_accuracy: 0.6916 - mlp_auc_41: 0.7623\n",
      "Epoch 4/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.5612 - siamese_loss: 3.9855 - mlp_loss: 0.5612 - mlp_binary_accuracy: 0.7143 - mlp_auc_41: 0.7816\n",
      "Epoch 5/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.5492 - siamese_loss: 4.1666 - mlp_loss: 0.5492 - mlp_binary_accuracy: 0.7306 - mlp_auc_41: 0.7936\n",
      "Epoch 6/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.5332 - siamese_loss: 4.2453 - mlp_loss: 0.5332 - mlp_binary_accuracy: 0.7386 - mlp_auc_41: 0.8078\n",
      "Epoch 7/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.5319 - siamese_loss: 4.1737 - mlp_loss: 0.5319 - mlp_binary_accuracy: 0.7459 - mlp_auc_41: 0.8089\n",
      "Epoch 8/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.5183 - siamese_loss: 4.1418 - mlp_loss: 0.5183 - mlp_binary_accuracy: 0.7549 - mlp_auc_41: 0.8214\n",
      "Epoch 9/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.5036 - siamese_loss: 4.4094 - mlp_loss: 0.5036 - mlp_binary_accuracy: 0.7565 - mlp_auc_41: 0.8325\n",
      "Epoch 10/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.5012 - siamese_loss: 4.4555 - mlp_loss: 0.5012 - mlp_binary_accuracy: 0.7559 - mlp_auc_41: 0.8355\n",
      "Epoch 11/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4913 - siamese_loss: 4.2889 - mlp_loss: 0.4913 - mlp_binary_accuracy: 0.7616 - mlp_auc_41: 0.8427\n",
      "Epoch 12/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4845 - siamese_loss: 4.3137 - mlp_loss: 0.4845 - mlp_binary_accuracy: 0.7632 - mlp_auc_41: 0.8470\n",
      "Epoch 13/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4735 - siamese_loss: 4.4651 - mlp_loss: 0.4735 - mlp_binary_accuracy: 0.7844 - mlp_auc_41: 0.8560\n",
      "Epoch 14/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4642 - siamese_loss: 4.4805 - mlp_loss: 0.4642 - mlp_binary_accuracy: 0.7883 - mlp_auc_41: 0.8613\n",
      "Epoch 15/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4523 - siamese_loss: 4.5883 - mlp_loss: 0.4523 - mlp_binary_accuracy: 0.7854 - mlp_auc_41: 0.8688\n",
      "Epoch 16/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4589 - siamese_loss: 4.6759 - mlp_loss: 0.4589 - mlp_binary_accuracy: 0.7818 - mlp_auc_41: 0.8657\n",
      "Epoch 17/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4514 - siamese_loss: 4.5663 - mlp_loss: 0.4514 - mlp_binary_accuracy: 0.7854 - mlp_auc_41: 0.8701\n",
      "Epoch 18/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4402 - siamese_loss: 4.8087 - mlp_loss: 0.4402 - mlp_binary_accuracy: 0.7978 - mlp_auc_41: 0.8776\n",
      "Epoch 19/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4366 - siamese_loss: 4.6539 - mlp_loss: 0.4366 - mlp_binary_accuracy: 0.7965 - mlp_auc_41: 0.8785\n",
      "Epoch 20/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4248 - siamese_loss: 4.5450 - mlp_loss: 0.4248 - mlp_binary_accuracy: 0.8097 - mlp_auc_41: 0.8865\n",
      "Epoch 21/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4229 - siamese_loss: 4.4213 - mlp_loss: 0.4229 - mlp_binary_accuracy: 0.8022 - mlp_auc_41: 0.8872\n",
      "Epoch 22/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4133 - siamese_loss: 4.6048 - mlp_loss: 0.4133 - mlp_binary_accuracy: 0.8128 - mlp_auc_41: 0.8932\n",
      "Epoch 23/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4175 - siamese_loss: 4.8372 - mlp_loss: 0.4175 - mlp_binary_accuracy: 0.8126 - mlp_auc_41: 0.8908\n",
      "Epoch 24/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4091 - siamese_loss: 4.9109 - mlp_loss: 0.4091 - mlp_binary_accuracy: 0.8193 - mlp_auc_41: 0.8956\n",
      "Epoch 25/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3989 - siamese_loss: 4.7558 - mlp_loss: 0.3989 - mlp_binary_accuracy: 0.8273 - mlp_auc_41: 0.9008\n",
      "Epoch 26/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3941 - siamese_loss: 4.6383 - mlp_loss: 0.3941 - mlp_binary_accuracy: 0.8242 - mlp_auc_41: 0.9034\n",
      "Epoch 27/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3854 - siamese_loss: 4.7301 - mlp_loss: 0.3854 - mlp_binary_accuracy: 0.8252 - mlp_auc_41: 0.9076\n",
      "Epoch 28/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3730 - siamese_loss: 4.7091 - mlp_loss: 0.3730 - mlp_binary_accuracy: 0.8299 - mlp_auc_41: 0.9137\n",
      "Epoch 29/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3715 - siamese_loss: 4.8160 - mlp_loss: 0.3715 - mlp_binary_accuracy: 0.8369 - mlp_auc_41: 0.9149\n",
      "Epoch 30/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3639 - siamese_loss: 4.6634 - mlp_loss: 0.3639 - mlp_binary_accuracy: 0.8392 - mlp_auc_41: 0.9181\n",
      "Epoch 31/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3598 - siamese_loss: 4.7625 - mlp_loss: 0.3598 - mlp_binary_accuracy: 0.8469 - mlp_auc_41: 0.9195\n",
      "Epoch 32/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3588 - siamese_loss: 4.7669 - mlp_loss: 0.3588 - mlp_binary_accuracy: 0.8400 - mlp_auc_41: 0.9205\n",
      "Epoch 33/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3483 - siamese_loss: 4.8642 - mlp_loss: 0.3483 - mlp_binary_accuracy: 0.8488 - mlp_auc_41: 0.9249\n",
      "Epoch 34/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3421 - siamese_loss: 4.8755 - mlp_loss: 0.3421 - mlp_binary_accuracy: 0.8583 - mlp_auc_41: 0.9277\n",
      "Epoch 35/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3318 - siamese_loss: 4.9277 - mlp_loss: 0.3318 - mlp_binary_accuracy: 0.8604 - mlp_auc_41: 0.9321\n",
      "Epoch 36/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3401 - siamese_loss: 4.9522 - mlp_loss: 0.3401 - mlp_binary_accuracy: 0.8542 - mlp_auc_41: 0.9293\n",
      "Epoch 37/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3304 - siamese_loss: 4.7725 - mlp_loss: 0.3304 - mlp_binary_accuracy: 0.8537 - mlp_auc_41: 0.9327\n",
      "Epoch 38/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3256 - siamese_loss: 4.7507 - mlp_loss: 0.3256 - mlp_binary_accuracy: 0.8596 - mlp_auc_41: 0.9350\n",
      "Epoch 39/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3048 - siamese_loss: 4.7099 - mlp_loss: 0.3048 - mlp_binary_accuracy: 0.8713 - mlp_auc_41: 0.9431\n",
      "Epoch 40/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3192 - siamese_loss: 4.7511 - mlp_loss: 0.3192 - mlp_binary_accuracy: 0.8635 - mlp_auc_41: 0.9373\n",
      "Epoch 41/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3148 - siamese_loss: 4.9135 - mlp_loss: 0.3148 - mlp_binary_accuracy: 0.8638 - mlp_auc_41: 0.9393\n",
      "Epoch 42/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3083 - siamese_loss: 4.9043 - mlp_loss: 0.3083 - mlp_binary_accuracy: 0.8663 - mlp_auc_41: 0.9416\n",
      "Epoch 43/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2964 - siamese_loss: 4.8879 - mlp_loss: 0.2964 - mlp_binary_accuracy: 0.8798 - mlp_auc_41: 0.9459\n",
      "Epoch 44/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3051 - siamese_loss: 4.9533 - mlp_loss: 0.3051 - mlp_binary_accuracy: 0.8728 - mlp_auc_41: 0.9427\n",
      "Epoch 45/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2913 - siamese_loss: 4.9929 - mlp_loss: 0.2913 - mlp_binary_accuracy: 0.8725 - mlp_auc_41: 0.9478\n",
      "Epoch 46/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2799 - siamese_loss: 4.9953 - mlp_loss: 0.2799 - mlp_binary_accuracy: 0.8811 - mlp_auc_41: 0.9523\n",
      "Epoch 47/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2809 - siamese_loss: 4.9583 - mlp_loss: 0.2809 - mlp_binary_accuracy: 0.8746 - mlp_auc_41: 0.9517\n",
      "Epoch 48/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2758 - siamese_loss: 5.0167 - mlp_loss: 0.2758 - mlp_binary_accuracy: 0.8837 - mlp_auc_41: 0.9537\n",
      "Epoch 49/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2741 - siamese_loss: 5.0335 - mlp_loss: 0.2741 - mlp_binary_accuracy: 0.8886 - mlp_auc_41: 0.9537\n",
      "Epoch 50/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2659 - siamese_loss: 5.0635 - mlp_loss: 0.2659 - mlp_binary_accuracy: 0.8901 - mlp_auc_41: 0.9567\n",
      "Epoch 51/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2703 - siamese_loss: 5.1671 - mlp_loss: 0.2703 - mlp_binary_accuracy: 0.8875 - mlp_auc_41: 0.9554\n",
      "Epoch 52/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2545 - siamese_loss: 5.1386 - mlp_loss: 0.2545 - mlp_binary_accuracy: 0.8899 - mlp_auc_41: 0.9604\n",
      "Epoch 53/200\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.2671 - siamese_loss: 5.1078 - mlp_loss: 0.2671 - mlp_binary_accuracy: 0.8837 - mlp_auc_41: 0.9564\n",
      "Epoch 54/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2596 - siamese_loss: 5.0160 - mlp_loss: 0.2596 - mlp_binary_accuracy: 0.8899 - mlp_auc_41: 0.9582\n",
      "Epoch 55/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2519 - siamese_loss: 5.1009 - mlp_loss: 0.2519 - mlp_binary_accuracy: 0.8948 - mlp_auc_41: 0.9607\n",
      "Epoch 56/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2638 - siamese_loss: 5.0831 - mlp_loss: 0.2638 - mlp_binary_accuracy: 0.8899 - mlp_auc_41: 0.9572\n",
      "Epoch 57/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2441 - siamese_loss: 5.0296 - mlp_loss: 0.2441 - mlp_binary_accuracy: 0.8984 - mlp_auc_41: 0.9636\n",
      "Epoch 58/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2400 - siamese_loss: 4.9994 - mlp_loss: 0.2400 - mlp_binary_accuracy: 0.9015 - mlp_auc_41: 0.9646\n",
      "Epoch 59/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2477 - siamese_loss: 4.9244 - mlp_loss: 0.2477 - mlp_binary_accuracy: 0.8989 - mlp_auc_41: 0.9625\n",
      "Epoch 60/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2435 - siamese_loss: 5.0425 - mlp_loss: 0.2435 - mlp_binary_accuracy: 0.8994 - mlp_auc_41: 0.9638\n",
      "Epoch 61/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2369 - siamese_loss: 4.9489 - mlp_loss: 0.2369 - mlp_binary_accuracy: 0.9018 - mlp_auc_41: 0.9655\n",
      "Epoch 62/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2342 - siamese_loss: 5.0212 - mlp_loss: 0.2342 - mlp_binary_accuracy: 0.9005 - mlp_auc_41: 0.9666\n",
      "Epoch 63/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2181 - siamese_loss: 5.0974 - mlp_loss: 0.2181 - mlp_binary_accuracy: 0.9129 - mlp_auc_41: 0.9708\n",
      "Epoch 64/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2242 - siamese_loss: 5.1699 - mlp_loss: 0.2242 - mlp_binary_accuracy: 0.9072 - mlp_auc_41: 0.9693\n",
      "Epoch 65/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2193 - siamese_loss: 5.1237 - mlp_loss: 0.2193 - mlp_binary_accuracy: 0.9038 - mlp_auc_41: 0.9708\n",
      "Epoch 66/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2240 - siamese_loss: 5.0591 - mlp_loss: 0.2240 - mlp_binary_accuracy: 0.9087 - mlp_auc_41: 0.9694\n",
      "Epoch 67/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2078 - siamese_loss: 5.0643 - mlp_loss: 0.2078 - mlp_binary_accuracy: 0.9142 - mlp_auc_41: 0.9735\n",
      "Epoch 68/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2067 - siamese_loss: 4.9590 - mlp_loss: 0.2067 - mlp_binary_accuracy: 0.9191 - mlp_auc_41: 0.9736\n",
      "Epoch 69/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2031 - siamese_loss: 5.1099 - mlp_loss: 0.2031 - mlp_binary_accuracy: 0.9178 - mlp_auc_41: 0.9745\n",
      "Epoch 70/200\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.2113 - siamese_loss: 5.2166 - mlp_loss: 0.2113 - mlp_binary_accuracy: 0.9126 - mlp_auc_41: 0.9727\n",
      "Epoch 71/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2161 - siamese_loss: 5.0986 - mlp_loss: 0.2161 - mlp_binary_accuracy: 0.9137 - mlp_auc_41: 0.9708\n",
      "Epoch 72/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2166 - siamese_loss: 5.0625 - mlp_loss: 0.2166 - mlp_binary_accuracy: 0.9108 - mlp_auc_41: 0.9707\n",
      "Epoch 73/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2048 - siamese_loss: 5.0078 - mlp_loss: 0.2048 - mlp_binary_accuracy: 0.9206 - mlp_auc_41: 0.9743\n",
      "Epoch 74/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1990 - siamese_loss: 4.9718 - mlp_loss: 0.1990 - mlp_binary_accuracy: 0.9178 - mlp_auc_41: 0.9756\n",
      "Epoch 75/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1999 - siamese_loss: 5.0041 - mlp_loss: 0.1999 - mlp_binary_accuracy: 0.9149 - mlp_auc_41: 0.9758\n",
      "Epoch 76/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.2071 - siamese_loss: 5.0603 - mlp_loss: 0.2071 - mlp_binary_accuracy: 0.9152 - mlp_auc_41: 0.9736\n",
      "Epoch 77/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1977 - siamese_loss: 5.0740 - mlp_loss: 0.1977 - mlp_binary_accuracy: 0.9209 - mlp_auc_41: 0.9757\n",
      "Epoch 78/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1918 - siamese_loss: 5.1538 - mlp_loss: 0.1918 - mlp_binary_accuracy: 0.9214 - mlp_auc_41: 0.9772\n",
      "Epoch 79/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1811 - siamese_loss: 5.1643 - mlp_loss: 0.1811 - mlp_binary_accuracy: 0.9263 - mlp_auc_41: 0.9792\n",
      "Epoch 80/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1840 - siamese_loss: 5.0998 - mlp_loss: 0.1840 - mlp_binary_accuracy: 0.9245 - mlp_auc_41: 0.9790\n",
      "Epoch 81/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1734 - siamese_loss: 5.1383 - mlp_loss: 0.1734 - mlp_binary_accuracy: 0.9292 - mlp_auc_41: 0.9814\n",
      "Epoch 82/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1757 - siamese_loss: 5.1919 - mlp_loss: 0.1757 - mlp_binary_accuracy: 0.9276 - mlp_auc_41: 0.9811\n",
      "Epoch 83/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1788 - siamese_loss: 5.2296 - mlp_loss: 0.1788 - mlp_binary_accuracy: 0.9307 - mlp_auc_41: 0.9802\n",
      "Epoch 84/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1728 - siamese_loss: 5.2251 - mlp_loss: 0.1728 - mlp_binary_accuracy: 0.9333 - mlp_auc_41: 0.9812\n",
      "Epoch 85/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1714 - siamese_loss: 5.1878 - mlp_loss: 0.1714 - mlp_binary_accuracy: 0.9268 - mlp_auc_41: 0.9819\n",
      "Epoch 86/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1756 - siamese_loss: 5.1542 - mlp_loss: 0.1756 - mlp_binary_accuracy: 0.9299 - mlp_auc_41: 0.9808\n",
      "Epoch 87/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1676 - siamese_loss: 5.0855 - mlp_loss: 0.1676 - mlp_binary_accuracy: 0.9310 - mlp_auc_41: 0.9822\n",
      "Epoch 88/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1659 - siamese_loss: 5.1020 - mlp_loss: 0.1659 - mlp_binary_accuracy: 0.9338 - mlp_auc_41: 0.9828\n",
      "Epoch 89/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1695 - siamese_loss: 5.1386 - mlp_loss: 0.1695 - mlp_binary_accuracy: 0.9312 - mlp_auc_41: 0.9824\n",
      "Epoch 90/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1669 - siamese_loss: 5.1778 - mlp_loss: 0.1669 - mlp_binary_accuracy: 0.9323 - mlp_auc_41: 0.9827\n",
      "Epoch 91/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1609 - siamese_loss: 5.1808 - mlp_loss: 0.1609 - mlp_binary_accuracy: 0.9369 - mlp_auc_41: 0.9838\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1627 - siamese_loss: 5.1697 - mlp_loss: 0.1627 - mlp_binary_accuracy: 0.9400 - mlp_auc_41: 0.9833\n",
      "Epoch 93/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1534 - siamese_loss: 5.2542 - mlp_loss: 0.1534 - mlp_binary_accuracy: 0.9400 - mlp_auc_41: 0.9854\n",
      "Epoch 94/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1560 - siamese_loss: 5.1993 - mlp_loss: 0.1560 - mlp_binary_accuracy: 0.9377 - mlp_auc_41: 0.9849\n",
      "Epoch 95/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1582 - siamese_loss: 5.2069 - mlp_loss: 0.1582 - mlp_binary_accuracy: 0.9411 - mlp_auc_41: 0.9842\n",
      "Epoch 96/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1489 - siamese_loss: 5.2410 - mlp_loss: 0.1489 - mlp_binary_accuracy: 0.9423 - mlp_auc_41: 0.9862\n",
      "Epoch 97/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1463 - siamese_loss: 5.2543 - mlp_loss: 0.1463 - mlp_binary_accuracy: 0.9395 - mlp_auc_41: 0.9868\n",
      "Epoch 98/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1466 - siamese_loss: 5.2894 - mlp_loss: 0.1466 - mlp_binary_accuracy: 0.9418 - mlp_auc_41: 0.9869\n",
      "Epoch 99/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1467 - siamese_loss: 5.2707 - mlp_loss: 0.1467 - mlp_binary_accuracy: 0.9416 - mlp_auc_41: 0.9867\n",
      "Epoch 100/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1553 - siamese_loss: 5.2389 - mlp_loss: 0.1553 - mlp_binary_accuracy: 0.9387 - mlp_auc_41: 0.9848\n",
      "Epoch 101/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1388 - siamese_loss: 5.1718 - mlp_loss: 0.1388 - mlp_binary_accuracy: 0.9462 - mlp_auc_41: 0.9882\n",
      "Epoch 102/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1398 - siamese_loss: 5.1611 - mlp_loss: 0.1398 - mlp_binary_accuracy: 0.9470 - mlp_auc_41: 0.9874\n",
      "Epoch 103/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1388 - siamese_loss: 5.1603 - mlp_loss: 0.1388 - mlp_binary_accuracy: 0.9460 - mlp_auc_41: 0.9882\n",
      "Epoch 104/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1425 - siamese_loss: 5.1230 - mlp_loss: 0.1425 - mlp_binary_accuracy: 0.9473 - mlp_auc_41: 0.9874\n",
      "Epoch 105/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1328 - siamese_loss: 5.2057 - mlp_loss: 0.1328 - mlp_binary_accuracy: 0.9501 - mlp_auc_41: 0.9888\n",
      "Epoch 106/200\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.1256 - siamese_loss: 5.1941 - mlp_loss: 0.1256 - mlp_binary_accuracy: 0.9511 - mlp_auc_41: 0.9899\n",
      "Epoch 107/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1278 - siamese_loss: 5.2895 - mlp_loss: 0.1278 - mlp_binary_accuracy: 0.9488 - mlp_auc_41: 0.9902\n",
      "Epoch 108/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1298 - siamese_loss: 5.3368 - mlp_loss: 0.1298 - mlp_binary_accuracy: 0.9540 - mlp_auc_41: 0.9889\n",
      "Epoch 109/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1227 - siamese_loss: 5.1893 - mlp_loss: 0.1227 - mlp_binary_accuracy: 0.9560 - mlp_auc_41: 0.9901\n",
      "Epoch 110/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1440 - siamese_loss: 5.2324 - mlp_loss: 0.1440 - mlp_binary_accuracy: 0.9465 - mlp_auc_41: 0.9868\n",
      "Epoch 111/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1273 - siamese_loss: 5.2487 - mlp_loss: 0.1273 - mlp_binary_accuracy: 0.9509 - mlp_auc_41: 0.9894\n",
      "Epoch 112/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1137 - siamese_loss: 5.2355 - mlp_loss: 0.1137 - mlp_binary_accuracy: 0.9584 - mlp_auc_41: 0.9912\n",
      "Epoch 113/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1137 - siamese_loss: 5.1477 - mlp_loss: 0.1137 - mlp_binary_accuracy: 0.9558 - mlp_auc_41: 0.9914\n",
      "Epoch 114/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1203 - siamese_loss: 5.3306 - mlp_loss: 0.1203 - mlp_binary_accuracy: 0.9535 - mlp_auc_41: 0.9907\n",
      "Epoch 115/200\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.1272 - siamese_loss: 5.3508 - mlp_loss: 0.1272 - mlp_binary_accuracy: 0.9517 - mlp_auc_41: 0.9894\n",
      "Epoch 116/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1226 - siamese_loss: 5.2374 - mlp_loss: 0.1226 - mlp_binary_accuracy: 0.9542 - mlp_auc_41: 0.9902\n",
      "Epoch 117/200\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.1230 - siamese_loss: 5.1735 - mlp_loss: 0.1230 - mlp_binary_accuracy: 0.9519 - mlp_auc_41: 0.9899\n",
      "Epoch 118/200\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.1157 - siamese_loss: 5.2761 - mlp_loss: 0.1157 - mlp_binary_accuracy: 0.9568 - mlp_auc_41: 0.9914\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 0.8854 - siamese_loss: 5.0164 - mlp_loss: 0.8854 - mlp_binary_accuracy: 0.7944 - mlp_auc_41: 0.8551\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "测试集上性能------ mlp_binary_accuracy: 79.44%\t ----- mlp_auc_41: 85.51% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvacc,cvauc_mlp,cvpre,cvpre_top,cvrec,cvauc_sia=[],[],[],[],[],[]\n",
    "cvacc_test,cvauc_mlp_test,cvauc_sia_test=[],[],[]\n",
    "\n",
    "###############################################################################################\n",
    "\"\"\"Randomly divide the 20% test set\"\"\"\n",
    "input_shape=(322)\n",
    "base_network = create_base_network(input_shape)\n",
    "\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)\n",
    "input_mesh_a = Input(shape=24)\n",
    "input_mesh_b = Input(shape=24)\n",
    "\n",
    "\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "\n",
    "eucl_model = create_euclLayer(processed_a.shape[-1])\n",
    "eucl_out = eucl_model([processed_a,processed_b]) \n",
    "\n",
    "\n",
    "concat_fea = tf.keras.layers.concatenate([processed_a,input_mesh_a,processed_b,input_mesh_b],axis=-1)\n",
    "print(\"concat_fea的shape：\",concat_fea.shape) #(None,112)\n",
    "MLP = create_MLP(concat_fea.shape[-1])\n",
    "MLP_out = MLP(concat_fea)\n",
    "\n",
    "model = Model(inputs=[input_a, input_b,input_mesh_a,input_mesh_b], outputs=[eucl_out,MLP_out])\n",
    "\n",
    "\"\"\"L2\"\"\"\n",
    "for layer in model.layers:\n",
    "    if hasattr(layer,'kernel_regularizer'):\n",
    "        layer.kernel_regularizer = tf.keras.regularizers.l2(0.001)\n",
    "    \n",
    "#####################################################################################################################\n",
    "\n",
    "print(\"-------------------------------------Train start-----------------------------------------\")\n",
    "\n",
    "rms = RMSprop()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=5)\n",
    "\n",
    "my_loss = {\"siamese\":contrastive_loss,\"mlp\":tf.keras.losses.BinaryCrossentropy()}\n",
    "my_loss_weight = {\"siamese\":0,\"mlp\":1}\n",
    "my_metrics ={ \"mlp\":[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()]} #tf.keras.metrics.Precision(top_k=len(rus_y)//100)\n",
    "\n",
    "model.compile(\n",
    "    loss= my_loss,\n",
    "    loss_weights=my_loss_weight,\n",
    "    metrics=my_metrics,\n",
    "    \n",
    "    optimizer=rms,\n",
    ")\n",
    "\n",
    "#symA_train,symA_test,symB_train,symB_test,meshA_train,meshA_test,meshB_train,meshB_test,y_train,y_test\n",
    "\n",
    "# fit()中 shuffle=True\n",
    "model.fit([symA_train,symB_train,meshA_train,meshB_train],\n",
    "          [y_train,y_train],\n",
    "          \n",
    "          batch_size=None,#默认32\n",
    "          epochs=200,  #200-->50-->100\n",
    "          callbacks=callback,\n",
    "          shuffle=True,\n",
    "         )\n",
    "\n",
    "test_scores = model.evaluate([symA_test,symB_test,meshA_test,meshB_test],\n",
    "                        [y_test,y_test],\n",
    "                        verbose=1,\n",
    "                        batch_size=32)\n",
    "#predictions[0]-siamese's outputs,predictions[1]-mlp's outputs\n",
    "predictions = model.predict([symA_test,symB_test,meshA_test,meshB_test],\n",
    "                        verbose=1)\n",
    "\n",
    "cvacc_test.append(test_scores[-2] * 100)\n",
    "cvauc_mlp_test.append(test_scores[-1] * 100)\n",
    "cvauc_sia_test.append(test_scores[-3] * 100)\n",
    "\n",
    "\n",
    "print(\"测试集上性能------ %s: %.2f%%\\t ----- %s: %.2f%% \\n\" % \n",
    "           (model.metrics_names[-2],test_scores[-2]*100, model.metrics_names[-1],test_scores[-1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集上性能------ mlp_binary_accuracy: 79.44%\t ----- mlp_auc_41: 85.51%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "968 2 968\n",
      "[[5.28936684e-02]\n",
      " [1.10655814e-01]\n",
      " [8.14563930e-02]\n",
      " [3.45423818e-03]\n",
      " [1.91136003e-02]\n",
      " [2.42131948e-03]\n",
      " [4.77382541e-03]\n",
      " [3.56525183e-04]\n",
      " [9.95916009e-01]\n",
      " [1.88028812e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(len(y_test),len(predictions),len(predictions[1]))\n",
    "print(predictions[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 968)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label   predict\n",
       "0    0.0  0.052894\n",
       "1    0.0  0.110656\n",
       "2    0.0  0.081456\n",
       "3    1.0  0.003454\n",
       "4    0.0  0.019114"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC_data = np.array([list(y_test),[i[0] for i in predictions[1]]])\n",
    "print(AUC_data.shape)\n",
    "AUC_data_df=pd.DataFrame(AUC_data.T,columns=[\"label\",\"predict\"])\n",
    "AUC_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC_data_df.to_csv(\"auc_data_SDSP.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat_fea的shape： (None, 112)\n",
      "-------------------------------------Kfold: 1 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.6703 - siamese_loss: 3.2520 - mlp_loss: 0.6703 - mlp_binary_accuracy: 0.5834 - mlp_auc_59: 0.6245\n",
      "Epoch 2/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.6315 - siamese_loss: 3.8003 - mlp_loss: 0.6315 - mlp_binary_accuracy: 0.6357 - mlp_auc_59: 0.6940\n",
      "Epoch 3/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5928 - siamese_loss: 3.9250 - mlp_loss: 0.5928 - mlp_binary_accuracy: 0.6800 - mlp_auc_59: 0.7457\n",
      "Epoch 4/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.5788 - siamese_loss: 4.0741 - mlp_loss: 0.5788 - mlp_binary_accuracy: 0.6975 - mlp_auc_59: 0.7622\n",
      "Epoch 5/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5621 - siamese_loss: 4.2057 - mlp_loss: 0.5621 - mlp_binary_accuracy: 0.7072 - mlp_auc_59: 0.7811\n",
      "Epoch 6/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.5490 - siamese_loss: 4.3733 - mlp_loss: 0.5490 - mlp_binary_accuracy: 0.7114 - mlp_auc_59: 0.7904\n",
      "Epoch 7/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.5363 - siamese_loss: 4.2996 - mlp_loss: 0.5363 - mlp_binary_accuracy: 0.7269 - mlp_auc_59: 0.8039\n",
      "Epoch 8/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.5193 - siamese_loss: 4.3064 - mlp_loss: 0.5193 - mlp_binary_accuracy: 0.7346 - mlp_auc_59: 0.8188\n",
      "Epoch 9/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5112 - siamese_loss: 4.4332 - mlp_loss: 0.5112 - mlp_binary_accuracy: 0.7418 - mlp_auc_59: 0.8246\n",
      "Epoch 10/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.5054 - siamese_loss: 4.6408 - mlp_loss: 0.5054 - mlp_binary_accuracy: 0.7372 - mlp_auc_59: 0.8286\n",
      "Epoch 11/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4926 - siamese_loss: 4.7135 - mlp_loss: 0.4926 - mlp_binary_accuracy: 0.7547 - mlp_auc_59: 0.8393\n",
      "Epoch 12/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4838 - siamese_loss: 4.6012 - mlp_loss: 0.4838 - mlp_binary_accuracy: 0.7666 - mlp_auc_59: 0.8477\n",
      "Epoch 13/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4809 - siamese_loss: 4.5690 - mlp_loss: 0.4809 - mlp_binary_accuracy: 0.7676 - mlp_auc_59: 0.8492\n",
      "Epoch 14/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4796 - siamese_loss: 4.6762 - mlp_loss: 0.4796 - mlp_binary_accuracy: 0.7650 - mlp_auc_59: 0.8504\n",
      "Epoch 15/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4697 - siamese_loss: 4.8470 - mlp_loss: 0.4697 - mlp_binary_accuracy: 0.7728 - mlp_auc_59: 0.8576\n",
      "Epoch 16/200\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.4624 - siamese_loss: 4.8793 - mlp_loss: 0.4624 - mlp_binary_accuracy: 0.7822 - mlp_auc_59: 0.8639\n",
      "Epoch 17/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4447 - siamese_loss: 4.8049 - mlp_loss: 0.4447 - mlp_binary_accuracy: 0.7880 - mlp_auc_59: 0.8733\n",
      "Epoch 18/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4348 - siamese_loss: 4.8757 - mlp_loss: 0.4348 - mlp_binary_accuracy: 0.7970 - mlp_auc_59: 0.8800\n",
      "Epoch 19/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4202 - siamese_loss: 4.8370 - mlp_loss: 0.4202 - mlp_binary_accuracy: 0.8032 - mlp_auc_59: 0.8891\n",
      "Epoch 20/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4267 - siamese_loss: 4.7810 - mlp_loss: 0.4267 - mlp_binary_accuracy: 0.8012 - mlp_auc_59: 0.8852\n",
      "Epoch 21/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4218 - siamese_loss: 4.7634 - mlp_loss: 0.4218 - mlp_binary_accuracy: 0.8067 - mlp_auc_59: 0.8886\n",
      "Epoch 22/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4088 - siamese_loss: 4.9534 - mlp_loss: 0.4088 - mlp_binary_accuracy: 0.8135 - mlp_auc_59: 0.8950\n",
      "Epoch 23/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4046 - siamese_loss: 4.9865 - mlp_loss: 0.4046 - mlp_binary_accuracy: 0.8180 - mlp_auc_59: 0.8971\n",
      "Epoch 24/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4004 - siamese_loss: 5.0201 - mlp_loss: 0.4004 - mlp_binary_accuracy: 0.8180 - mlp_auc_59: 0.9000\n",
      "Epoch 25/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3923 - siamese_loss: 5.0451 - mlp_loss: 0.3923 - mlp_binary_accuracy: 0.8229 - mlp_auc_59: 0.9041\n",
      "Epoch 26/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3838 - siamese_loss: 4.9914 - mlp_loss: 0.3838 - mlp_binary_accuracy: 0.8235 - mlp_auc_59: 0.9083\n",
      "Epoch 27/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3765 - siamese_loss: 5.1212 - mlp_loss: 0.3765 - mlp_binary_accuracy: 0.8261 - mlp_auc_59: 0.9115\n",
      "Epoch 28/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3720 - siamese_loss: 5.1475 - mlp_loss: 0.3720 - mlp_binary_accuracy: 0.8413 - mlp_auc_59: 0.9149\n",
      "Epoch 29/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3688 - siamese_loss: 5.2368 - mlp_loss: 0.3688 - mlp_binary_accuracy: 0.8329 - mlp_auc_59: 0.9150\n",
      "Epoch 30/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3577 - siamese_loss: 5.1883 - mlp_loss: 0.3577 - mlp_binary_accuracy: 0.8413 - mlp_auc_59: 0.9206\n",
      "Epoch 31/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3540 - siamese_loss: 5.2982 - mlp_loss: 0.3540 - mlp_binary_accuracy: 0.8410 - mlp_auc_59: 0.9220\n",
      "Epoch 32/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3470 - siamese_loss: 5.3121 - mlp_loss: 0.3470 - mlp_binary_accuracy: 0.8478 - mlp_auc_59: 0.9254\n",
      "Epoch 33/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3489 - siamese_loss: 5.3308 - mlp_loss: 0.3489 - mlp_binary_accuracy: 0.8455 - mlp_auc_59: 0.9247\n",
      "Epoch 34/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3315 - siamese_loss: 5.3073 - mlp_loss: 0.3315 - mlp_binary_accuracy: 0.8617 - mlp_auc_59: 0.9322\n",
      "Epoch 35/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3280 - siamese_loss: 5.2559 - mlp_loss: 0.3280 - mlp_binary_accuracy: 0.8575 - mlp_auc_59: 0.9337\n",
      "Epoch 36/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3235 - siamese_loss: 5.1052 - mlp_loss: 0.3235 - mlp_binary_accuracy: 0.8555 - mlp_auc_59: 0.9352\n",
      "Epoch 37/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3117 - siamese_loss: 5.1780 - mlp_loss: 0.3117 - mlp_binary_accuracy: 0.8665 - mlp_auc_59: 0.9402\n",
      "Epoch 38/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2977 - siamese_loss: 5.2425 - mlp_loss: 0.2977 - mlp_binary_accuracy: 0.8752 - mlp_auc_59: 0.9455\n",
      "Epoch 39/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3009 - siamese_loss: 5.3214 - mlp_loss: 0.3009 - mlp_binary_accuracy: 0.8727 - mlp_auc_59: 0.9446\n",
      "Epoch 40/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2940 - siamese_loss: 5.2236 - mlp_loss: 0.2940 - mlp_binary_accuracy: 0.8765 - mlp_auc_59: 0.9467\n",
      "Epoch 41/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2979 - siamese_loss: 5.1319 - mlp_loss: 0.2979 - mlp_binary_accuracy: 0.8707 - mlp_auc_59: 0.9456\n",
      "Epoch 42/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2960 - siamese_loss: 5.0011 - mlp_loss: 0.2960 - mlp_binary_accuracy: 0.8736 - mlp_auc_59: 0.9463\n",
      "Epoch 43/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3042 - siamese_loss: 5.1105 - mlp_loss: 0.3042 - mlp_binary_accuracy: 0.8736 - mlp_auc_59: 0.9432\n",
      "Epoch 44/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2790 - siamese_loss: 5.3337 - mlp_loss: 0.2790 - mlp_binary_accuracy: 0.8830 - mlp_auc_59: 0.9521\n",
      "Epoch 45/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2812 - siamese_loss: 5.2384 - mlp_loss: 0.2812 - mlp_binary_accuracy: 0.8836 - mlp_auc_59: 0.9519\n",
      "Epoch 46/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2737 - siamese_loss: 5.3923 - mlp_loss: 0.2737 - mlp_binary_accuracy: 0.8869 - mlp_auc_59: 0.9541\n",
      "Epoch 47/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2815 - siamese_loss: 5.2541 - mlp_loss: 0.2815 - mlp_binary_accuracy: 0.8869 - mlp_auc_59: 0.9516\n",
      "Epoch 48/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2708 - siamese_loss: 5.2746 - mlp_loss: 0.2708 - mlp_binary_accuracy: 0.8872 - mlp_auc_59: 0.9554\n",
      "Epoch 49/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2852 - siamese_loss: 5.4114 - mlp_loss: 0.2852 - mlp_binary_accuracy: 0.8807 - mlp_auc_59: 0.9503\n",
      "Epoch 50/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2607 - siamese_loss: 5.3876 - mlp_loss: 0.2607 - mlp_binary_accuracy: 0.8930 - mlp_auc_59: 0.9581\n",
      "Epoch 51/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2611 - siamese_loss: 5.2477 - mlp_loss: 0.2611 - mlp_binary_accuracy: 0.8914 - mlp_auc_59: 0.9582\n",
      "Epoch 52/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2494 - siamese_loss: 5.3691 - mlp_loss: 0.2494 - mlp_binary_accuracy: 0.8959 - mlp_auc_59: 0.9619\n",
      "Epoch 53/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2598 - siamese_loss: 5.4745 - mlp_loss: 0.2598 - mlp_binary_accuracy: 0.8911 - mlp_auc_59: 0.9583\n",
      "Epoch 54/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2472 - siamese_loss: 5.4318 - mlp_loss: 0.2472 - mlp_binary_accuracy: 0.8940 - mlp_auc_59: 0.9626\n",
      "Epoch 55/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2275 - siamese_loss: 5.4437 - mlp_loss: 0.2275 - mlp_binary_accuracy: 0.9053 - mlp_auc_59: 0.9682\n",
      "Epoch 56/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2346 - siamese_loss: 5.3989 - mlp_loss: 0.2346 - mlp_binary_accuracy: 0.9059 - mlp_auc_59: 0.9664\n",
      "Epoch 57/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2263 - siamese_loss: 5.2151 - mlp_loss: 0.2263 - mlp_binary_accuracy: 0.9082 - mlp_auc_59: 0.9682\n",
      "Epoch 58/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2371 - siamese_loss: 5.2634 - mlp_loss: 0.2371 - mlp_binary_accuracy: 0.9014 - mlp_auc_59: 0.9653\n",
      "Epoch 59/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2280 - siamese_loss: 5.3845 - mlp_loss: 0.2280 - mlp_binary_accuracy: 0.9053 - mlp_auc_59: 0.9679\n",
      "Epoch 60/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2271 - siamese_loss: 5.3985 - mlp_loss: 0.2271 - mlp_binary_accuracy: 0.9105 - mlp_auc_59: 0.9679\n",
      "Epoch 61/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2312 - siamese_loss: 5.3332 - mlp_loss: 0.2312 - mlp_binary_accuracy: 0.9024 - mlp_auc_59: 0.9671\n",
      "Epoch 62/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2150 - siamese_loss: 5.2949 - mlp_loss: 0.2150 - mlp_binary_accuracy: 0.9156 - mlp_auc_59: 0.9713\n",
      "Epoch 63/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2075 - siamese_loss: 5.3268 - mlp_loss: 0.2075 - mlp_binary_accuracy: 0.9205 - mlp_auc_59: 0.9728\n",
      "Epoch 64/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2038 - siamese_loss: 5.4382 - mlp_loss: 0.2038 - mlp_binary_accuracy: 0.9153 - mlp_auc_59: 0.9746\n",
      "Epoch 65/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1969 - siamese_loss: 5.5211 - mlp_loss: 0.1969 - mlp_binary_accuracy: 0.9176 - mlp_auc_59: 0.9761\n",
      "Epoch 66/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1978 - siamese_loss: 5.5791 - mlp_loss: 0.1978 - mlp_binary_accuracy: 0.9173 - mlp_auc_59: 0.9758\n",
      "Epoch 67/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1917 - siamese_loss: 5.5294 - mlp_loss: 0.1917 - mlp_binary_accuracy: 0.9244 - mlp_auc_59: 0.9768\n",
      "Epoch 68/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1906 - siamese_loss: 5.4663 - mlp_loss: 0.1906 - mlp_binary_accuracy: 0.9247 - mlp_auc_59: 0.9767\n",
      "Epoch 69/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1967 - siamese_loss: 5.4187 - mlp_loss: 0.1967 - mlp_binary_accuracy: 0.9215 - mlp_auc_59: 0.9761\n",
      "Epoch 70/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1962 - siamese_loss: 5.5111 - mlp_loss: 0.1962 - mlp_binary_accuracy: 0.9202 - mlp_auc_59: 0.9756\n",
      "Epoch 71/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1868 - siamese_loss: 5.5421 - mlp_loss: 0.1868 - mlp_binary_accuracy: 0.9221 - mlp_auc_59: 0.9786\n",
      "Epoch 72/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1800 - siamese_loss: 5.5454 - mlp_loss: 0.1800 - mlp_binary_accuracy: 0.9289 - mlp_auc_59: 0.9801\n",
      "Epoch 73/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1788 - siamese_loss: 5.5327 - mlp_loss: 0.1788 - mlp_binary_accuracy: 0.9341 - mlp_auc_59: 0.9791\n",
      "Epoch 74/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1756 - siamese_loss: 5.5005 - mlp_loss: 0.1756 - mlp_binary_accuracy: 0.9292 - mlp_auc_59: 0.9804\n",
      "Epoch 75/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1714 - siamese_loss: 5.3827 - mlp_loss: 0.1714 - mlp_binary_accuracy: 0.9315 - mlp_auc_59: 0.9815\n",
      "Epoch 76/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1671 - siamese_loss: 5.4986 - mlp_loss: 0.1671 - mlp_binary_accuracy: 0.9350 - mlp_auc_59: 0.9818\n",
      "Epoch 77/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1672 - siamese_loss: 5.3617 - mlp_loss: 0.1672 - mlp_binary_accuracy: 0.9312 - mlp_auc_59: 0.9827\n",
      "Epoch 78/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1759 - siamese_loss: 5.3881 - mlp_loss: 0.1759 - mlp_binary_accuracy: 0.9344 - mlp_auc_59: 0.9801\n",
      "Epoch 79/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1588 - siamese_loss: 5.4447 - mlp_loss: 0.1588 - mlp_binary_accuracy: 0.9386 - mlp_auc_59: 0.9843\n",
      "Epoch 80/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1571 - siamese_loss: 5.4696 - mlp_loss: 0.1571 - mlp_binary_accuracy: 0.9412 - mlp_auc_59: 0.9843\n",
      "Epoch 81/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1593 - siamese_loss: 5.4825 - mlp_loss: 0.1593 - mlp_binary_accuracy: 0.9415 - mlp_auc_59: 0.9834\n",
      "Epoch 82/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1592 - siamese_loss: 5.4888 - mlp_loss: 0.1592 - mlp_binary_accuracy: 0.9396 - mlp_auc_59: 0.9839\n",
      "Epoch 83/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1592 - siamese_loss: 5.5615 - mlp_loss: 0.1592 - mlp_binary_accuracy: 0.9337 - mlp_auc_59: 0.9845\n",
      "Epoch 84/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1462 - siamese_loss: 5.5049 - mlp_loss: 0.1462 - mlp_binary_accuracy: 0.9396 - mlp_auc_59: 0.9864\n",
      "Epoch 85/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1574 - siamese_loss: 5.5732 - mlp_loss: 0.1574 - mlp_binary_accuracy: 0.9357 - mlp_auc_59: 0.9844\n",
      "Epoch 86/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1565 - siamese_loss: 5.6381 - mlp_loss: 0.1565 - mlp_binary_accuracy: 0.9451 - mlp_auc_59: 0.9842\n",
      "Epoch 87/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1480 - siamese_loss: 5.6336 - mlp_loss: 0.1480 - mlp_binary_accuracy: 0.9451 - mlp_auc_59: 0.9852\n",
      "Epoch 88/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1532 - siamese_loss: 5.5337 - mlp_loss: 0.1532 - mlp_binary_accuracy: 0.9444 - mlp_auc_59: 0.9843\n",
      "Epoch 89/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1599 - siamese_loss: 5.6820 - mlp_loss: 0.1599 - mlp_binary_accuracy: 0.9409 - mlp_auc_59: 0.9833\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7250 - siamese_loss: 5.5885 - mlp_loss: 0.7250 - mlp_binary_accuracy: 0.7946 - mlp_auc_59: 0.8594\n",
      "------ mlp_binary_accuracy: 79.46%\t ----- mlp_auc_59: 85.94%\n",
      "concat_fea的shape： (None, 112)\n",
      "-------------------------------------Kfold: 2 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.6785 - siamese_loss: 3.0826 - mlp_loss: 0.6785 - mlp_binary_accuracy: 0.5879 - mlp_auc_60: 0.6233\n",
      "Epoch 2/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.6304 - siamese_loss: 3.2274 - mlp_loss: 0.6304 - mlp_binary_accuracy: 0.6648 - mlp_auc_60: 0.7067\n",
      "Epoch 3/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5940 - siamese_loss: 3.2158 - mlp_loss: 0.5940 - mlp_binary_accuracy: 0.6803 - mlp_auc_60: 0.7490\n",
      "Epoch 4/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5762 - siamese_loss: 3.4715 - mlp_loss: 0.5762 - mlp_binary_accuracy: 0.7075 - mlp_auc_60: 0.7677\n",
      "Epoch 5/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5595 - siamese_loss: 3.6831 - mlp_loss: 0.5595 - mlp_binary_accuracy: 0.7136 - mlp_auc_60: 0.7841\n",
      "Epoch 6/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5520 - siamese_loss: 3.7292 - mlp_loss: 0.5520 - mlp_binary_accuracy: 0.7220 - mlp_auc_60: 0.7921\n",
      "Epoch 7/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5313 - siamese_loss: 3.8881 - mlp_loss: 0.5313 - mlp_binary_accuracy: 0.7304 - mlp_auc_60: 0.8107\n",
      "Epoch 8/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5245 - siamese_loss: 3.8493 - mlp_loss: 0.5245 - mlp_binary_accuracy: 0.7424 - mlp_auc_60: 0.8167\n",
      "Epoch 9/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5138 - siamese_loss: 3.9230 - mlp_loss: 0.5138 - mlp_binary_accuracy: 0.7489 - mlp_auc_60: 0.8250\n",
      "Epoch 10/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5156 - siamese_loss: 3.9223 - mlp_loss: 0.5156 - mlp_binary_accuracy: 0.7476 - mlp_auc_60: 0.8241\n",
      "Epoch 11/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5100 - siamese_loss: 3.8715 - mlp_loss: 0.5100 - mlp_binary_accuracy: 0.7544 - mlp_auc_60: 0.8287\n",
      "Epoch 12/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4945 - siamese_loss: 4.0188 - mlp_loss: 0.4945 - mlp_binary_accuracy: 0.7612 - mlp_auc_60: 0.8404\n",
      "Epoch 13/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4909 - siamese_loss: 4.1257 - mlp_loss: 0.4909 - mlp_binary_accuracy: 0.7631 - mlp_auc_60: 0.8432\n",
      "Epoch 14/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4858 - siamese_loss: 4.0294 - mlp_loss: 0.4858 - mlp_binary_accuracy: 0.7754 - mlp_auc_60: 0.8467\n",
      "Epoch 15/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4789 - siamese_loss: 4.0919 - mlp_loss: 0.4789 - mlp_binary_accuracy: 0.7731 - mlp_auc_60: 0.8519\n",
      "Epoch 16/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4721 - siamese_loss: 4.3273 - mlp_loss: 0.4721 - mlp_binary_accuracy: 0.7776 - mlp_auc_60: 0.8565\n",
      "Epoch 17/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4680 - siamese_loss: 4.3925 - mlp_loss: 0.4680 - mlp_binary_accuracy: 0.7802 - mlp_auc_60: 0.8595\n",
      "Epoch 18/200\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.4608 - siamese_loss: 4.3736 - mlp_loss: 0.4608 - mlp_binary_accuracy: 0.7912 - mlp_auc_60: 0.8643\n",
      "Epoch 19/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4516 - siamese_loss: 4.5121 - mlp_loss: 0.4516 - mlp_binary_accuracy: 0.7922 - mlp_auc_60: 0.8699\n",
      "Epoch 20/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4425 - siamese_loss: 4.5992 - mlp_loss: 0.4425 - mlp_binary_accuracy: 0.7964 - mlp_auc_60: 0.8757\n",
      "Epoch 21/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4441 - siamese_loss: 4.6507 - mlp_loss: 0.4441 - mlp_binary_accuracy: 0.7961 - mlp_auc_60: 0.8750\n",
      "Epoch 22/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4372 - siamese_loss: 4.6510 - mlp_loss: 0.4372 - mlp_binary_accuracy: 0.8061 - mlp_auc_60: 0.8791\n",
      "Epoch 23/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4332 - siamese_loss: 4.5922 - mlp_loss: 0.4332 - mlp_binary_accuracy: 0.8041 - mlp_auc_60: 0.8811\n",
      "Epoch 24/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4206 - siamese_loss: 4.6128 - mlp_loss: 0.4206 - mlp_binary_accuracy: 0.8116 - mlp_auc_60: 0.8886\n",
      "Epoch 25/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4100 - siamese_loss: 4.6795 - mlp_loss: 0.4100 - mlp_binary_accuracy: 0.8129 - mlp_auc_60: 0.8953\n",
      "Epoch 26/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4172 - siamese_loss: 4.6476 - mlp_loss: 0.4172 - mlp_binary_accuracy: 0.8171 - mlp_auc_60: 0.8907\n",
      "Epoch 27/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4121 - siamese_loss: 4.4794 - mlp_loss: 0.4121 - mlp_binary_accuracy: 0.8206 - mlp_auc_60: 0.8932\n",
      "Epoch 28/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4003 - siamese_loss: 4.5800 - mlp_loss: 0.4003 - mlp_binary_accuracy: 0.8226 - mlp_auc_60: 0.8996\n",
      "Epoch 29/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3904 - siamese_loss: 4.5866 - mlp_loss: 0.3904 - mlp_binary_accuracy: 0.8316 - mlp_auc_60: 0.9047\n",
      "Epoch 30/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3858 - siamese_loss: 4.6148 - mlp_loss: 0.3858 - mlp_binary_accuracy: 0.8323 - mlp_auc_60: 0.9067\n",
      "Epoch 31/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3912 - siamese_loss: 4.6876 - mlp_loss: 0.3912 - mlp_binary_accuracy: 0.8300 - mlp_auc_60: 0.9043\n",
      "Epoch 32/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3774 - siamese_loss: 4.7037 - mlp_loss: 0.3774 - mlp_binary_accuracy: 0.8368 - mlp_auc_60: 0.9112\n",
      "Epoch 33/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3745 - siamese_loss: 4.6745 - mlp_loss: 0.3745 - mlp_binary_accuracy: 0.8371 - mlp_auc_60: 0.9125\n",
      "Epoch 34/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3666 - siamese_loss: 4.7029 - mlp_loss: 0.3666 - mlp_binary_accuracy: 0.8478 - mlp_auc_60: 0.9163\n",
      "Epoch 35/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3608 - siamese_loss: 4.6909 - mlp_loss: 0.3608 - mlp_binary_accuracy: 0.8432 - mlp_auc_60: 0.9185\n",
      "Epoch 36/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3611 - siamese_loss: 4.6106 - mlp_loss: 0.3611 - mlp_binary_accuracy: 0.8439 - mlp_auc_60: 0.9184\n",
      "Epoch 37/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3463 - siamese_loss: 4.6201 - mlp_loss: 0.3463 - mlp_binary_accuracy: 0.8594 - mlp_auc_60: 0.9254\n",
      "Epoch 38/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3518 - siamese_loss: 4.7254 - mlp_loss: 0.3518 - mlp_binary_accuracy: 0.8546 - mlp_auc_60: 0.9217\n",
      "Epoch 39/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3510 - siamese_loss: 4.9296 - mlp_loss: 0.3510 - mlp_binary_accuracy: 0.8500 - mlp_auc_60: 0.9234\n",
      "Epoch 40/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3425 - siamese_loss: 4.8874 - mlp_loss: 0.3425 - mlp_binary_accuracy: 0.8539 - mlp_auc_60: 0.9266\n",
      "Epoch 41/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3527 - siamese_loss: 4.7660 - mlp_loss: 0.3527 - mlp_binary_accuracy: 0.8497 - mlp_auc_60: 0.9223\n",
      "Epoch 42/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3301 - siamese_loss: 4.7398 - mlp_loss: 0.3301 - mlp_binary_accuracy: 0.8617 - mlp_auc_60: 0.9323\n",
      "Epoch 43/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3241 - siamese_loss: 4.8624 - mlp_loss: 0.3241 - mlp_binary_accuracy: 0.8604 - mlp_auc_60: 0.9349\n",
      "Epoch 44/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3307 - siamese_loss: 4.8154 - mlp_loss: 0.3307 - mlp_binary_accuracy: 0.8588 - mlp_auc_60: 0.9319\n",
      "Epoch 45/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3127 - siamese_loss: 4.9572 - mlp_loss: 0.3127 - mlp_binary_accuracy: 0.8697 - mlp_auc_60: 0.9388\n",
      "Epoch 46/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3276 - siamese_loss: 4.9778 - mlp_loss: 0.3276 - mlp_binary_accuracy: 0.8617 - mlp_auc_60: 0.9332\n",
      "Epoch 47/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3309 - siamese_loss: 4.9796 - mlp_loss: 0.3309 - mlp_binary_accuracy: 0.8604 - mlp_auc_60: 0.9322\n",
      "Epoch 48/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3164 - siamese_loss: 5.1121 - mlp_loss: 0.3164 - mlp_binary_accuracy: 0.8659 - mlp_auc_60: 0.9377\n",
      "Epoch 49/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3121 - siamese_loss: 5.1004 - mlp_loss: 0.3121 - mlp_binary_accuracy: 0.8668 - mlp_auc_60: 0.9393\n",
      "Epoch 50/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3121 - siamese_loss: 5.0484 - mlp_loss: 0.3121 - mlp_binary_accuracy: 0.8672 - mlp_auc_60: 0.9392\n",
      "Epoch 51/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3116 - siamese_loss: 5.1045 - mlp_loss: 0.3116 - mlp_binary_accuracy: 0.8707 - mlp_auc_60: 0.9398\n",
      "Epoch 52/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3018 - siamese_loss: 5.2233 - mlp_loss: 0.3018 - mlp_binary_accuracy: 0.8749 - mlp_auc_60: 0.9438\n",
      "Epoch 53/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2962 - siamese_loss: 5.1731 - mlp_loss: 0.2962 - mlp_binary_accuracy: 0.8717 - mlp_auc_60: 0.9450\n",
      "Epoch 54/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2898 - siamese_loss: 5.2144 - mlp_loss: 0.2898 - mlp_binary_accuracy: 0.8820 - mlp_auc_60: 0.9477\n",
      "Epoch 55/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2946 - siamese_loss: 5.1136 - mlp_loss: 0.2946 - mlp_binary_accuracy: 0.8811 - mlp_auc_60: 0.9456\n",
      "Epoch 56/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2884 - siamese_loss: 5.1425 - mlp_loss: 0.2884 - mlp_binary_accuracy: 0.8846 - mlp_auc_60: 0.9485\n",
      "Epoch 57/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2859 - siamese_loss: 5.1918 - mlp_loss: 0.2859 - mlp_binary_accuracy: 0.8814 - mlp_auc_60: 0.9495\n",
      "Epoch 58/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2826 - siamese_loss: 5.2223 - mlp_loss: 0.2826 - mlp_binary_accuracy: 0.8882 - mlp_auc_60: 0.9497\n",
      "Epoch 59/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2818 - siamese_loss: 5.2530 - mlp_loss: 0.2818 - mlp_binary_accuracy: 0.8817 - mlp_auc_60: 0.9503\n",
      "Epoch 60/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2819 - siamese_loss: 5.3090 - mlp_loss: 0.2819 - mlp_binary_accuracy: 0.8840 - mlp_auc_60: 0.9506\n",
      "Epoch 61/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2672 - siamese_loss: 5.3723 - mlp_loss: 0.2672 - mlp_binary_accuracy: 0.8888 - mlp_auc_60: 0.9549\n",
      "Epoch 62/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2807 - siamese_loss: 5.3328 - mlp_loss: 0.2807 - mlp_binary_accuracy: 0.8827 - mlp_auc_60: 0.9512\n",
      "Epoch 63/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2675 - siamese_loss: 5.3402 - mlp_loss: 0.2675 - mlp_binary_accuracy: 0.9011 - mlp_auc_60: 0.9549\n",
      "Epoch 64/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2662 - siamese_loss: 5.2650 - mlp_loss: 0.2662 - mlp_binary_accuracy: 0.8943 - mlp_auc_60: 0.9558\n",
      "Epoch 65/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2612 - siamese_loss: 5.2863 - mlp_loss: 0.2612 - mlp_binary_accuracy: 0.8888 - mlp_auc_60: 0.9578\n",
      "Epoch 66/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2657 - siamese_loss: 5.3972 - mlp_loss: 0.2657 - mlp_binary_accuracy: 0.8946 - mlp_auc_60: 0.9561\n",
      "Epoch 67/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2608 - siamese_loss: 5.4294 - mlp_loss: 0.2608 - mlp_binary_accuracy: 0.8911 - mlp_auc_60: 0.9574\n",
      "Epoch 68/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2649 - siamese_loss: 5.3130 - mlp_loss: 0.2649 - mlp_binary_accuracy: 0.8891 - mlp_auc_60: 0.9563\n",
      "Epoch 69/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2530 - siamese_loss: 5.2461 - mlp_loss: 0.2530 - mlp_binary_accuracy: 0.8972 - mlp_auc_60: 0.9594\n",
      "Epoch 70/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2506 - siamese_loss: 5.3219 - mlp_loss: 0.2506 - mlp_binary_accuracy: 0.8940 - mlp_auc_60: 0.9608\n",
      "Epoch 71/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2431 - siamese_loss: 5.4750 - mlp_loss: 0.2431 - mlp_binary_accuracy: 0.9066 - mlp_auc_60: 0.9632\n",
      "Epoch 72/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2547 - siamese_loss: 5.3942 - mlp_loss: 0.2547 - mlp_binary_accuracy: 0.8979 - mlp_auc_60: 0.9599\n",
      "Epoch 73/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2617 - siamese_loss: 5.3253 - mlp_loss: 0.2617 - mlp_binary_accuracy: 0.8901 - mlp_auc_60: 0.9581\n",
      "Epoch 74/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2503 - siamese_loss: 5.3598 - mlp_loss: 0.2503 - mlp_binary_accuracy: 0.8979 - mlp_auc_60: 0.9615\n",
      "Epoch 75/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2408 - siamese_loss: 5.4242 - mlp_loss: 0.2408 - mlp_binary_accuracy: 0.9050 - mlp_auc_60: 0.9632\n",
      "Epoch 76/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2233 - siamese_loss: 5.5916 - mlp_loss: 0.2233 - mlp_binary_accuracy: 0.9131 - mlp_auc_60: 0.9681\n",
      "Epoch 77/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2295 - siamese_loss: 5.7058 - mlp_loss: 0.2295 - mlp_binary_accuracy: 0.9079 - mlp_auc_60: 0.9668\n",
      "Epoch 78/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2321 - siamese_loss: 5.7634 - mlp_loss: 0.2321 - mlp_binary_accuracy: 0.9072 - mlp_auc_60: 0.9663\n",
      "Epoch 79/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2244 - siamese_loss: 5.7372 - mlp_loss: 0.2244 - mlp_binary_accuracy: 0.9050 - mlp_auc_60: 0.9683\n",
      "Epoch 80/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2318 - siamese_loss: 5.6600 - mlp_loss: 0.2318 - mlp_binary_accuracy: 0.9085 - mlp_auc_60: 0.9669\n",
      "Epoch 81/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2228 - siamese_loss: 5.5509 - mlp_loss: 0.2228 - mlp_binary_accuracy: 0.9134 - mlp_auc_60: 0.9686\n",
      "Epoch 82/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2185 - siamese_loss: 5.5624 - mlp_loss: 0.2185 - mlp_binary_accuracy: 0.9150 - mlp_auc_60: 0.9692\n",
      "Epoch 83/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2231 - siamese_loss: 5.5780 - mlp_loss: 0.2231 - mlp_binary_accuracy: 0.9105 - mlp_auc_60: 0.9689\n",
      "Epoch 84/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2211 - siamese_loss: 5.6520 - mlp_loss: 0.2211 - mlp_binary_accuracy: 0.9163 - mlp_auc_60: 0.9694\n",
      "Epoch 85/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2195 - siamese_loss: 5.6226 - mlp_loss: 0.2195 - mlp_binary_accuracy: 0.9150 - mlp_auc_60: 0.9695\n",
      "Epoch 86/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2022 - siamese_loss: 5.5298 - mlp_loss: 0.2022 - mlp_binary_accuracy: 0.9215 - mlp_auc_60: 0.9737\n",
      "Epoch 87/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2123 - siamese_loss: 5.5003 - mlp_loss: 0.2123 - mlp_binary_accuracy: 0.9179 - mlp_auc_60: 0.9716\n",
      "Epoch 88/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2132 - siamese_loss: 5.3725 - mlp_loss: 0.2132 - mlp_binary_accuracy: 0.9160 - mlp_auc_60: 0.9718\n",
      "Epoch 89/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2115 - siamese_loss: 5.2965 - mlp_loss: 0.2115 - mlp_binary_accuracy: 0.9205 - mlp_auc_60: 0.9720\n",
      "Epoch 90/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2137 - siamese_loss: 5.4940 - mlp_loss: 0.2137 - mlp_binary_accuracy: 0.9163 - mlp_auc_60: 0.9715\n",
      "Epoch 91/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2063 - siamese_loss: 5.5825 - mlp_loss: 0.2063 - mlp_binary_accuracy: 0.9234 - mlp_auc_60: 0.9730\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7333 - siamese_loss: 5.7904 - mlp_loss: 0.7333 - mlp_binary_accuracy: 0.7364 - mlp_auc_60: 0.8241\n",
      "------ mlp_binary_accuracy: 73.64%\t ----- mlp_auc_60: 82.41%\n",
      "concat_fea的shape： (None, 112)\n",
      "-------------------------------------Kfold: 3 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.6760 - siamese_loss: 3.2239 - mlp_loss: 0.6760 - mlp_binary_accuracy: 0.5627 - mlp_auc_61: 0.6155\n",
      "Epoch 2/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.6295 - siamese_loss: 3.1604 - mlp_loss: 0.6295 - mlp_binary_accuracy: 0.6526 - mlp_auc_61: 0.7033\n",
      "Epoch 3/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.6009 - siamese_loss: 3.4930 - mlp_loss: 0.6009 - mlp_binary_accuracy: 0.6787 - mlp_auc_61: 0.7367\n",
      "Epoch 4/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5700 - siamese_loss: 3.8852 - mlp_loss: 0.5700 - mlp_binary_accuracy: 0.7091 - mlp_auc_61: 0.7736\n",
      "Epoch 5/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5612 - siamese_loss: 3.8817 - mlp_loss: 0.5612 - mlp_binary_accuracy: 0.7220 - mlp_auc_61: 0.7812\n",
      "Epoch 6/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5364 - siamese_loss: 4.0943 - mlp_loss: 0.5364 - mlp_binary_accuracy: 0.7453 - mlp_auc_61: 0.8051\n",
      "Epoch 7/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5233 - siamese_loss: 4.1239 - mlp_loss: 0.5233 - mlp_binary_accuracy: 0.7434 - mlp_auc_61: 0.8170\n",
      "Epoch 8/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5148 - siamese_loss: 3.9992 - mlp_loss: 0.5148 - mlp_binary_accuracy: 0.7502 - mlp_auc_61: 0.8237\n",
      "Epoch 9/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5057 - siamese_loss: 4.0279 - mlp_loss: 0.5057 - mlp_binary_accuracy: 0.7502 - mlp_auc_61: 0.8319\n",
      "Epoch 10/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4944 - siamese_loss: 4.1138 - mlp_loss: 0.4944 - mlp_binary_accuracy: 0.7634 - mlp_auc_61: 0.8417\n",
      "Epoch 11/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4828 - siamese_loss: 4.0394 - mlp_loss: 0.4828 - mlp_binary_accuracy: 0.7715 - mlp_auc_61: 0.8492\n",
      "Epoch 12/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4717 - siamese_loss: 4.0448 - mlp_loss: 0.4717 - mlp_binary_accuracy: 0.7763 - mlp_auc_61: 0.8575\n",
      "Epoch 13/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4701 - siamese_loss: 4.0171 - mlp_loss: 0.4701 - mlp_binary_accuracy: 0.7721 - mlp_auc_61: 0.8582\n",
      "Epoch 14/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4627 - siamese_loss: 4.0998 - mlp_loss: 0.4627 - mlp_binary_accuracy: 0.7893 - mlp_auc_61: 0.8629\n",
      "Epoch 15/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4529 - siamese_loss: 4.2670 - mlp_loss: 0.4529 - mlp_binary_accuracy: 0.7847 - mlp_auc_61: 0.8693\n",
      "Epoch 16/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4402 - siamese_loss: 4.2011 - mlp_loss: 0.4402 - mlp_binary_accuracy: 0.7996 - mlp_auc_61: 0.8775\n",
      "Epoch 17/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4320 - siamese_loss: 4.4471 - mlp_loss: 0.4320 - mlp_binary_accuracy: 0.8019 - mlp_auc_61: 0.8818\n",
      "Epoch 18/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4322 - siamese_loss: 4.6052 - mlp_loss: 0.4322 - mlp_binary_accuracy: 0.8019 - mlp_auc_61: 0.8822\n",
      "Epoch 19/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4145 - siamese_loss: 4.4211 - mlp_loss: 0.4145 - mlp_binary_accuracy: 0.8138 - mlp_auc_61: 0.8921\n",
      "Epoch 20/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4169 - siamese_loss: 4.5929 - mlp_loss: 0.4169 - mlp_binary_accuracy: 0.8090 - mlp_auc_61: 0.8903\n",
      "Epoch 21/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4007 - siamese_loss: 4.6868 - mlp_loss: 0.4007 - mlp_binary_accuracy: 0.8167 - mlp_auc_61: 0.8996\n",
      "Epoch 22/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4140 - siamese_loss: 4.8504 - mlp_loss: 0.4140 - mlp_binary_accuracy: 0.8022 - mlp_auc_61: 0.8921\n",
      "Epoch 23/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3953 - siamese_loss: 4.8126 - mlp_loss: 0.3953 - mlp_binary_accuracy: 0.8206 - mlp_auc_61: 0.9025\n",
      "Epoch 24/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3914 - siamese_loss: 4.8338 - mlp_loss: 0.3914 - mlp_binary_accuracy: 0.8255 - mlp_auc_61: 0.9043\n",
      "Epoch 25/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3802 - siamese_loss: 4.9235 - mlp_loss: 0.3802 - mlp_binary_accuracy: 0.8306 - mlp_auc_61: 0.9103\n",
      "Epoch 26/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3743 - siamese_loss: 4.9458 - mlp_loss: 0.3743 - mlp_binary_accuracy: 0.8342 - mlp_auc_61: 0.9132\n",
      "Epoch 27/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3763 - siamese_loss: 4.8819 - mlp_loss: 0.3763 - mlp_binary_accuracy: 0.8329 - mlp_auc_61: 0.9122\n",
      "Epoch 28/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3734 - siamese_loss: 4.9426 - mlp_loss: 0.3734 - mlp_binary_accuracy: 0.8329 - mlp_auc_61: 0.9140\n",
      "Epoch 29/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3641 - siamese_loss: 4.9351 - mlp_loss: 0.3641 - mlp_binary_accuracy: 0.8384 - mlp_auc_61: 0.9180\n",
      "Epoch 30/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3517 - siamese_loss: 5.0322 - mlp_loss: 0.3517 - mlp_binary_accuracy: 0.8458 - mlp_auc_61: 0.9236\n",
      "Epoch 31/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3424 - siamese_loss: 5.0417 - mlp_loss: 0.3424 - mlp_binary_accuracy: 0.8491 - mlp_auc_61: 0.9280\n",
      "Epoch 32/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3459 - siamese_loss: 4.9951 - mlp_loss: 0.3459 - mlp_binary_accuracy: 0.8520 - mlp_auc_61: 0.9265\n",
      "Epoch 33/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3396 - siamese_loss: 5.0398 - mlp_loss: 0.3396 - mlp_binary_accuracy: 0.8526 - mlp_auc_61: 0.9293\n",
      "Epoch 34/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3340 - siamese_loss: 5.0026 - mlp_loss: 0.3340 - mlp_binary_accuracy: 0.8591 - mlp_auc_61: 0.9317\n",
      "Epoch 35/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3321 - siamese_loss: 5.2251 - mlp_loss: 0.3321 - mlp_binary_accuracy: 0.8555 - mlp_auc_61: 0.9321\n",
      "Epoch 36/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3273 - siamese_loss: 5.0654 - mlp_loss: 0.3273 - mlp_binary_accuracy: 0.8575 - mlp_auc_61: 0.9342\n",
      "Epoch 37/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3197 - siamese_loss: 5.1666 - mlp_loss: 0.3197 - mlp_binary_accuracy: 0.8607 - mlp_auc_61: 0.9378\n",
      "Epoch 38/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3269 - siamese_loss: 5.2292 - mlp_loss: 0.3269 - mlp_binary_accuracy: 0.8636 - mlp_auc_61: 0.9343\n",
      "Epoch 39/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3080 - siamese_loss: 5.2320 - mlp_loss: 0.3080 - mlp_binary_accuracy: 0.8678 - mlp_auc_61: 0.9420\n",
      "Epoch 40/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3219 - siamese_loss: 5.1985 - mlp_loss: 0.3219 - mlp_binary_accuracy: 0.8584 - mlp_auc_61: 0.9363\n",
      "Epoch 41/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3016 - siamese_loss: 5.1397 - mlp_loss: 0.3016 - mlp_binary_accuracy: 0.8736 - mlp_auc_61: 0.9443\n",
      "Epoch 42/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3132 - siamese_loss: 5.0421 - mlp_loss: 0.3132 - mlp_binary_accuracy: 0.8639 - mlp_auc_61: 0.9400\n",
      "Epoch 43/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3017 - siamese_loss: 5.2564 - mlp_loss: 0.3017 - mlp_binary_accuracy: 0.8727 - mlp_auc_61: 0.9445\n",
      "Epoch 44/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2941 - siamese_loss: 5.2040 - mlp_loss: 0.2941 - mlp_binary_accuracy: 0.8788 - mlp_auc_61: 0.9472\n",
      "Epoch 45/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2975 - siamese_loss: 5.1649 - mlp_loss: 0.2975 - mlp_binary_accuracy: 0.8720 - mlp_auc_61: 0.9456\n",
      "Epoch 46/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2968 - siamese_loss: 5.1861 - mlp_loss: 0.2968 - mlp_binary_accuracy: 0.8746 - mlp_auc_61: 0.9458\n",
      "Epoch 47/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2753 - siamese_loss: 5.3116 - mlp_loss: 0.2753 - mlp_binary_accuracy: 0.8875 - mlp_auc_61: 0.9533\n",
      "Epoch 48/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2824 - siamese_loss: 5.2365 - mlp_loss: 0.2824 - mlp_binary_accuracy: 0.8817 - mlp_auc_61: 0.9510\n",
      "Epoch 49/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2811 - siamese_loss: 5.2319 - mlp_loss: 0.2811 - mlp_binary_accuracy: 0.8846 - mlp_auc_61: 0.9516\n",
      "Epoch 50/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2747 - siamese_loss: 5.2356 - mlp_loss: 0.2747 - mlp_binary_accuracy: 0.8898 - mlp_auc_61: 0.9534\n",
      "Epoch 51/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2831 - siamese_loss: 5.3228 - mlp_loss: 0.2831 - mlp_binary_accuracy: 0.8814 - mlp_auc_61: 0.9513\n",
      "Epoch 52/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2728 - siamese_loss: 5.3524 - mlp_loss: 0.2728 - mlp_binary_accuracy: 0.8882 - mlp_auc_61: 0.9542\n",
      "Epoch 53/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2685 - siamese_loss: 5.2854 - mlp_loss: 0.2685 - mlp_binary_accuracy: 0.8856 - mlp_auc_61: 0.9557\n",
      "Epoch 54/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2747 - siamese_loss: 5.2238 - mlp_loss: 0.2747 - mlp_binary_accuracy: 0.8888 - mlp_auc_61: 0.9534\n",
      "Epoch 55/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2741 - siamese_loss: 5.1946 - mlp_loss: 0.2741 - mlp_binary_accuracy: 0.8836 - mlp_auc_61: 0.9534\n",
      "Epoch 56/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2532 - siamese_loss: 5.2460 - mlp_loss: 0.2532 - mlp_binary_accuracy: 0.8982 - mlp_auc_61: 0.9604\n",
      "Epoch 57/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2623 - siamese_loss: 5.3114 - mlp_loss: 0.2623 - mlp_binary_accuracy: 0.8917 - mlp_auc_61: 0.9576\n",
      "Epoch 58/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2577 - siamese_loss: 5.3691 - mlp_loss: 0.2577 - mlp_binary_accuracy: 0.8901 - mlp_auc_61: 0.9593\n",
      "Epoch 59/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2545 - siamese_loss: 5.4084 - mlp_loss: 0.2545 - mlp_binary_accuracy: 0.8937 - mlp_auc_61: 0.9607\n",
      "Epoch 60/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2468 - siamese_loss: 5.2514 - mlp_loss: 0.2468 - mlp_binary_accuracy: 0.8927 - mlp_auc_61: 0.9624\n",
      "Epoch 61/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2444 - siamese_loss: 5.1594 - mlp_loss: 0.2444 - mlp_binary_accuracy: 0.9014 - mlp_auc_61: 0.9632\n",
      "Epoch 62/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2265 - siamese_loss: 5.1436 - mlp_loss: 0.2265 - mlp_binary_accuracy: 0.9072 - mlp_auc_61: 0.9682\n",
      "Epoch 63/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2357 - siamese_loss: 5.2034 - mlp_loss: 0.2357 - mlp_binary_accuracy: 0.9066 - mlp_auc_61: 0.9653\n",
      "Epoch 64/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2350 - siamese_loss: 5.2062 - mlp_loss: 0.2350 - mlp_binary_accuracy: 0.9034 - mlp_auc_61: 0.9660\n",
      "Epoch 65/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2165 - siamese_loss: 5.2745 - mlp_loss: 0.2165 - mlp_binary_accuracy: 0.9124 - mlp_auc_61: 0.9712\n",
      "Epoch 66/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2215 - siamese_loss: 5.3018 - mlp_loss: 0.2215 - mlp_binary_accuracy: 0.9085 - mlp_auc_61: 0.9699\n",
      "Epoch 67/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2269 - siamese_loss: 5.2230 - mlp_loss: 0.2269 - mlp_binary_accuracy: 0.9118 - mlp_auc_61: 0.9682\n",
      "Epoch 68/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2171 - siamese_loss: 5.2101 - mlp_loss: 0.2171 - mlp_binary_accuracy: 0.9127 - mlp_auc_61: 0.9706\n",
      "Epoch 69/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2139 - siamese_loss: 5.2558 - mlp_loss: 0.2139 - mlp_binary_accuracy: 0.9173 - mlp_auc_61: 0.9717\n",
      "Epoch 70/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2150 - siamese_loss: 5.2962 - mlp_loss: 0.2150 - mlp_binary_accuracy: 0.9121 - mlp_auc_61: 0.9718\n",
      "Epoch 71/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2060 - siamese_loss: 5.2479 - mlp_loss: 0.2060 - mlp_binary_accuracy: 0.9198 - mlp_auc_61: 0.9736\n",
      "Epoch 72/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2112 - siamese_loss: 5.3390 - mlp_loss: 0.2112 - mlp_binary_accuracy: 0.9173 - mlp_auc_61: 0.9720\n",
      "Epoch 73/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2085 - siamese_loss: 5.4012 - mlp_loss: 0.2085 - mlp_binary_accuracy: 0.9176 - mlp_auc_61: 0.9733\n",
      "Epoch 74/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2141 - siamese_loss: 5.3426 - mlp_loss: 0.2141 - mlp_binary_accuracy: 0.9114 - mlp_auc_61: 0.9718\n",
      "Epoch 75/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2109 - siamese_loss: 5.3443 - mlp_loss: 0.2109 - mlp_binary_accuracy: 0.9163 - mlp_auc_61: 0.9723\n",
      "Epoch 76/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1967 - siamese_loss: 5.4014 - mlp_loss: 0.1967 - mlp_binary_accuracy: 0.9173 - mlp_auc_61: 0.9758\n",
      "Epoch 77/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1900 - siamese_loss: 5.5419 - mlp_loss: 0.1900 - mlp_binary_accuracy: 0.9240 - mlp_auc_61: 0.9776\n",
      "Epoch 78/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1912 - siamese_loss: 5.5631 - mlp_loss: 0.1912 - mlp_binary_accuracy: 0.9244 - mlp_auc_61: 0.9774\n",
      "Epoch 79/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2023 - siamese_loss: 5.4909 - mlp_loss: 0.2023 - mlp_binary_accuracy: 0.9192 - mlp_auc_61: 0.9748\n",
      "Epoch 80/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1866 - siamese_loss: 5.5126 - mlp_loss: 0.1866 - mlp_binary_accuracy: 0.9215 - mlp_auc_61: 0.9784\n",
      "Epoch 81/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1930 - siamese_loss: 5.5548 - mlp_loss: 0.1930 - mlp_binary_accuracy: 0.9234 - mlp_auc_61: 0.9769\n",
      "Epoch 82/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1775 - siamese_loss: 5.5166 - mlp_loss: 0.1775 - mlp_binary_accuracy: 0.9257 - mlp_auc_61: 0.9808\n",
      "Epoch 83/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1829 - siamese_loss: 5.4091 - mlp_loss: 0.1829 - mlp_binary_accuracy: 0.9292 - mlp_auc_61: 0.9794\n",
      "Epoch 84/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1863 - siamese_loss: 5.4449 - mlp_loss: 0.1863 - mlp_binary_accuracy: 0.9247 - mlp_auc_61: 0.9782\n",
      "Epoch 85/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1959 - siamese_loss: 5.4121 - mlp_loss: 0.1959 - mlp_binary_accuracy: 0.9198 - mlp_auc_61: 0.9762\n",
      "Epoch 86/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1834 - siamese_loss: 5.4149 - mlp_loss: 0.1834 - mlp_binary_accuracy: 0.9237 - mlp_auc_61: 0.9794\n",
      "Epoch 87/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1900 - siamese_loss: 5.4333 - mlp_loss: 0.1900 - mlp_binary_accuracy: 0.9266 - mlp_auc_61: 0.9771\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6982 - siamese_loss: 5.4257 - mlp_loss: 0.6982 - mlp_binary_accuracy: 0.7649 - mlp_auc_61: 0.8446\n",
      "------ mlp_binary_accuracy: 76.49%\t ----- mlp_auc_61: 84.46%\n",
      "concat_fea的shape： (None, 112)\n",
      "-------------------------------------Kfold: 4 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.6729 - siamese_loss: 2.7263 - mlp_loss: 0.6729 - mlp_binary_accuracy: 0.5861 - mlp_auc_62: 0.6178\n",
      "Epoch 2/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.6214 - siamese_loss: 3.1366 - mlp_loss: 0.6214 - mlp_binary_accuracy: 0.6575 - mlp_auc_62: 0.7136\n",
      "Epoch 3/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5851 - siamese_loss: 3.2272 - mlp_loss: 0.5851 - mlp_binary_accuracy: 0.7005 - mlp_auc_62: 0.7587\n",
      "Epoch 4/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5670 - siamese_loss: 3.5297 - mlp_loss: 0.5670 - mlp_binary_accuracy: 0.7076 - mlp_auc_62: 0.7778\n",
      "Epoch 5/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5585 - siamese_loss: 3.8824 - mlp_loss: 0.5585 - mlp_binary_accuracy: 0.7134 - mlp_auc_62: 0.7863\n",
      "Epoch 6/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5429 - siamese_loss: 4.0731 - mlp_loss: 0.5429 - mlp_binary_accuracy: 0.7325 - mlp_auc_62: 0.8004\n",
      "Epoch 7/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5259 - siamese_loss: 3.6406 - mlp_loss: 0.5259 - mlp_binary_accuracy: 0.7418 - mlp_auc_62: 0.8154\n",
      "Epoch 8/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5231 - siamese_loss: 3.7114 - mlp_loss: 0.5231 - mlp_binary_accuracy: 0.7431 - mlp_auc_62: 0.8172\n",
      "Epoch 9/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5159 - siamese_loss: 3.7774 - mlp_loss: 0.5159 - mlp_binary_accuracy: 0.7519 - mlp_auc_62: 0.8248\n",
      "Epoch 10/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5107 - siamese_loss: 3.6479 - mlp_loss: 0.5107 - mlp_binary_accuracy: 0.7496 - mlp_auc_62: 0.8284\n",
      "Epoch 11/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4964 - siamese_loss: 3.7740 - mlp_loss: 0.4964 - mlp_binary_accuracy: 0.7628 - mlp_auc_62: 0.8393\n",
      "Epoch 12/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4968 - siamese_loss: 3.8576 - mlp_loss: 0.4968 - mlp_binary_accuracy: 0.7667 - mlp_auc_62: 0.8390\n",
      "Epoch 13/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4848 - siamese_loss: 3.9467 - mlp_loss: 0.4848 - mlp_binary_accuracy: 0.7719 - mlp_auc_62: 0.8478\n",
      "Epoch 14/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4756 - siamese_loss: 3.9939 - mlp_loss: 0.4756 - mlp_binary_accuracy: 0.7790 - mlp_auc_62: 0.8547\n",
      "Epoch 15/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4755 - siamese_loss: 3.8758 - mlp_loss: 0.4755 - mlp_binary_accuracy: 0.7800 - mlp_auc_62: 0.8551\n",
      "Epoch 16/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4629 - siamese_loss: 3.9617 - mlp_loss: 0.4629 - mlp_binary_accuracy: 0.7809 - mlp_auc_62: 0.8635\n",
      "Epoch 17/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4663 - siamese_loss: 4.0173 - mlp_loss: 0.4663 - mlp_binary_accuracy: 0.7738 - mlp_auc_62: 0.8606\n",
      "Epoch 18/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4639 - siamese_loss: 4.0181 - mlp_loss: 0.4639 - mlp_binary_accuracy: 0.7855 - mlp_auc_62: 0.8625\n",
      "Epoch 19/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4555 - siamese_loss: 4.0918 - mlp_loss: 0.4555 - mlp_binary_accuracy: 0.7945 - mlp_auc_62: 0.8675\n",
      "Epoch 20/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4420 - siamese_loss: 4.1851 - mlp_loss: 0.4420 - mlp_binary_accuracy: 0.7964 - mlp_auc_62: 0.8765\n",
      "Epoch 21/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4433 - siamese_loss: 3.9589 - mlp_loss: 0.4433 - mlp_binary_accuracy: 0.7974 - mlp_auc_62: 0.8761\n",
      "Epoch 22/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4341 - siamese_loss: 4.0819 - mlp_loss: 0.4341 - mlp_binary_accuracy: 0.8087 - mlp_auc_62: 0.8818\n",
      "Epoch 23/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4260 - siamese_loss: 4.1208 - mlp_loss: 0.4260 - mlp_binary_accuracy: 0.8078 - mlp_auc_62: 0.8859\n",
      "Epoch 24/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4199 - siamese_loss: 4.2717 - mlp_loss: 0.4199 - mlp_binary_accuracy: 0.8065 - mlp_auc_62: 0.8893\n",
      "Epoch 25/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4143 - siamese_loss: 4.6684 - mlp_loss: 0.4143 - mlp_binary_accuracy: 0.8168 - mlp_auc_62: 0.8924\n",
      "Epoch 26/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4086 - siamese_loss: 4.6520 - mlp_loss: 0.4086 - mlp_binary_accuracy: 0.8168 - mlp_auc_62: 0.8953\n",
      "Epoch 27/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4051 - siamese_loss: 4.8082 - mlp_loss: 0.4051 - mlp_binary_accuracy: 0.8210 - mlp_auc_62: 0.8976\n",
      "Epoch 28/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4045 - siamese_loss: 4.8100 - mlp_loss: 0.4045 - mlp_binary_accuracy: 0.8281 - mlp_auc_62: 0.8978\n",
      "Epoch 29/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3941 - siamese_loss: 4.6743 - mlp_loss: 0.3941 - mlp_binary_accuracy: 0.8278 - mlp_auc_62: 0.9031\n",
      "Epoch 30/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3883 - siamese_loss: 4.6024 - mlp_loss: 0.3883 - mlp_binary_accuracy: 0.8275 - mlp_auc_62: 0.9060\n",
      "Epoch 31/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3854 - siamese_loss: 4.6267 - mlp_loss: 0.3854 - mlp_binary_accuracy: 0.8262 - mlp_auc_62: 0.9072\n",
      "Epoch 32/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3738 - siamese_loss: 4.3925 - mlp_loss: 0.3738 - mlp_binary_accuracy: 0.8388 - mlp_auc_62: 0.9135\n",
      "Epoch 33/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3786 - siamese_loss: 4.4520 - mlp_loss: 0.3786 - mlp_binary_accuracy: 0.8362 - mlp_auc_62: 0.9107\n",
      "Epoch 34/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3589 - siamese_loss: 4.3570 - mlp_loss: 0.3589 - mlp_binary_accuracy: 0.8472 - mlp_auc_62: 0.9202\n",
      "Epoch 35/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3607 - siamese_loss: 4.4538 - mlp_loss: 0.3607 - mlp_binary_accuracy: 0.8462 - mlp_auc_62: 0.9193\n",
      "Epoch 36/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3583 - siamese_loss: 4.5804 - mlp_loss: 0.3583 - mlp_binary_accuracy: 0.8436 - mlp_auc_62: 0.9207\n",
      "Epoch 37/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3559 - siamese_loss: 4.7565 - mlp_loss: 0.3559 - mlp_binary_accuracy: 0.8456 - mlp_auc_62: 0.9219\n",
      "Epoch 38/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3419 - siamese_loss: 4.5255 - mlp_loss: 0.3419 - mlp_binary_accuracy: 0.8543 - mlp_auc_62: 0.9280\n",
      "Epoch 39/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3456 - siamese_loss: 4.6871 - mlp_loss: 0.3456 - mlp_binary_accuracy: 0.8533 - mlp_auc_62: 0.9263\n",
      "Epoch 40/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3311 - siamese_loss: 4.6731 - mlp_loss: 0.3311 - mlp_binary_accuracy: 0.8607 - mlp_auc_62: 0.9326\n",
      "Epoch 41/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3333 - siamese_loss: 4.8121 - mlp_loss: 0.3333 - mlp_binary_accuracy: 0.8601 - mlp_auc_62: 0.9314\n",
      "Epoch 42/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3285 - siamese_loss: 4.8280 - mlp_loss: 0.3285 - mlp_binary_accuracy: 0.8649 - mlp_auc_62: 0.9335\n",
      "Epoch 43/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3206 - siamese_loss: 4.8246 - mlp_loss: 0.3206 - mlp_binary_accuracy: 0.8679 - mlp_auc_62: 0.9369\n",
      "Epoch 44/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3279 - siamese_loss: 4.9874 - mlp_loss: 0.3279 - mlp_binary_accuracy: 0.8620 - mlp_auc_62: 0.9337\n",
      "Epoch 45/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3130 - siamese_loss: 4.8556 - mlp_loss: 0.3130 - mlp_binary_accuracy: 0.8682 - mlp_auc_62: 0.9398\n",
      "Epoch 46/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3021 - siamese_loss: 4.8732 - mlp_loss: 0.3021 - mlp_binary_accuracy: 0.8772 - mlp_auc_62: 0.9438\n",
      "Epoch 47/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3040 - siamese_loss: 4.8368 - mlp_loss: 0.3040 - mlp_binary_accuracy: 0.8701 - mlp_auc_62: 0.9429\n",
      "Epoch 48/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3027 - siamese_loss: 4.9669 - mlp_loss: 0.3027 - mlp_binary_accuracy: 0.8763 - mlp_auc_62: 0.9432\n",
      "Epoch 49/200\n",
      "97/97 [==============================] - ETA: 0s - loss: 0.2856 - siamese_loss: 4.6932 - mlp_loss: 0.2856 - mlp_binary_accuracy: 0.8825 - mlp_auc_62: 0.94 - 0s 2ms/step - loss: 0.2901 - siamese_loss: 4.7698 - mlp_loss: 0.2901 - mlp_binary_accuracy: 0.8795 - mlp_auc_62: 0.9479\n",
      "Epoch 50/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2937 - siamese_loss: 4.8229 - mlp_loss: 0.2937 - mlp_binary_accuracy: 0.8788 - mlp_auc_62: 0.9461\n",
      "Epoch 51/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2824 - siamese_loss: 4.8527 - mlp_loss: 0.2824 - mlp_binary_accuracy: 0.8840 - mlp_auc_62: 0.9504\n",
      "Epoch 52/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2931 - siamese_loss: 4.9286 - mlp_loss: 0.2931 - mlp_binary_accuracy: 0.8782 - mlp_auc_62: 0.9469\n",
      "Epoch 53/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2849 - siamese_loss: 4.8970 - mlp_loss: 0.2849 - mlp_binary_accuracy: 0.8876 - mlp_auc_62: 0.9500\n",
      "Epoch 54/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2845 - siamese_loss: 4.6950 - mlp_loss: 0.2845 - mlp_binary_accuracy: 0.8892 - mlp_auc_62: 0.9498\n",
      "Epoch 55/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2814 - siamese_loss: 4.7963 - mlp_loss: 0.2814 - mlp_binary_accuracy: 0.8843 - mlp_auc_62: 0.9509\n",
      "Epoch 56/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2857 - siamese_loss: 4.8240 - mlp_loss: 0.2857 - mlp_binary_accuracy: 0.8834 - mlp_auc_62: 0.9491\n",
      "Epoch 57/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2732 - siamese_loss: 4.7916 - mlp_loss: 0.2732 - mlp_binary_accuracy: 0.8937 - mlp_auc_62: 0.9532\n",
      "Epoch 58/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2797 - siamese_loss: 4.7462 - mlp_loss: 0.2797 - mlp_binary_accuracy: 0.8853 - mlp_auc_62: 0.9520\n",
      "Epoch 59/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2708 - siamese_loss: 4.8290 - mlp_loss: 0.2708 - mlp_binary_accuracy: 0.8914 - mlp_auc_62: 0.9547\n",
      "Epoch 60/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2716 - siamese_loss: 4.7379 - mlp_loss: 0.2716 - mlp_binary_accuracy: 0.8895 - mlp_auc_62: 0.9540\n",
      "Epoch 61/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2623 - siamese_loss: 4.7596 - mlp_loss: 0.2623 - mlp_binary_accuracy: 0.8931 - mlp_auc_62: 0.9574\n",
      "Epoch 62/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2554 - siamese_loss: 4.9170 - mlp_loss: 0.2554 - mlp_binary_accuracy: 0.8982 - mlp_auc_62: 0.9596\n",
      "Epoch 63/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2438 - siamese_loss: 4.9651 - mlp_loss: 0.2438 - mlp_binary_accuracy: 0.9063 - mlp_auc_62: 0.9626\n",
      "Epoch 64/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2422 - siamese_loss: 4.8450 - mlp_loss: 0.2422 - mlp_binary_accuracy: 0.9066 - mlp_auc_62: 0.9638\n",
      "Epoch 65/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2523 - siamese_loss: 4.8945 - mlp_loss: 0.2523 - mlp_binary_accuracy: 0.8953 - mlp_auc_62: 0.9607\n",
      "Epoch 66/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2500 - siamese_loss: 4.9614 - mlp_loss: 0.2500 - mlp_binary_accuracy: 0.9021 - mlp_auc_62: 0.9606\n",
      "Epoch 67/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2354 - siamese_loss: 4.9281 - mlp_loss: 0.2354 - mlp_binary_accuracy: 0.9050 - mlp_auc_62: 0.9658\n",
      "Epoch 68/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2405 - siamese_loss: 4.9225 - mlp_loss: 0.2405 - mlp_binary_accuracy: 0.9037 - mlp_auc_62: 0.9638\n",
      "Epoch 69/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2402 - siamese_loss: 4.8218 - mlp_loss: 0.2402 - mlp_binary_accuracy: 0.9011 - mlp_auc_62: 0.9645\n",
      "Epoch 70/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2380 - siamese_loss: 4.8799 - mlp_loss: 0.2380 - mlp_binary_accuracy: 0.9050 - mlp_auc_62: 0.9648\n",
      "Epoch 71/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2385 - siamese_loss: 4.8433 - mlp_loss: 0.2385 - mlp_binary_accuracy: 0.9031 - mlp_auc_62: 0.9646\n",
      "Epoch 72/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2505 - siamese_loss: 4.9697 - mlp_loss: 0.2505 - mlp_binary_accuracy: 0.8937 - mlp_auc_62: 0.9613\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5969 - siamese_loss: 4.9001 - mlp_loss: 0.5969 - mlp_binary_accuracy: 0.7853 - mlp_auc_62: 0.8562\n",
      "------ mlp_binary_accuracy: 78.53%\t ----- mlp_auc_62: 85.62%\n",
      "concat_fea的shape： (None, 112)\n",
      "-------------------------------------Kfold: 5 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.6920 - siamese_loss: 2.9435 - mlp_loss: 0.6920 - mlp_binary_accuracy: 0.5257 - mlp_auc_63: 0.5508\n",
      "Epoch 2/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.6679 - siamese_loss: 2.9942 - mlp_loss: 0.6679 - mlp_binary_accuracy: 0.6107 - mlp_auc_63: 0.6484\n",
      "Epoch 3/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.6465 - siamese_loss: 2.6989 - mlp_loss: 0.6465 - mlp_binary_accuracy: 0.6472 - mlp_auc_63: 0.6854\n",
      "Epoch 4/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.6333 - siamese_loss: 3.0268 - mlp_loss: 0.6333 - mlp_binary_accuracy: 0.6620 - mlp_auc_63: 0.7050\n",
      "Epoch 5/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.6132 - siamese_loss: 3.2291 - mlp_loss: 0.6132 - mlp_binary_accuracy: 0.6940 - mlp_auc_63: 0.7341\n",
      "Epoch 6/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.6031 - siamese_loss: 3.3673 - mlp_loss: 0.6031 - mlp_binary_accuracy: 0.6947 - mlp_auc_63: 0.7369\n",
      "Epoch 7/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5929 - siamese_loss: 3.4941 - mlp_loss: 0.5929 - mlp_binary_accuracy: 0.7124 - mlp_auc_63: 0.7569\n",
      "Epoch 8/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.5875 - siamese_loss: 3.6531 - mlp_loss: 0.5875 - mlp_binary_accuracy: 0.7053 - mlp_auc_63: 0.7532\n",
      "Epoch 9/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.5788 - siamese_loss: 3.4326 - mlp_loss: 0.5788 - mlp_binary_accuracy: 0.7102 - mlp_auc_63: 0.7519\n",
      "Epoch 10/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.5624 - siamese_loss: 3.3475 - mlp_loss: 0.5624 - mlp_binary_accuracy: 0.7315 - mlp_auc_63: 0.7785\n",
      "Epoch 11/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5597 - siamese_loss: 3.6643 - mlp_loss: 0.5597 - mlp_binary_accuracy: 0.7367 - mlp_auc_63: 0.7819\n",
      "Epoch 12/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.5514 - siamese_loss: 3.7800 - mlp_loss: 0.5514 - mlp_binary_accuracy: 0.7334 - mlp_auc_63: 0.7835\n",
      "Epoch 13/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.5453 - siamese_loss: 4.0302 - mlp_loss: 0.5453 - mlp_binary_accuracy: 0.7409 - mlp_auc_63: 0.7940\n",
      "Epoch 14/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5371 - siamese_loss: 4.1513 - mlp_loss: 0.5371 - mlp_binary_accuracy: 0.7477 - mlp_auc_63: 0.7899\n",
      "Epoch 15/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.5353 - siamese_loss: 4.1273 - mlp_loss: 0.5353 - mlp_binary_accuracy: 0.7467 - mlp_auc_63: 0.7987\n",
      "Epoch 16/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.5236 - siamese_loss: 3.9915 - mlp_loss: 0.5236 - mlp_binary_accuracy: 0.7538 - mlp_auc_63: 0.8064\n",
      "Epoch 17/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5232 - siamese_loss: 3.9493 - mlp_loss: 0.5232 - mlp_binary_accuracy: 0.7515 - mlp_auc_63: 0.8075\n",
      "Epoch 18/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5181 - siamese_loss: 4.2012 - mlp_loss: 0.5181 - mlp_binary_accuracy: 0.7586 - mlp_auc_63: 0.8193\n",
      "Epoch 19/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5198 - siamese_loss: 4.2981 - mlp_loss: 0.5198 - mlp_binary_accuracy: 0.7606 - mlp_auc_63: 0.8153\n",
      "Epoch 20/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4976 - siamese_loss: 4.3346 - mlp_loss: 0.4976 - mlp_binary_accuracy: 0.7719 - mlp_auc_63: 0.8333\n",
      "Epoch 21/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4961 - siamese_loss: 4.3210 - mlp_loss: 0.4961 - mlp_binary_accuracy: 0.7813 - mlp_auc_63: 0.8340\n",
      "Epoch 22/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4892 - siamese_loss: 4.4909 - mlp_loss: 0.4892 - mlp_binary_accuracy: 0.7800 - mlp_auc_63: 0.8379\n",
      "Epoch 23/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4860 - siamese_loss: 4.3861 - mlp_loss: 0.4860 - mlp_binary_accuracy: 0.7813 - mlp_auc_63: 0.8471\n",
      "Epoch 24/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4777 - siamese_loss: 4.4373 - mlp_loss: 0.4777 - mlp_binary_accuracy: 0.7780 - mlp_auc_63: 0.8476\n",
      "Epoch 25/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4761 - siamese_loss: 4.3132 - mlp_loss: 0.4761 - mlp_binary_accuracy: 0.7832 - mlp_auc_63: 0.8496\n",
      "Epoch 26/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4676 - siamese_loss: 4.5456 - mlp_loss: 0.4676 - mlp_binary_accuracy: 0.7984 - mlp_auc_63: 0.8557\n",
      "Epoch 27/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4510 - siamese_loss: 4.5396 - mlp_loss: 0.4510 - mlp_binary_accuracy: 0.7987 - mlp_auc_63: 0.8654\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4522 - siamese_loss: 4.7038 - mlp_loss: 0.4522 - mlp_binary_accuracy: 0.7961 - mlp_auc_63: 0.8646\n",
      "Epoch 29/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4533 - siamese_loss: 4.7822 - mlp_loss: 0.4533 - mlp_binary_accuracy: 0.8039 - mlp_auc_63: 0.8637\n",
      "Epoch 30/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4454 - siamese_loss: 4.8157 - mlp_loss: 0.4454 - mlp_binary_accuracy: 0.8081 - mlp_auc_63: 0.8674\n",
      "Epoch 31/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4365 - siamese_loss: 4.8619 - mlp_loss: 0.4365 - mlp_binary_accuracy: 0.8097 - mlp_auc_63: 0.8736\n",
      "Epoch 32/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4353 - siamese_loss: 5.0266 - mlp_loss: 0.4353 - mlp_binary_accuracy: 0.8181 - mlp_auc_63: 0.8778\n",
      "Epoch 33/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4323 - siamese_loss: 4.9395 - mlp_loss: 0.4323 - mlp_binary_accuracy: 0.8078 - mlp_auc_63: 0.8769\n",
      "Epoch 34/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4236 - siamese_loss: 5.2489 - mlp_loss: 0.4236 - mlp_binary_accuracy: 0.8149 - mlp_auc_63: 0.8827\n",
      "Epoch 35/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4158 - siamese_loss: 5.1745 - mlp_loss: 0.4158 - mlp_binary_accuracy: 0.8265 - mlp_auc_63: 0.8835\n",
      "Epoch 36/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4202 - siamese_loss: 5.0434 - mlp_loss: 0.4202 - mlp_binary_accuracy: 0.8197 - mlp_auc_63: 0.8815\n",
      "Epoch 37/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4139 - siamese_loss: 5.0008 - mlp_loss: 0.4139 - mlp_binary_accuracy: 0.8187 - mlp_auc_63: 0.8838\n",
      "Epoch 38/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4070 - siamese_loss: 4.8540 - mlp_loss: 0.4070 - mlp_binary_accuracy: 0.8258 - mlp_auc_63: 0.8870\n",
      "Epoch 39/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4121 - siamese_loss: 4.9585 - mlp_loss: 0.4121 - mlp_binary_accuracy: 0.8255 - mlp_auc_63: 0.8881\n",
      "Epoch 40/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4027 - siamese_loss: 5.1036 - mlp_loss: 0.4027 - mlp_binary_accuracy: 0.8297 - mlp_auc_63: 0.8936\n",
      "Epoch 41/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4028 - siamese_loss: 5.0701 - mlp_loss: 0.4028 - mlp_binary_accuracy: 0.8300 - mlp_auc_63: 0.8882\n",
      "Epoch 42/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4036 - siamese_loss: 5.2144 - mlp_loss: 0.4036 - mlp_binary_accuracy: 0.8313 - mlp_auc_63: 0.8917\n",
      "Epoch 43/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.4021 - siamese_loss: 5.2621 - mlp_loss: 0.4021 - mlp_binary_accuracy: 0.8278 - mlp_auc_63: 0.8915\n",
      "Epoch 44/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3893 - siamese_loss: 5.2781 - mlp_loss: 0.3893 - mlp_binary_accuracy: 0.8313 - mlp_auc_63: 0.9011\n",
      "Epoch 45/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3800 - siamese_loss: 5.1324 - mlp_loss: 0.3800 - mlp_binary_accuracy: 0.8394 - mlp_auc_63: 0.9055\n",
      "Epoch 46/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3697 - siamese_loss: 5.1343 - mlp_loss: 0.3697 - mlp_binary_accuracy: 0.8452 - mlp_auc_63: 0.9080\n",
      "Epoch 47/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3729 - siamese_loss: 5.3959 - mlp_loss: 0.3729 - mlp_binary_accuracy: 0.8478 - mlp_auc_63: 0.9076\n",
      "Epoch 48/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3758 - siamese_loss: 5.3259 - mlp_loss: 0.3758 - mlp_binary_accuracy: 0.8414 - mlp_auc_63: 0.9060\n",
      "Epoch 49/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3637 - siamese_loss: 5.3039 - mlp_loss: 0.3637 - mlp_binary_accuracy: 0.8514 - mlp_auc_63: 0.9123\n",
      "Epoch 50/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3565 - siamese_loss: 5.2013 - mlp_loss: 0.3565 - mlp_binary_accuracy: 0.8501 - mlp_auc_63: 0.9146\n",
      "Epoch 51/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3650 - siamese_loss: 5.2522 - mlp_loss: 0.3650 - mlp_binary_accuracy: 0.8423 - mlp_auc_63: 0.9109\n",
      "Epoch 52/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3501 - siamese_loss: 5.3244 - mlp_loss: 0.3501 - mlp_binary_accuracy: 0.8523 - mlp_auc_63: 0.9177\n",
      "Epoch 53/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3336 - siamese_loss: 5.2871 - mlp_loss: 0.3336 - mlp_binary_accuracy: 0.8656 - mlp_auc_63: 0.9232\n",
      "Epoch 54/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3456 - siamese_loss: 5.1520 - mlp_loss: 0.3456 - mlp_binary_accuracy: 0.8578 - mlp_auc_63: 0.9197\n",
      "Epoch 55/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3500 - siamese_loss: 5.3366 - mlp_loss: 0.3500 - mlp_binary_accuracy: 0.8543 - mlp_auc_63: 0.9175\n",
      "Epoch 56/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3314 - siamese_loss: 5.3525 - mlp_loss: 0.3314 - mlp_binary_accuracy: 0.8682 - mlp_auc_63: 0.9263\n",
      "Epoch 57/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3194 - siamese_loss: 5.2927 - mlp_loss: 0.3194 - mlp_binary_accuracy: 0.8691 - mlp_auc_63: 0.9281\n",
      "Epoch 58/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3284 - siamese_loss: 5.3654 - mlp_loss: 0.3284 - mlp_binary_accuracy: 0.8659 - mlp_auc_63: 0.9265\n",
      "Epoch 59/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3322 - siamese_loss: 5.4924 - mlp_loss: 0.3322 - mlp_binary_accuracy: 0.8659 - mlp_auc_63: 0.9225\n",
      "Epoch 60/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3226 - siamese_loss: 5.2904 - mlp_loss: 0.3226 - mlp_binary_accuracy: 0.8679 - mlp_auc_63: 0.9281\n",
      "Epoch 61/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3205 - siamese_loss: 5.2627 - mlp_loss: 0.3205 - mlp_binary_accuracy: 0.8733 - mlp_auc_63: 0.9258\n",
      "Epoch 62/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3166 - siamese_loss: 5.3345 - mlp_loss: 0.3166 - mlp_binary_accuracy: 0.8766 - mlp_auc_63: 0.9281\n",
      "Epoch 63/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3146 - siamese_loss: 5.3203 - mlp_loss: 0.3146 - mlp_binary_accuracy: 0.8698 - mlp_auc_63: 0.9309\n",
      "Epoch 64/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.3155 - siamese_loss: 5.3705 - mlp_loss: 0.3155 - mlp_binary_accuracy: 0.8763 - mlp_auc_63: 0.9278\n",
      "Epoch 65/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3057 - siamese_loss: 5.3996 - mlp_loss: 0.3057 - mlp_binary_accuracy: 0.8834 - mlp_auc_63: 0.9364\n",
      "Epoch 66/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3107 - siamese_loss: 5.3895 - mlp_loss: 0.3107 - mlp_binary_accuracy: 0.8746 - mlp_auc_63: 0.9303\n",
      "Epoch 67/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3047 - siamese_loss: 5.6684 - mlp_loss: 0.3047 - mlp_binary_accuracy: 0.8779 - mlp_auc_63: 0.9328\n",
      "Epoch 68/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2952 - siamese_loss: 5.7234 - mlp_loss: 0.2952 - mlp_binary_accuracy: 0.8821 - mlp_auc_63: 0.9387\n",
      "Epoch 69/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2939 - siamese_loss: 5.5483 - mlp_loss: 0.2939 - mlp_binary_accuracy: 0.8798 - mlp_auc_63: 0.9385\n",
      "Epoch 70/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2892 - siamese_loss: 5.5750 - mlp_loss: 0.2892 - mlp_binary_accuracy: 0.8856 - mlp_auc_63: 0.9385\n",
      "Epoch 71/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2874 - siamese_loss: 5.5250 - mlp_loss: 0.2874 - mlp_binary_accuracy: 0.8872 - mlp_auc_63: 0.9408\n",
      "Epoch 72/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2714 - siamese_loss: 5.5077 - mlp_loss: 0.2714 - mlp_binary_accuracy: 0.8924 - mlp_auc_63: 0.9461\n",
      "Epoch 73/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2752 - siamese_loss: 5.4233 - mlp_loss: 0.2752 - mlp_binary_accuracy: 0.8918 - mlp_auc_63: 0.9435\n",
      "Epoch 74/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2733 - siamese_loss: 5.5747 - mlp_loss: 0.2733 - mlp_binary_accuracy: 0.8982 - mlp_auc_63: 0.9452\n",
      "Epoch 75/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2766 - siamese_loss: 5.6726 - mlp_loss: 0.2766 - mlp_binary_accuracy: 0.8927 - mlp_auc_63: 0.9424\n",
      "Epoch 76/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2751 - siamese_loss: 5.6476 - mlp_loss: 0.2751 - mlp_binary_accuracy: 0.8947 - mlp_auc_63: 0.9462\n",
      "Epoch 77/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2685 - siamese_loss: 5.7624 - mlp_loss: 0.2685 - mlp_binary_accuracy: 0.9018 - mlp_auc_63: 0.9493\n",
      "Epoch 78/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2635 - siamese_loss: 5.7537 - mlp_loss: 0.2635 - mlp_binary_accuracy: 0.9005 - mlp_auc_63: 0.9490\n",
      "Epoch 79/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2539 - siamese_loss: 5.7597 - mlp_loss: 0.2539 - mlp_binary_accuracy: 0.9005 - mlp_auc_63: 0.9524\n",
      "Epoch 80/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2560 - siamese_loss: 5.6651 - mlp_loss: 0.2560 - mlp_binary_accuracy: 0.9008 - mlp_auc_63: 0.9518\n",
      "Epoch 81/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2521 - siamese_loss: 5.6496 - mlp_loss: 0.2521 - mlp_binary_accuracy: 0.9037 - mlp_auc_63: 0.9515\n",
      "Epoch 82/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2438 - siamese_loss: 5.6267 - mlp_loss: 0.2438 - mlp_binary_accuracy: 0.9121 - mlp_auc_63: 0.9552\n",
      "Epoch 83/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2476 - siamese_loss: 5.6572 - mlp_loss: 0.2476 - mlp_binary_accuracy: 0.9073 - mlp_auc_63: 0.9533\n",
      "Epoch 84/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2542 - siamese_loss: 5.6165 - mlp_loss: 0.2542 - mlp_binary_accuracy: 0.9040 - mlp_auc_63: 0.9520: 0s - loss: 0.2394 - siamese_loss: 5.7299 - mlp_loss: 0.2394 - mlp_binary_accuracy: 0.9056 - mlp_auc_63: \n",
      "Epoch 85/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2531 - siamese_loss: 5.6832 - mlp_loss: 0.2531 - mlp_binary_accuracy: 0.9021 - mlp_auc_63: 0.9541\n",
      "Epoch 86/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2461 - siamese_loss: 5.6301 - mlp_loss: 0.2461 - mlp_binary_accuracy: 0.9063 - mlp_auc_63: 0.9561\n",
      "Epoch 87/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2376 - siamese_loss: 5.5448 - mlp_loss: 0.2376 - mlp_binary_accuracy: 0.9111 - mlp_auc_63: 0.9574\n",
      "Epoch 88/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2404 - siamese_loss: 5.5351 - mlp_loss: 0.2404 - mlp_binary_accuracy: 0.9082 - mlp_auc_63: 0.9584\n",
      "Epoch 89/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2379 - siamese_loss: 5.6148 - mlp_loss: 0.2379 - mlp_binary_accuracy: 0.9082 - mlp_auc_63: 0.9603\n",
      "Epoch 90/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2306 - siamese_loss: 5.5951 - mlp_loss: 0.2306 - mlp_binary_accuracy: 0.9166 - mlp_auc_63: 0.9605\n",
      "Epoch 91/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2282 - siamese_loss: 5.6978 - mlp_loss: 0.2282 - mlp_binary_accuracy: 0.9212 - mlp_auc_63: 0.9612\n",
      "Epoch 92/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2273 - siamese_loss: 5.7214 - mlp_loss: 0.2273 - mlp_binary_accuracy: 0.9221 - mlp_auc_63: 0.9624\n",
      "Epoch 93/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2178 - siamese_loss: 5.5876 - mlp_loss: 0.2178 - mlp_binary_accuracy: 0.9192 - mlp_auc_63: 0.9646\n",
      "Epoch 94/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2176 - siamese_loss: 5.5149 - mlp_loss: 0.2176 - mlp_binary_accuracy: 0.9183 - mlp_auc_63: 0.9646\n",
      "Epoch 95/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2133 - siamese_loss: 5.5645 - mlp_loss: 0.2133 - mlp_binary_accuracy: 0.9212 - mlp_auc_63: 0.9657\n",
      "Epoch 96/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2216 - siamese_loss: 5.6405 - mlp_loss: 0.2216 - mlp_binary_accuracy: 0.9215 - mlp_auc_63: 0.9644\n",
      "Epoch 97/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2310 - siamese_loss: 5.6150 - mlp_loss: 0.2310 - mlp_binary_accuracy: 0.9131 - mlp_auc_63: 0.9620\n",
      "Epoch 98/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2076 - siamese_loss: 5.6323 - mlp_loss: 0.2076 - mlp_binary_accuracy: 0.9244 - mlp_auc_63: 0.9674\n",
      "Epoch 99/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2021 - siamese_loss: 5.6544 - mlp_loss: 0.2021 - mlp_binary_accuracy: 0.9257 - mlp_auc_63: 0.9715\n",
      "Epoch 100/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2184 - siamese_loss: 5.7641 - mlp_loss: 0.2184 - mlp_binary_accuracy: 0.9225 - mlp_auc_63: 0.9658\n",
      "Epoch 101/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2062 - siamese_loss: 5.6932 - mlp_loss: 0.2062 - mlp_binary_accuracy: 0.9247 - mlp_auc_63: 0.9684\n",
      "Epoch 102/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2182 - siamese_loss: 5.6619 - mlp_loss: 0.2182 - mlp_binary_accuracy: 0.9241 - mlp_auc_63: 0.9663\n",
      "Epoch 103/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2039 - siamese_loss: 5.5990 - mlp_loss: 0.2039 - mlp_binary_accuracy: 0.9254 - mlp_auc_63: 0.9686\n",
      "Epoch 104/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2003 - siamese_loss: 5.6224 - mlp_loss: 0.2003 - mlp_binary_accuracy: 0.9305 - mlp_auc_63: 0.9691\n",
      "Epoch 105/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1972 - siamese_loss: 5.7937 - mlp_loss: 0.1972 - mlp_binary_accuracy: 0.9276 - mlp_auc_63: 0.9705\n",
      "Epoch 106/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1910 - siamese_loss: 5.7393 - mlp_loss: 0.1910 - mlp_binary_accuracy: 0.9318 - mlp_auc_63: 0.9730\n",
      "Epoch 107/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2116 - siamese_loss: 5.6661 - mlp_loss: 0.2116 - mlp_binary_accuracy: 0.9212 - mlp_auc_63: 0.9680\n",
      "Epoch 108/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1921 - siamese_loss: 5.7458 - mlp_loss: 0.1921 - mlp_binary_accuracy: 0.9325 - mlp_auc_63: 0.9728\n",
      "Epoch 109/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1929 - siamese_loss: 5.7631 - mlp_loss: 0.1929 - mlp_binary_accuracy: 0.9315 - mlp_auc_63: 0.9718\n",
      "Epoch 110/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1943 - siamese_loss: 5.6672 - mlp_loss: 0.1943 - mlp_binary_accuracy: 0.9273 - mlp_auc_63: 0.9723\n",
      "Epoch 111/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1780 - siamese_loss: 5.6997 - mlp_loss: 0.1780 - mlp_binary_accuracy: 0.9347 - mlp_auc_63: 0.9758\n",
      "Epoch 112/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1812 - siamese_loss: 5.5760 - mlp_loss: 0.1812 - mlp_binary_accuracy: 0.9357 - mlp_auc_63: 0.9755\n",
      "Epoch 113/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1863 - siamese_loss: 5.6694 - mlp_loss: 0.1863 - mlp_binary_accuracy: 0.9325 - mlp_auc_63: 0.9750\n",
      "Epoch 114/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1813 - siamese_loss: 5.6479 - mlp_loss: 0.1813 - mlp_binary_accuracy: 0.9344 - mlp_auc_63: 0.9774\n",
      "Epoch 115/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1677 - siamese_loss: 5.7912 - mlp_loss: 0.1677 - mlp_binary_accuracy: 0.9367 - mlp_auc_63: 0.9797\n",
      "Epoch 116/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1924 - siamese_loss: 5.8882 - mlp_loss: 0.1924 - mlp_binary_accuracy: 0.9299 - mlp_auc_63: 0.9755\n",
      "Epoch 117/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1676 - siamese_loss: 5.8092 - mlp_loss: 0.1676 - mlp_binary_accuracy: 0.9393 - mlp_auc_63: 0.9797\n",
      "Epoch 118/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1773 - siamese_loss: 5.7347 - mlp_loss: 0.1773 - mlp_binary_accuracy: 0.9373 - mlp_auc_63: 0.9789\n",
      "Epoch 119/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1864 - siamese_loss: 5.7670 - mlp_loss: 0.1864 - mlp_binary_accuracy: 0.9373 - mlp_auc_63: 0.9757\n",
      "Epoch 120/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1789 - siamese_loss: 5.7081 - mlp_loss: 0.1789 - mlp_binary_accuracy: 0.9351 - mlp_auc_63: 0.9762\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1673 - siamese_loss: 5.6689 - mlp_loss: 0.1673 - mlp_binary_accuracy: 0.9399 - mlp_auc_63: 0.9801\n",
      "Epoch 122/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1741 - siamese_loss: 5.7036 - mlp_loss: 0.1741 - mlp_binary_accuracy: 0.9357 - mlp_auc_63: 0.9782\n",
      "Epoch 123/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1675 - siamese_loss: 5.7149 - mlp_loss: 0.1675 - mlp_binary_accuracy: 0.9409 - mlp_auc_63: 0.9789\n",
      "Epoch 124/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1702 - siamese_loss: 5.6010 - mlp_loss: 0.1702 - mlp_binary_accuracy: 0.9409 - mlp_auc_63: 0.9797\n",
      "Epoch 125/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1653 - siamese_loss: 5.7187 - mlp_loss: 0.1653 - mlp_binary_accuracy: 0.9425 - mlp_auc_63: 0.9805\n",
      "Epoch 126/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1686 - siamese_loss: 5.6833 - mlp_loss: 0.1686 - mlp_binary_accuracy: 0.9435 - mlp_auc_63: 0.9812\n",
      "Epoch 127/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1632 - siamese_loss: 5.6065 - mlp_loss: 0.1632 - mlp_binary_accuracy: 0.9405 - mlp_auc_63: 0.9798\n",
      "Epoch 128/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1620 - siamese_loss: 5.6090 - mlp_loss: 0.1620 - mlp_binary_accuracy: 0.9418 - mlp_auc_63: 0.9798\n",
      "Epoch 129/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1538 - siamese_loss: 5.6950 - mlp_loss: 0.1538 - mlp_binary_accuracy: 0.9460 - mlp_auc_63: 0.9825\n",
      "Epoch 130/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1571 - siamese_loss: 5.7136 - mlp_loss: 0.1571 - mlp_binary_accuracy: 0.9451 - mlp_auc_63: 0.9824\n",
      "Epoch 131/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1490 - siamese_loss: 5.6850 - mlp_loss: 0.1490 - mlp_binary_accuracy: 0.9489 - mlp_auc_63: 0.9833\n",
      "Epoch 132/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1538 - siamese_loss: 5.6332 - mlp_loss: 0.1538 - mlp_binary_accuracy: 0.9451 - mlp_auc_63: 0.9834\n",
      "Epoch 133/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1408 - siamese_loss: 5.7726 - mlp_loss: 0.1408 - mlp_binary_accuracy: 0.9502 - mlp_auc_63: 0.9856\n",
      "Epoch 134/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1525 - siamese_loss: 5.8147 - mlp_loss: 0.1525 - mlp_binary_accuracy: 0.9473 - mlp_auc_63: 0.9835\n",
      "Epoch 135/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1444 - siamese_loss: 5.7965 - mlp_loss: 0.1444 - mlp_binary_accuracy: 0.9464 - mlp_auc_63: 0.9845\n",
      "Epoch 136/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1444 - siamese_loss: 5.7643 - mlp_loss: 0.1444 - mlp_binary_accuracy: 0.9528 - mlp_auc_63: 0.9842\n",
      "Epoch 137/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1506 - siamese_loss: 5.8848 - mlp_loss: 0.1506 - mlp_binary_accuracy: 0.9435 - mlp_auc_63: 0.9851\n",
      "Epoch 138/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1355 - siamese_loss: 5.7215 - mlp_loss: 0.1355 - mlp_binary_accuracy: 0.9522 - mlp_auc_63: 0.9863\n",
      "Epoch 139/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1466 - siamese_loss: 5.8201 - mlp_loss: 0.1466 - mlp_binary_accuracy: 0.9532 - mlp_auc_63: 0.9836\n",
      "Epoch 140/200\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1552 - siamese_loss: 5.8132 - mlp_loss: 0.1552 - mlp_binary_accuracy: 0.9422 - mlp_auc_63: 0.9826\n",
      "Epoch 141/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1499 - siamese_loss: 5.7788 - mlp_loss: 0.1499 - mlp_binary_accuracy: 0.9457 - mlp_auc_63: 0.9848\n",
      "Epoch 142/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1469 - siamese_loss: 5.7517 - mlp_loss: 0.1469 - mlp_binary_accuracy: 0.9480 - mlp_auc_63: 0.9854\n",
      "Epoch 143/200\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1431 - siamese_loss: 5.7478 - mlp_loss: 0.1431 - mlp_binary_accuracy: 0.9499 - mlp_auc_63: 0.9849\n",
      "25/25 [==============================] - 0s 899us/step - loss: 0.9362 - siamese_loss: 5.8007 - mlp_loss: 0.9362 - mlp_binary_accuracy: 0.7516 - mlp_auc_63: 0.8169\n",
      "------ mlp_binary_accuracy: 75.16%\t ----- mlp_auc_63: 81.69%\n",
      "五折性能均值：-------- ave_acc: 76.65% (+/- 2.13%)\t ----- ave_auc_mlp:84.02% (+/- 1.70%)\n"
     ]
    }
   ],
   "source": [
    "cvacc,cvauc_mlp,cvpre,cvpre_top,cvrec,cvauc_sia=[],[],[],[],[],[]\n",
    "seed=1\n",
    "i =1\n",
    "\n",
    "Kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\n",
    "for train,test in Kfold.split(X=symA_train,y=y_train):\n",
    "    ###############################################################################################\n",
    "\n",
    "    input_shape=(322)\n",
    "    base_network = create_base_network(input_shape)\n",
    "    \n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "    input_mesh_a = Input(shape=24)\n",
    "    input_mesh_b = Input(shape=24)\n",
    "    \n",
    "    # because we re-use the same instance `base_network`,the weights of the network will be shared across the two branches\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    \n",
    "\n",
    "    eucl_model = create_euclLayer(processed_a.shape[-1])\n",
    "    eucl_out = eucl_model([processed_a,processed_b]) \n",
    "    \n",
    "    \n",
    "    concat_fea = tf.keras.layers.concatenate([processed_a,input_mesh_a,processed_b,input_mesh_b],axis=-1)\n",
    "    print(\"concat_fea的shape：\",concat_fea.shape) #(None,32)\n",
    "    MLP = create_MLP(concat_fea.shape[-1])\n",
    "    MLP_out = MLP(concat_fea)\n",
    "    \n",
    "    model = Model(inputs=[input_a, input_b,input_mesh_a,input_mesh_b], outputs=[eucl_out,MLP_out])\n",
    "    \n",
    "    \"\"\"L2\"\"\"\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer,'kernel_regularizer'):\n",
    "            layer.kernel_regularizer = tf.keras.regularizers.l2(0.001)\n",
    "        \n",
    "    #####################################################################################################################\n",
    "\n",
    "    print(\"-------------------------------------Kfold: {} iter-----------------------------------------\".format(i))\n",
    "    i+=1\n",
    "\n",
    "    rms = RMSprop()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=5)\n",
    "    \n",
    "    my_loss = {\"siamese\":contrastive_loss,\"mlp\":tf.keras.losses.BinaryCrossentropy()}\n",
    "    my_loss_weight = {\"siamese\":0,\"mlp\":1}\n",
    "    my_metrics ={ \"mlp\":[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()]} #tf.keras.metrics.Precision(top_k=len(rus_y)//100)\n",
    "    \n",
    "    model.compile(\n",
    "#         loss= [tfa.losses.ContrastiveLoss(),tf.keras.losses.BinaryCrossentropy()],\n",
    "#         loss_weights=[0.01,1],\n",
    "#         metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()],\n",
    "        \n",
    "        loss= my_loss,\n",
    "        loss_weights=my_loss_weight,\n",
    "        metrics=my_metrics,\n",
    "        \n",
    "        optimizer=rms,\n",
    "    )\n",
    "    \n",
    "    # fit()中 shuffle=True\n",
    "    model.fit([symA_train[train],symB_train[train],meshA_train[train],meshB_train[train]],\n",
    "              [y_train[train],y_train[train]],\n",
    "              \n",
    "              batch_size=None,\n",
    "              epochs=200,\n",
    "              callbacks=callback,\n",
    "              shuffle=True,\n",
    "             )\n",
    "    \n",
    "    scores = model.evaluate([symA_train[test],symB_train[test],meshA_train[test],meshB_train[test]],\n",
    "                            [y_train[test],y_train[test]],\n",
    "                            verbose=1,\n",
    "                            batch_size=32,\n",
    "                            callbacks=callback)\n",
    "    \n",
    "    cvacc.append(scores[-2] * 100)\n",
    "    cvauc_mlp.append(scores[-1] * 100)\n",
    "    cvauc_sia.append(scores[-3] * 100)\n",
    "    \n",
    "    print(\"------ %s: %.2f%%\\t ----- %s: %.2f%%\" % \n",
    "           (model.metrics_names[-2],scores[-2]*100, model.metrics_names[-1],scores[-1]*100))\n",
    "     \n",
    "print(\"五折性能均值：-------- ave_acc: %.2f%% (+/- %.2f%%)\\t ----- ave_auc_mlp:%.2f%% (+/- %.2f%%)\" % \n",
    "      (np.mean(cvacc), np.std(cvacc),np.mean(cvauc_mlp), np.std(cvauc_mlp)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "五折性能均值：-------- ave_acc: 76.65% (+/- 2.13%)\t ----- ave_auc_mlp:84.02% (+/- 1.70%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test3:only Mesh (siamese+mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_siamese_test(input_shape):\n",
    "    '''Base network to be shared (eq. to feature extraction).'''\n",
    "    input = Input(shape=input_shape)\n",
    "    \n",
    "    x= Activation(activation='relu')(Dense(48)(input))\n",
    "    x= Activation(activation='relu')(Dense(32)(x))\n",
    "    x= Activation(activation='relu')(Dense(16)(x))\n",
    "    x= Activation(activation='sigmoid')(Dense(8)(x))\n",
    "    \n",
    "    return Model(input, x,name=\"siamese\")\n",
    "\n",
    "def create_MLP_test(input_shape):\n",
    "    \n",
    "    input = Input(shape=input_shape)\n",
    "    x= Activation(activation='relu')(Dense(16)(input))\n",
    "    \n",
    "    x= Activation(activation='relu')(Dense(8)(x))\n",
    "    \n",
    "    x= Dense(1,activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(input, x ,name=\"mlp_test\")\n",
    "\n",
    "def create_euclLayer_test(input_shape):\n",
    "    pair1 = Input(shape=(input_shape))\n",
    "    pair2 = Input(shape=(input_shape))\n",
    "              \n",
    "    x = euclLayer()(pair1,pair2)\n",
    "    x = Activation(activation='relu')(x)\n",
    "#     x = Activation(activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(inputs=[pair1,pair2],outputs=x,name=\"siamese_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus_y的长度: 4836\n",
      "concat_fea的shape： (None, 16)\n",
      "-------------------------------------Kfold: 1 iter-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'PYTHONHASHSEED' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6866 - siamese_test_loss: 0.4158 - mlp_test_loss: 0.6866 - mlp_test_binary_accuracy: 0.5338 - mlp_test_auc_14: 0.5785\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6726 - siamese_test_loss: 0.3599 - mlp_test_loss: 0.6726 - mlp_test_binary_accuracy: 0.5864 - mlp_test_auc_14: 0.6383\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6610 - siamese_test_loss: 0.3408 - mlp_test_loss: 0.6610 - mlp_test_binary_accuracy: 0.6103 - mlp_test_auc_14: 0.6612\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6498 - siamese_test_loss: 0.3408 - mlp_test_loss: 0.6498 - mlp_test_binary_accuracy: 0.6326 - mlp_test_auc_14: 0.6730\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6399 - siamese_test_loss: 0.3397 - mlp_test_loss: 0.6399 - mlp_test_binary_accuracy: 0.6448 - mlp_test_auc_14: 0.6940\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6311 - siamese_test_loss: 0.3420 - mlp_test_loss: 0.6311 - mlp_test_binary_accuracy: 0.6503 - mlp_test_auc_14: 0.7062\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6230 - siamese_test_loss: 0.3459 - mlp_test_loss: 0.6230 - mlp_test_binary_accuracy: 0.6579 - mlp_test_auc_14: 0.7103\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6158 - siamese_test_loss: 0.3451 - mlp_test_loss: 0.6158 - mlp_test_binary_accuracy: 0.6677 - mlp_test_auc_14: 0.7219\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6100 - siamese_test_loss: 0.3506 - mlp_test_loss: 0.6100 - mlp_test_binary_accuracy: 0.6785 - mlp_test_auc_14: 0.7258\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6073 - siamese_test_loss: 0.3514 - mlp_test_loss: 0.6073 - mlp_test_binary_accuracy: 0.6703 - mlp_test_auc_14: 0.7257\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6041 - siamese_test_loss: 0.3511 - mlp_test_loss: 0.6041 - mlp_test_binary_accuracy: 0.6744 - mlp_test_auc_14: 0.7332\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5995 - siamese_test_loss: 0.3521 - mlp_test_loss: 0.5995 - mlp_test_binary_accuracy: 0.6813 - mlp_test_auc_14: 0.7365\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5969 - siamese_test_loss: 0.3551 - mlp_test_loss: 0.5969 - mlp_test_binary_accuracy: 0.6801 - mlp_test_auc_14: 0.7396\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5943 - siamese_test_loss: 0.3544 - mlp_test_loss: 0.5943 - mlp_test_binary_accuracy: 0.6818 - mlp_test_auc_14: 0.7423\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5928 - siamese_test_loss: 0.3567 - mlp_test_loss: 0.5928 - mlp_test_binary_accuracy: 0.6866 - mlp_test_auc_14: 0.7429\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5916 - siamese_test_loss: 0.3564 - mlp_test_loss: 0.5916 - mlp_test_binary_accuracy: 0.6820 - mlp_test_auc_14: 0.7449\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5908 - siamese_test_loss: 0.3562 - mlp_test_loss: 0.5908 - mlp_test_binary_accuracy: 0.6836 - mlp_test_auc_14: 0.7458\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5892 - siamese_test_loss: 0.3579 - mlp_test_loss: 0.5892 - mlp_test_binary_accuracy: 0.6843 - mlp_test_auc_14: 0.7475\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5892 - siamese_test_loss: 0.3558 - mlp_test_loss: 0.5892 - mlp_test_binary_accuracy: 0.6896 - mlp_test_auc_14: 0.7479\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5862 - siamese_test_loss: 0.3571 - mlp_test_loss: 0.5862 - mlp_test_binary_accuracy: 0.6850 - mlp_test_auc_14: 0.7512\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5860 - siamese_test_loss: 0.3590 - mlp_test_loss: 0.5860 - mlp_test_binary_accuracy: 0.6891 - mlp_test_auc_14: 0.7524\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5869 - siamese_test_loss: 0.3581 - mlp_test_loss: 0.5869 - mlp_test_binary_accuracy: 0.6859 - mlp_test_auc_14: 0.7514\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5848 - siamese_test_loss: 0.3611 - mlp_test_loss: 0.5848 - mlp_test_binary_accuracy: 0.6854 - mlp_test_auc_14: 0.7538\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5847 - siamese_test_loss: 0.3591 - mlp_test_loss: 0.5847 - mlp_test_binary_accuracy: 0.6857 - mlp_test_auc_14: 0.7531\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5843 - siamese_test_loss: 0.3582 - mlp_test_loss: 0.5843 - mlp_test_binary_accuracy: 0.6847 - mlp_test_auc_14: 0.7544\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5835 - siamese_test_loss: 0.3596 - mlp_test_loss: 0.5835 - mlp_test_binary_accuracy: 0.6898 - mlp_test_auc_14: 0.7563\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5820 - siamese_test_loss: 0.3569 - mlp_test_loss: 0.5820 - mlp_test_binary_accuracy: 0.6873 - mlp_test_auc_14: 0.7568\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5819 - siamese_test_loss: 0.3601 - mlp_test_loss: 0.5819 - mlp_test_binary_accuracy: 0.6866 - mlp_test_auc_14: 0.7561\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5812 - siamese_test_loss: 0.3581 - mlp_test_loss: 0.5812 - mlp_test_binary_accuracy: 0.6900 - mlp_test_auc_14: 0.7580\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5806 - siamese_test_loss: 0.3590 - mlp_test_loss: 0.5806 - mlp_test_binary_accuracy: 0.6882 - mlp_test_auc_14: 0.7586\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5801 - siamese_test_loss: 0.3628 - mlp_test_loss: 0.5801 - mlp_test_binary_accuracy: 0.6896 - mlp_test_auc_14: 0.7592\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5799 - siamese_test_loss: 0.3615 - mlp_test_loss: 0.5799 - mlp_test_binary_accuracy: 0.6921 - mlp_test_auc_14: 0.7582\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5778 - siamese_test_loss: 0.3628 - mlp_test_loss: 0.5778 - mlp_test_binary_accuracy: 0.6939 - mlp_test_auc_14: 0.7615\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5767 - siamese_test_loss: 0.3631 - mlp_test_loss: 0.5767 - mlp_test_binary_accuracy: 0.6944 - mlp_test_auc_14: 0.7629\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5757 - siamese_test_loss: 0.3640 - mlp_test_loss: 0.5757 - mlp_test_binary_accuracy: 0.6919 - mlp_test_auc_14: 0.7638\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5766 - siamese_test_loss: 0.3625 - mlp_test_loss: 0.5766 - mlp_test_binary_accuracy: 0.6951 - mlp_test_auc_14: 0.7625\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5753 - siamese_test_loss: 0.3649 - mlp_test_loss: 0.5753 - mlp_test_binary_accuracy: 0.6893 - mlp_test_auc_14: 0.7639\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5751 - siamese_test_loss: 0.3623 - mlp_test_loss: 0.5751 - mlp_test_binary_accuracy: 0.6889 - mlp_test_auc_14: 0.7639\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5752 - siamese_test_loss: 0.3680 - mlp_test_loss: 0.5752 - mlp_test_binary_accuracy: 0.6926 - mlp_test_auc_14: 0.7638\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5738 - siamese_test_loss: 0.3661 - mlp_test_loss: 0.5738 - mlp_test_binary_accuracy: 0.6978 - mlp_test_auc_14: 0.7652\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 934us/step - loss: 0.5725 - siamese_test_loss: 0.3679 - mlp_test_loss: 0.5725 - mlp_test_binary_accuracy: 0.6951 - mlp_test_auc_14: 0.7666\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5735 - siamese_test_loss: 0.3650 - mlp_test_loss: 0.5735 - mlp_test_binary_accuracy: 0.6960 - mlp_test_auc_14: 0.7653\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5734 - siamese_test_loss: 0.3684 - mlp_test_loss: 0.5734 - mlp_test_binary_accuracy: 0.6889 - mlp_test_auc_14: 0.7654\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5729 - siamese_test_loss: 0.3694 - mlp_test_loss: 0.5729 - mlp_test_binary_accuracy: 0.6916 - mlp_test_auc_14: 0.7648\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5706 - siamese_test_loss: 0.3699 - mlp_test_loss: 0.5706 - mlp_test_binary_accuracy: 0.6990 - mlp_test_auc_14: 0.7695\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 934us/step - loss: 0.5722 - siamese_test_loss: 0.3692 - mlp_test_loss: 0.5722 - mlp_test_binary_accuracy: 0.6946 - mlp_test_auc_14: 0.7663\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 929us/step - loss: 0.5708 - siamese_test_loss: 0.3738 - mlp_test_loss: 0.5708 - mlp_test_binary_accuracy: 0.6923 - mlp_test_auc_14: 0.7676\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5707 - siamese_test_loss: 0.3774 - mlp_test_loss: 0.5707 - mlp_test_binary_accuracy: 0.6900 - mlp_test_auc_14: 0.7681\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5708 - siamese_test_loss: 0.3712 - mlp_test_loss: 0.5708 - mlp_test_binary_accuracy: 0.6921 - mlp_test_auc_14: 0.7675\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - 0s 949us/step - loss: 0.5696 - siamese_test_loss: 0.3762 - mlp_test_loss: 0.5696 - mlp_test_binary_accuracy: 0.6992 - mlp_test_auc_14: 0.7693\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5696 - siamese_test_loss: 0.3762 - mlp_test_loss: 0.5696 - mlp_test_binary_accuracy: 0.6997 - mlp_test_auc_14: 0.7684\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5685 - siamese_test_loss: 0.3789 - mlp_test_loss: 0.5685 - mlp_test_binary_accuracy: 0.6859 - mlp_test_auc_14: 0.7700\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5680 - siamese_test_loss: 0.3798 - mlp_test_loss: 0.5680 - mlp_test_binary_accuracy: 0.6905 - mlp_test_auc_14: 0.7701\n",
      "Epoch 54/200\n",
      "136/136 [==============================] - 0s 984us/step - loss: 0.5687 - siamese_test_loss: 0.3769 - mlp_test_loss: 0.5687 - mlp_test_binary_accuracy: 0.6937 - mlp_test_auc_14: 0.7702\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5674 - siamese_test_loss: 0.3819 - mlp_test_loss: 0.5674 - mlp_test_binary_accuracy: 0.6972 - mlp_test_auc_14: 0.7708\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - 0s 961us/step - loss: 0.5655 - siamese_test_loss: 0.3796 - mlp_test_loss: 0.5655 - mlp_test_binary_accuracy: 0.7001 - mlp_test_auc_14: 0.7722\n",
      "Epoch 57/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5667 - siamese_test_loss: 0.3789 - mlp_test_loss: 0.5667 - mlp_test_binary_accuracy: 0.6916 - mlp_test_auc_14: 0.7710\n",
      "Epoch 58/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5652 - siamese_test_loss: 0.3743 - mlp_test_loss: 0.5652 - mlp_test_binary_accuracy: 0.6976 - mlp_test_auc_14: 0.7735\n",
      "Epoch 59/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5665 - siamese_test_loss: 0.3802 - mlp_test_loss: 0.5665 - mlp_test_binary_accuracy: 0.6928 - mlp_test_auc_14: 0.7712\n",
      "Epoch 60/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5649 - siamese_test_loss: 0.3820 - mlp_test_loss: 0.5649 - mlp_test_binary_accuracy: 0.6997 - mlp_test_auc_14: 0.7731\n",
      "Epoch 61/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5655 - siamese_test_loss: 0.3842 - mlp_test_loss: 0.5655 - mlp_test_binary_accuracy: 0.6962 - mlp_test_auc_14: 0.7730\n",
      "Epoch 62/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5644 - siamese_test_loss: 0.3851 - mlp_test_loss: 0.5644 - mlp_test_binary_accuracy: 0.6955 - mlp_test_auc_14: 0.7734\n",
      "Epoch 63/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5654 - siamese_test_loss: 0.3883 - mlp_test_loss: 0.5654 - mlp_test_binary_accuracy: 0.6944 - mlp_test_auc_14: 0.7719\n",
      "Epoch 64/200\n",
      "136/136 [==============================] - 0s 1000us/step - loss: 0.5636 - siamese_test_loss: 0.3875 - mlp_test_loss: 0.5636 - mlp_test_binary_accuracy: 0.6974 - mlp_test_auc_14: 0.7748\n",
      "Epoch 65/200\n",
      "136/136 [==============================] - 0s 914us/step - loss: 0.5630 - siamese_test_loss: 0.3836 - mlp_test_loss: 0.5630 - mlp_test_binary_accuracy: 0.7008 - mlp_test_auc_14: 0.7747\n",
      "Epoch 66/200\n",
      "136/136 [==============================] - 0s 912us/step - loss: 0.5635 - siamese_test_loss: 0.3835 - mlp_test_loss: 0.5635 - mlp_test_binary_accuracy: 0.6965 - mlp_test_auc_14: 0.7747\n",
      "Epoch 67/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5644 - siamese_test_loss: 0.3785 - mlp_test_loss: 0.5644 - mlp_test_binary_accuracy: 0.6946 - mlp_test_auc_14: 0.7726\n",
      "Epoch 68/200\n",
      "136/136 [==============================] - 0s 921us/step - loss: 0.5625 - siamese_test_loss: 0.3823 - mlp_test_loss: 0.5625 - mlp_test_binary_accuracy: 0.6981 - mlp_test_auc_14: 0.7756\n",
      "Epoch 69/200\n",
      "136/136 [==============================] - 0s 917us/step - loss: 0.5617 - siamese_test_loss: 0.3839 - mlp_test_loss: 0.5617 - mlp_test_binary_accuracy: 0.7001 - mlp_test_auc_14: 0.7757\n",
      "Epoch 70/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5621 - siamese_test_loss: 0.3892 - mlp_test_loss: 0.5621 - mlp_test_binary_accuracy: 0.7059 - mlp_test_auc_14: 0.7755\n",
      "Epoch 71/200\n",
      "136/136 [==============================] - 0s 911us/step - loss: 0.5612 - siamese_test_loss: 0.3862 - mlp_test_loss: 0.5612 - mlp_test_binary_accuracy: 0.6974 - mlp_test_auc_14: 0.7756\n",
      "Epoch 72/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5612 - siamese_test_loss: 0.3832 - mlp_test_loss: 0.5612 - mlp_test_binary_accuracy: 0.7001 - mlp_test_auc_14: 0.7768\n",
      "Epoch 73/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5601 - siamese_test_loss: 0.3882 - mlp_test_loss: 0.5601 - mlp_test_binary_accuracy: 0.7043 - mlp_test_auc_14: 0.7784\n",
      "Epoch 74/200\n",
      "136/136 [==============================] - 0s 916us/step - loss: 0.5615 - siamese_test_loss: 0.3877 - mlp_test_loss: 0.5615 - mlp_test_binary_accuracy: 0.6965 - mlp_test_auc_14: 0.7765\n",
      "Epoch 75/200\n",
      "136/136 [==============================] - 0s 928us/step - loss: 0.5615 - siamese_test_loss: 0.3919 - mlp_test_loss: 0.5615 - mlp_test_binary_accuracy: 0.6958 - mlp_test_auc_14: 0.7756\n",
      "Epoch 76/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5608 - siamese_test_loss: 0.3937 - mlp_test_loss: 0.5608 - mlp_test_binary_accuracy: 0.6928 - mlp_test_auc_14: 0.7761\n",
      "Epoch 77/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5590 - siamese_test_loss: 0.3946 - mlp_test_loss: 0.5590 - mlp_test_binary_accuracy: 0.7013 - mlp_test_auc_14: 0.7793\n",
      "Epoch 78/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5603 - siamese_test_loss: 0.3862 - mlp_test_loss: 0.5603 - mlp_test_binary_accuracy: 0.6967 - mlp_test_auc_14: 0.7770\n",
      "Epoch 79/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5590 - siamese_test_loss: 0.3898 - mlp_test_loss: 0.5590 - mlp_test_binary_accuracy: 0.7034 - mlp_test_auc_14: 0.7787\n",
      "Epoch 80/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5589 - siamese_test_loss: 0.3881 - mlp_test_loss: 0.5589 - mlp_test_binary_accuracy: 0.6997 - mlp_test_auc_14: 0.7777\n",
      "Epoch 81/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5598 - siamese_test_loss: 0.3976 - mlp_test_loss: 0.5598 - mlp_test_binary_accuracy: 0.7004 - mlp_test_auc_14: 0.7781\n",
      "Epoch 82/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5593 - siamese_test_loss: 0.3992 - mlp_test_loss: 0.5593 - mlp_test_binary_accuracy: 0.7047 - mlp_test_auc_14: 0.7786\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5587 - siamese_test_loss: 0.4034 - mlp_test_loss: 0.5587 - mlp_test_binary_accuracy: 0.7006 - mlp_test_auc_14: 0.7785\n",
      "Epoch 84/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5587 - siamese_test_loss: 0.3945 - mlp_test_loss: 0.5587 - mlp_test_binary_accuracy: 0.6962 - mlp_test_auc_14: 0.7768\n",
      "Epoch 85/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5575 - siamese_test_loss: 0.3942 - mlp_test_loss: 0.5575 - mlp_test_binary_accuracy: 0.7107 - mlp_test_auc_14: 0.7800\n",
      "Epoch 86/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5588 - siamese_test_loss: 0.4016 - mlp_test_loss: 0.5588 - mlp_test_binary_accuracy: 0.7057 - mlp_test_auc_14: 0.7777\n",
      "Epoch 87/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5579 - siamese_test_loss: 0.4070 - mlp_test_loss: 0.5579 - mlp_test_binary_accuracy: 0.6981 - mlp_test_auc_14: 0.7787\n",
      "Epoch 88/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5575 - siamese_test_loss: 0.3982 - mlp_test_loss: 0.5575 - mlp_test_binary_accuracy: 0.7047 - mlp_test_auc_14: 0.7795\n",
      "Epoch 89/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5571 - siamese_test_loss: 0.4035 - mlp_test_loss: 0.5571 - mlp_test_binary_accuracy: 0.7029 - mlp_test_auc_14: 0.7794\n",
      "Epoch 90/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5580 - siamese_test_loss: 0.4046 - mlp_test_loss: 0.5580 - mlp_test_binary_accuracy: 0.6953 - mlp_test_auc_14: 0.7782\n",
      "Epoch 91/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5574 - siamese_test_loss: 0.4079 - mlp_test_loss: 0.5574 - mlp_test_binary_accuracy: 0.6997 - mlp_test_auc_14: 0.7794\n",
      "Epoch 92/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5551 - siamese_test_loss: 0.4131 - mlp_test_loss: 0.5551 - mlp_test_binary_accuracy: 0.7011 - mlp_test_auc_14: 0.7812\n",
      "Epoch 93/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5554 - siamese_test_loss: 0.4092 - mlp_test_loss: 0.5554 - mlp_test_binary_accuracy: 0.6985 - mlp_test_auc_14: 0.7819\n",
      "Epoch 94/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5556 - siamese_test_loss: 0.4165 - mlp_test_loss: 0.5556 - mlp_test_binary_accuracy: 0.6960 - mlp_test_auc_14: 0.7805\n",
      "Epoch 95/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5559 - siamese_test_loss: 0.4144 - mlp_test_loss: 0.5559 - mlp_test_binary_accuracy: 0.7040 - mlp_test_auc_14: 0.7821\n",
      "Epoch 96/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5539 - siamese_test_loss: 0.4169 - mlp_test_loss: 0.5539 - mlp_test_binary_accuracy: 0.7008 - mlp_test_auc_14: 0.7830\n",
      "Epoch 97/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5555 - siamese_test_loss: 0.4151 - mlp_test_loss: 0.5555 - mlp_test_binary_accuracy: 0.7006 - mlp_test_auc_14: 0.7809\n",
      "Epoch 98/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5535 - siamese_test_loss: 0.4236 - mlp_test_loss: 0.5535 - mlp_test_binary_accuracy: 0.7091 - mlp_test_auc_14: 0.7838\n",
      "Epoch 99/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5533 - siamese_test_loss: 0.4180 - mlp_test_loss: 0.5533 - mlp_test_binary_accuracy: 0.7061 - mlp_test_auc_14: 0.7831\n",
      "Epoch 100/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5544 - siamese_test_loss: 0.4313 - mlp_test_loss: 0.5544 - mlp_test_binary_accuracy: 0.7045 - mlp_test_auc_14: 0.7832\n",
      "Epoch 101/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5529 - siamese_test_loss: 0.4201 - mlp_test_loss: 0.5529 - mlp_test_binary_accuracy: 0.7066 - mlp_test_auc_14: 0.7845\n",
      "Epoch 102/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5523 - siamese_test_loss: 0.4210 - mlp_test_loss: 0.5523 - mlp_test_binary_accuracy: 0.7031 - mlp_test_auc_14: 0.7840\n",
      "Epoch 103/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5522 - siamese_test_loss: 0.4169 - mlp_test_loss: 0.5522 - mlp_test_binary_accuracy: 0.7043 - mlp_test_auc_14: 0.7846\n",
      "Epoch 104/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5524 - siamese_test_loss: 0.4197 - mlp_test_loss: 0.5524 - mlp_test_binary_accuracy: 0.7017 - mlp_test_auc_14: 0.7842\n",
      "Epoch 105/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5512 - siamese_test_loss: 0.4184 - mlp_test_loss: 0.5512 - mlp_test_binary_accuracy: 0.7080 - mlp_test_auc_14: 0.7858\n",
      "Epoch 106/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5516 - siamese_test_loss: 0.4221 - mlp_test_loss: 0.5516 - mlp_test_binary_accuracy: 0.7015 - mlp_test_auc_14: 0.7850\n",
      "Epoch 107/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5495 - siamese_test_loss: 0.4183 - mlp_test_loss: 0.5495 - mlp_test_binary_accuracy: 0.7123 - mlp_test_auc_14: 0.7878\n",
      "Epoch 108/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5517 - siamese_test_loss: 0.4244 - mlp_test_loss: 0.5517 - mlp_test_binary_accuracy: 0.7061 - mlp_test_auc_14: 0.7849\n",
      "Epoch 109/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5510 - siamese_test_loss: 0.4215 - mlp_test_loss: 0.5510 - mlp_test_binary_accuracy: 0.7098 - mlp_test_auc_14: 0.7854\n",
      "Epoch 110/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5494 - siamese_test_loss: 0.4213 - mlp_test_loss: 0.5494 - mlp_test_binary_accuracy: 0.7070 - mlp_test_auc_14: 0.7881\n",
      "Epoch 111/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5479 - siamese_test_loss: 0.4262 - mlp_test_loss: 0.5479 - mlp_test_binary_accuracy: 0.7066 - mlp_test_auc_14: 0.7889\n",
      "Epoch 112/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5480 - siamese_test_loss: 0.4302 - mlp_test_loss: 0.5480 - mlp_test_binary_accuracy: 0.7096 - mlp_test_auc_14: 0.7889\n",
      "Epoch 113/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5472 - siamese_test_loss: 0.4407 - mlp_test_loss: 0.5472 - mlp_test_binary_accuracy: 0.7151 - mlp_test_auc_14: 0.7901\n",
      "Epoch 114/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5466 - siamese_test_loss: 0.4391 - mlp_test_loss: 0.5466 - mlp_test_binary_accuracy: 0.7077 - mlp_test_auc_14: 0.7904\n",
      "Epoch 115/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5464 - siamese_test_loss: 0.4337 - mlp_test_loss: 0.5464 - mlp_test_binary_accuracy: 0.7114 - mlp_test_auc_14: 0.7906\n",
      "Epoch 116/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5466 - siamese_test_loss: 0.4378 - mlp_test_loss: 0.5466 - mlp_test_binary_accuracy: 0.7073 - mlp_test_auc_14: 0.7907\n",
      "Epoch 117/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5465 - siamese_test_loss: 0.4437 - mlp_test_loss: 0.5465 - mlp_test_binary_accuracy: 0.7100 - mlp_test_auc_14: 0.7907\n",
      "Epoch 118/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5459 - siamese_test_loss: 0.4410 - mlp_test_loss: 0.5459 - mlp_test_binary_accuracy: 0.7082 - mlp_test_auc_14: 0.7904\n",
      "Epoch 119/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5448 - siamese_test_loss: 0.4442 - mlp_test_loss: 0.5448 - mlp_test_binary_accuracy: 0.7109 - mlp_test_auc_14: 0.7918\n",
      "Epoch 120/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5458 - siamese_test_loss: 0.4445 - mlp_test_loss: 0.5458 - mlp_test_binary_accuracy: 0.7080 - mlp_test_auc_14: 0.7904\n",
      "Epoch 121/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5445 - siamese_test_loss: 0.4512 - mlp_test_loss: 0.5445 - mlp_test_binary_accuracy: 0.7107 - mlp_test_auc_14: 0.7917\n",
      "Epoch 122/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5427 - siamese_test_loss: 0.4530 - mlp_test_loss: 0.5427 - mlp_test_binary_accuracy: 0.7066 - mlp_test_auc_14: 0.7931\n",
      "Epoch 123/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5430 - siamese_test_loss: 0.4572 - mlp_test_loss: 0.5430 - mlp_test_binary_accuracy: 0.7093 - mlp_test_auc_14: 0.7927\n",
      "Epoch 124/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5428 - siamese_test_loss: 0.4535 - mlp_test_loss: 0.5428 - mlp_test_binary_accuracy: 0.7080 - mlp_test_auc_14: 0.7927\n",
      "Epoch 125/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5416 - siamese_test_loss: 0.4576 - mlp_test_loss: 0.5416 - mlp_test_binary_accuracy: 0.7084 - mlp_test_auc_14: 0.7943\n",
      "Epoch 126/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5431 - siamese_test_loss: 0.4554 - mlp_test_loss: 0.5431 - mlp_test_binary_accuracy: 0.7105 - mlp_test_auc_14: 0.7931\n",
      "Epoch 127/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5408 - siamese_test_loss: 0.4598 - mlp_test_loss: 0.5408 - mlp_test_binary_accuracy: 0.7112 - mlp_test_auc_14: 0.7950\n",
      "Epoch 128/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5419 - siamese_test_loss: 0.4607 - mlp_test_loss: 0.5419 - mlp_test_binary_accuracy: 0.7142 - mlp_test_auc_14: 0.7943\n",
      "Epoch 129/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5415 - siamese_test_loss: 0.4617 - mlp_test_loss: 0.5415 - mlp_test_binary_accuracy: 0.7084 - mlp_test_auc_14: 0.7940\n",
      "Epoch 130/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5421 - siamese_test_loss: 0.4706 - mlp_test_loss: 0.5421 - mlp_test_binary_accuracy: 0.7082 - mlp_test_auc_14: 0.7931\n",
      "Epoch 131/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5392 - siamese_test_loss: 0.4680 - mlp_test_loss: 0.5392 - mlp_test_binary_accuracy: 0.7135 - mlp_test_auc_14: 0.7969\n",
      "Epoch 132/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5410 - siamese_test_loss: 0.4720 - mlp_test_loss: 0.5410 - mlp_test_binary_accuracy: 0.7068 - mlp_test_auc_14: 0.7944\n",
      "Epoch 133/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5400 - siamese_test_loss: 0.4735 - mlp_test_loss: 0.5400 - mlp_test_binary_accuracy: 0.7125 - mlp_test_auc_14: 0.7957\n",
      "Epoch 134/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5393 - siamese_test_loss: 0.4705 - mlp_test_loss: 0.5393 - mlp_test_binary_accuracy: 0.7171 - mlp_test_auc_14: 0.7973\n",
      "Epoch 135/200\n",
      "136/136 [==============================] - 0s 988us/step - loss: 0.5395 - siamese_test_loss: 0.4749 - mlp_test_loss: 0.5395 - mlp_test_binary_accuracy: 0.7086 - mlp_test_auc_14: 0.7954\n",
      "Epoch 136/200\n",
      "136/136 [==============================] - 0s 931us/step - loss: 0.5390 - siamese_test_loss: 0.4790 - mlp_test_loss: 0.5390 - mlp_test_binary_accuracy: 0.7107 - mlp_test_auc_14: 0.7964\n",
      "Epoch 137/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5385 - siamese_test_loss: 0.4804 - mlp_test_loss: 0.5385 - mlp_test_binary_accuracy: 0.7185 - mlp_test_auc_14: 0.7984\n",
      "Epoch 138/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5390 - siamese_test_loss: 0.4809 - mlp_test_loss: 0.5390 - mlp_test_binary_accuracy: 0.7137 - mlp_test_auc_14: 0.7965\n",
      "Epoch 139/200\n",
      "136/136 [==============================] - 0s 911us/step - loss: 0.5381 - siamese_test_loss: 0.4850 - mlp_test_loss: 0.5381 - mlp_test_binary_accuracy: 0.7151 - mlp_test_auc_14: 0.7971\n",
      "Epoch 140/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5376 - siamese_test_loss: 0.4834 - mlp_test_loss: 0.5376 - mlp_test_binary_accuracy: 0.7089 - mlp_test_auc_14: 0.7978\n",
      "Epoch 141/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5379 - siamese_test_loss: 0.4868 - mlp_test_loss: 0.5379 - mlp_test_binary_accuracy: 0.7144 - mlp_test_auc_14: 0.7978\n",
      "Epoch 142/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5372 - siamese_test_loss: 0.4931 - mlp_test_loss: 0.5372 - mlp_test_binary_accuracy: 0.7112 - mlp_test_auc_14: 0.7982\n",
      "Epoch 143/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5367 - siamese_test_loss: 0.4908 - mlp_test_loss: 0.5367 - mlp_test_binary_accuracy: 0.7153 - mlp_test_auc_14: 0.7985\n",
      "Epoch 144/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5360 - siamese_test_loss: 0.4878 - mlp_test_loss: 0.5360 - mlp_test_binary_accuracy: 0.7121 - mlp_test_auc_14: 0.7987\n",
      "Epoch 145/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5362 - siamese_test_loss: 0.5005 - mlp_test_loss: 0.5362 - mlp_test_binary_accuracy: 0.7137 - mlp_test_auc_14: 0.7989\n",
      "Epoch 146/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5359 - siamese_test_loss: 0.4941 - mlp_test_loss: 0.5359 - mlp_test_binary_accuracy: 0.7125 - mlp_test_auc_14: 0.7998\n",
      "Epoch 147/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5343 - siamese_test_loss: 0.5020 - mlp_test_loss: 0.5343 - mlp_test_binary_accuracy: 0.7185 - mlp_test_auc_14: 0.8005\n",
      "Epoch 148/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5348 - siamese_test_loss: 0.4988 - mlp_test_loss: 0.5348 - mlp_test_binary_accuracy: 0.7165 - mlp_test_auc_14: 0.8003\n",
      "Epoch 149/200\n",
      "136/136 [==============================] - 0s 941us/step - loss: 0.5353 - siamese_test_loss: 0.5039 - mlp_test_loss: 0.5353 - mlp_test_binary_accuracy: 0.7176 - mlp_test_auc_14: 0.8003\n",
      "Epoch 150/200\n",
      "136/136 [==============================] - 0s 947us/step - loss: 0.5339 - siamese_test_loss: 0.5008 - mlp_test_loss: 0.5339 - mlp_test_binary_accuracy: 0.7146 - mlp_test_auc_14: 0.8009\n",
      "Epoch 151/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5337 - siamese_test_loss: 0.4977 - mlp_test_loss: 0.5337 - mlp_test_binary_accuracy: 0.7132 - mlp_test_auc_14: 0.8010\n",
      "Epoch 152/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5348 - siamese_test_loss: 0.5081 - mlp_test_loss: 0.5348 - mlp_test_binary_accuracy: 0.7155 - mlp_test_auc_14: 0.8007\n",
      "Epoch 153/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5317 - siamese_test_loss: 0.5052 - mlp_test_loss: 0.5317 - mlp_test_binary_accuracy: 0.7174 - mlp_test_auc_14: 0.8031\n",
      "Epoch 154/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5342 - siamese_test_loss: 0.5100 - mlp_test_loss: 0.5342 - mlp_test_binary_accuracy: 0.7181 - mlp_test_auc_14: 0.8005\n",
      "Epoch 155/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5312 - siamese_test_loss: 0.5120 - mlp_test_loss: 0.5312 - mlp_test_binary_accuracy: 0.7201 - mlp_test_auc_14: 0.8035\n",
      "Epoch 156/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5321 - siamese_test_loss: 0.5144 - mlp_test_loss: 0.5321 - mlp_test_binary_accuracy: 0.7148 - mlp_test_auc_14: 0.8021\n",
      "Epoch 157/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5320 - siamese_test_loss: 0.5160 - mlp_test_loss: 0.5320 - mlp_test_binary_accuracy: 0.7206 - mlp_test_auc_14: 0.8033\n",
      "Epoch 158/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5330 - siamese_test_loss: 0.5248 - mlp_test_loss: 0.5330 - mlp_test_binary_accuracy: 0.7112 - mlp_test_auc_14: 0.8014\n",
      "Epoch 159/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5320 - siamese_test_loss: 0.5165 - mlp_test_loss: 0.5320 - mlp_test_binary_accuracy: 0.7137 - mlp_test_auc_14: 0.8028\n",
      "Epoch 160/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5314 - siamese_test_loss: 0.5197 - mlp_test_loss: 0.5314 - mlp_test_binary_accuracy: 0.7162 - mlp_test_auc_14: 0.8027\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002186C669288> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 983us/step - loss: 0.6083 - siamese_test_loss: 0.4927 - mlp_test_loss: 0.6083 - mlp_test_binary_accuracy: 0.6963 - mlp_test_auc_14: 0.7542\n",
      "------ mlp_test_binary_accuracy: 69.63%\t ----- mlp_test_auc_14: 75.42%\n",
      "concat_fea的shape： (None, 16)\n",
      "-------------------------------------Kfold: 2 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6905 - siamese_test_loss: 0.4033 - mlp_test_loss: 0.6905 - mlp_test_binary_accuracy: 0.5280 - mlp_test_auc_15: 0.5444\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6746 - siamese_test_loss: 0.3269 - mlp_test_loss: 0.6746 - mlp_test_binary_accuracy: 0.5967 - mlp_test_auc_15: 0.6388\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6530 - siamese_test_loss: 0.3266 - mlp_test_loss: 0.6530 - mlp_test_binary_accuracy: 0.6213 - mlp_test_auc_15: 0.6670\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6394 - siamese_test_loss: 0.3447 - mlp_test_loss: 0.6394 - mlp_test_binary_accuracy: 0.6324 - mlp_test_auc_15: 0.6838\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6284 - siamese_test_loss: 0.3452 - mlp_test_loss: 0.6284 - mlp_test_binary_accuracy: 0.6321 - mlp_test_auc_15: 0.6984\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6213 - siamese_test_loss: 0.3535 - mlp_test_loss: 0.6213 - mlp_test_binary_accuracy: 0.6413 - mlp_test_auc_15: 0.7057\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 991us/step - loss: 0.6155 - siamese_test_loss: 0.3502 - mlp_test_loss: 0.6155 - mlp_test_binary_accuracy: 0.6429 - mlp_test_auc_15: 0.7122\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6116 - siamese_test_loss: 0.3520 - mlp_test_loss: 0.6116 - mlp_test_binary_accuracy: 0.6526 - mlp_test_auc_15: 0.7167\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 940us/step - loss: 0.6060 - siamese_test_loss: 0.3520 - mlp_test_loss: 0.6060 - mlp_test_binary_accuracy: 0.6562 - mlp_test_auc_15: 0.7239\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6043 - siamese_test_loss: 0.3523 - mlp_test_loss: 0.6043 - mlp_test_binary_accuracy: 0.6643 - mlp_test_auc_15: 0.7262\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6013 - siamese_test_loss: 0.3446 - mlp_test_loss: 0.6013 - mlp_test_binary_accuracy: 0.6666 - mlp_test_auc_15: 0.7300\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5985 - siamese_test_loss: 0.3443 - mlp_test_loss: 0.5985 - mlp_test_binary_accuracy: 0.6677 - mlp_test_auc_15: 0.7325\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5980 - siamese_test_loss: 0.3445 - mlp_test_loss: 0.5980 - mlp_test_binary_accuracy: 0.6631 - mlp_test_auc_15: 0.7335\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5953 - siamese_test_loss: 0.3441 - mlp_test_loss: 0.5953 - mlp_test_binary_accuracy: 0.6700 - mlp_test_auc_15: 0.7376\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5949 - siamese_test_loss: 0.3434 - mlp_test_loss: 0.5949 - mlp_test_binary_accuracy: 0.6687 - mlp_test_auc_15: 0.7369\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5920 - siamese_test_loss: 0.3425 - mlp_test_loss: 0.5920 - mlp_test_binary_accuracy: 0.6712 - mlp_test_auc_15: 0.7410\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5924 - siamese_test_loss: 0.3439 - mlp_test_loss: 0.5924 - mlp_test_binary_accuracy: 0.6716 - mlp_test_auc_15: 0.7397\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5917 - siamese_test_loss: 0.3450 - mlp_test_loss: 0.5917 - mlp_test_binary_accuracy: 0.6767 - mlp_test_auc_15: 0.7409\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5899 - siamese_test_loss: 0.3448 - mlp_test_loss: 0.5899 - mlp_test_binary_accuracy: 0.6691 - mlp_test_auc_15: 0.7430\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5878 - siamese_test_loss: 0.3449 - mlp_test_loss: 0.5878 - mlp_test_binary_accuracy: 0.6783 - mlp_test_auc_15: 0.7449\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5879 - siamese_test_loss: 0.3452 - mlp_test_loss: 0.5879 - mlp_test_binary_accuracy: 0.6769 - mlp_test_auc_15: 0.7462\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5887 - siamese_test_loss: 0.3465 - mlp_test_loss: 0.5887 - mlp_test_binary_accuracy: 0.6737 - mlp_test_auc_15: 0.7448\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5846 - siamese_test_loss: 0.3482 - mlp_test_loss: 0.5846 - mlp_test_binary_accuracy: 0.6783 - mlp_test_auc_15: 0.7491\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 923us/step - loss: 0.5849 - siamese_test_loss: 0.3499 - mlp_test_loss: 0.5849 - mlp_test_binary_accuracy: 0.6854 - mlp_test_auc_15: 0.7486\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 930us/step - loss: 0.5841 - siamese_test_loss: 0.3518 - mlp_test_loss: 0.5841 - mlp_test_binary_accuracy: 0.6744 - mlp_test_auc_15: 0.7481\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 937us/step - loss: 0.5829 - siamese_test_loss: 0.3546 - mlp_test_loss: 0.5829 - mlp_test_binary_accuracy: 0.6762 - mlp_test_auc_15: 0.7505\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 920us/step - loss: 0.5815 - siamese_test_loss: 0.3573 - mlp_test_loss: 0.5815 - mlp_test_binary_accuracy: 0.6827 - mlp_test_auc_15: 0.7518\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 926us/step - loss: 0.5797 - siamese_test_loss: 0.3642 - mlp_test_loss: 0.5797 - mlp_test_binary_accuracy: 0.6827 - mlp_test_auc_15: 0.7545\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 919us/step - loss: 0.5811 - siamese_test_loss: 0.3594 - mlp_test_loss: 0.5811 - mlp_test_binary_accuracy: 0.6820 - mlp_test_auc_15: 0.7531\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5791 - siamese_test_loss: 0.3644 - mlp_test_loss: 0.5791 - mlp_test_binary_accuracy: 0.6767 - mlp_test_auc_15: 0.7555\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 937us/step - loss: 0.5786 - siamese_test_loss: 0.3652 - mlp_test_loss: 0.5786 - mlp_test_binary_accuracy: 0.6804 - mlp_test_auc_15: 0.7540\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5773 - siamese_test_loss: 0.3684 - mlp_test_loss: 0.5773 - mlp_test_binary_accuracy: 0.6834 - mlp_test_auc_15: 0.7575\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5756 - siamese_test_loss: 0.3720 - mlp_test_loss: 0.5756 - mlp_test_binary_accuracy: 0.6859 - mlp_test_auc_15: 0.7594\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5756 - siamese_test_loss: 0.3751 - mlp_test_loss: 0.5756 - mlp_test_binary_accuracy: 0.6884 - mlp_test_auc_15: 0.7598\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5733 - siamese_test_loss: 0.3713 - mlp_test_loss: 0.5733 - mlp_test_binary_accuracy: 0.6944 - mlp_test_auc_15: 0.7617\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 957us/step - loss: 0.5749 - siamese_test_loss: 0.3755 - mlp_test_loss: 0.5749 - mlp_test_binary_accuracy: 0.6880 - mlp_test_auc_15: 0.7603\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 925us/step - loss: 0.5711 - siamese_test_loss: 0.3780 - mlp_test_loss: 0.5711 - mlp_test_binary_accuracy: 0.6882 - mlp_test_auc_15: 0.7639\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5729 - siamese_test_loss: 0.3770 - mlp_test_loss: 0.5729 - mlp_test_binary_accuracy: 0.6907 - mlp_test_auc_15: 0.7623\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5706 - siamese_test_loss: 0.3816 - mlp_test_loss: 0.5706 - mlp_test_binary_accuracy: 0.6926 - mlp_test_auc_15: 0.7656\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 985us/step - loss: 0.5710 - siamese_test_loss: 0.3787 - mlp_test_loss: 0.5710 - mlp_test_binary_accuracy: 0.6905 - mlp_test_auc_15: 0.7641\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 916us/step - loss: 0.5687 - siamese_test_loss: 0.3831 - mlp_test_loss: 0.5687 - mlp_test_binary_accuracy: 0.6953 - mlp_test_auc_15: 0.7679\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5678 - siamese_test_loss: 0.3918 - mlp_test_loss: 0.5678 - mlp_test_binary_accuracy: 0.6955 - mlp_test_auc_15: 0.7690\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5679 - siamese_test_loss: 0.3858 - mlp_test_loss: 0.5679 - mlp_test_binary_accuracy: 0.6949 - mlp_test_auc_15: 0.7681\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 923us/step - loss: 0.5678 - siamese_test_loss: 0.3901 - mlp_test_loss: 0.5678 - mlp_test_binary_accuracy: 0.6955 - mlp_test_auc_15: 0.7688\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 938us/step - loss: 0.5646 - siamese_test_loss: 0.3910 - mlp_test_loss: 0.5646 - mlp_test_binary_accuracy: 0.6946 - mlp_test_auc_15: 0.7722\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5656 - siamese_test_loss: 0.3938 - mlp_test_loss: 0.5656 - mlp_test_binary_accuracy: 0.6960 - mlp_test_auc_15: 0.7719\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 925us/step - loss: 0.5649 - siamese_test_loss: 0.3968 - mlp_test_loss: 0.5649 - mlp_test_binary_accuracy: 0.6942 - mlp_test_auc_15: 0.7714\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5643 - siamese_test_loss: 0.3993 - mlp_test_loss: 0.5643 - mlp_test_binary_accuracy: 0.7038 - mlp_test_auc_15: 0.7738\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5640 - siamese_test_loss: 0.3992 - mlp_test_loss: 0.5640 - mlp_test_binary_accuracy: 0.7015 - mlp_test_auc_15: 0.7742\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5625 - siamese_test_loss: 0.4046 - mlp_test_loss: 0.5625 - mlp_test_binary_accuracy: 0.7008 - mlp_test_auc_15: 0.7750\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 926us/step - loss: 0.5633 - siamese_test_loss: 0.4086 - mlp_test_loss: 0.5633 - mlp_test_binary_accuracy: 0.7011 - mlp_test_auc_15: 0.7745\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5624 - siamese_test_loss: 0.4124 - mlp_test_loss: 0.5624 - mlp_test_binary_accuracy: 0.7043 - mlp_test_auc_15: 0.7761\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5603 - siamese_test_loss: 0.4152 - mlp_test_loss: 0.5603 - mlp_test_binary_accuracy: 0.7070 - mlp_test_auc_15: 0.7775\n",
      "Epoch 54/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5609 - siamese_test_loss: 0.4134 - mlp_test_loss: 0.5609 - mlp_test_binary_accuracy: 0.7022 - mlp_test_auc_15: 0.7770\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - 0s 985us/step - loss: 0.5608 - siamese_test_loss: 0.4185 - mlp_test_loss: 0.5608 - mlp_test_binary_accuracy: 0.7047 - mlp_test_auc_15: 0.7770\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - 0s 997us/step - loss: 0.5582 - siamese_test_loss: 0.4263 - mlp_test_loss: 0.5582 - mlp_test_binary_accuracy: 0.7093 - mlp_test_auc_15: 0.7808\n",
      "Epoch 57/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5582 - siamese_test_loss: 0.4241 - mlp_test_loss: 0.5582 - mlp_test_binary_accuracy: 0.7036 - mlp_test_auc_15: 0.7790\n",
      "Epoch 58/200\n",
      "136/136 [==============================] - 0s 929us/step - loss: 0.5571 - siamese_test_loss: 0.4302 - mlp_test_loss: 0.5571 - mlp_test_binary_accuracy: 0.7077 - mlp_test_auc_15: 0.7813\n",
      "Epoch 59/200\n",
      "136/136 [==============================] - 0s 923us/step - loss: 0.5587 - siamese_test_loss: 0.4308 - mlp_test_loss: 0.5587 - mlp_test_binary_accuracy: 0.7047 - mlp_test_auc_15: 0.7797\n",
      "Epoch 60/200\n",
      "136/136 [==============================] - 0s 926us/step - loss: 0.5574 - siamese_test_loss: 0.4336 - mlp_test_loss: 0.5574 - mlp_test_binary_accuracy: 0.7054 - mlp_test_auc_15: 0.7804\n",
      "Epoch 61/200\n",
      "136/136 [==============================] - 0s 925us/step - loss: 0.5569 - siamese_test_loss: 0.4393 - mlp_test_loss: 0.5569 - mlp_test_binary_accuracy: 0.7047 - mlp_test_auc_15: 0.7809\n",
      "Epoch 62/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5567 - siamese_test_loss: 0.4410 - mlp_test_loss: 0.5567 - mlp_test_binary_accuracy: 0.7057 - mlp_test_auc_15: 0.7818\n",
      "Epoch 63/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5561 - siamese_test_loss: 0.4430 - mlp_test_loss: 0.5561 - mlp_test_binary_accuracy: 0.7093 - mlp_test_auc_15: 0.7820\n",
      "Epoch 64/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5553 - siamese_test_loss: 0.4441 - mlp_test_loss: 0.5553 - mlp_test_binary_accuracy: 0.7098 - mlp_test_auc_15: 0.7829\n",
      "Epoch 65/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5547 - siamese_test_loss: 0.4485 - mlp_test_loss: 0.5547 - mlp_test_binary_accuracy: 0.7102 - mlp_test_auc_15: 0.7834\n",
      "Epoch 66/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5544 - siamese_test_loss: 0.4467 - mlp_test_loss: 0.5544 - mlp_test_binary_accuracy: 0.7066 - mlp_test_auc_15: 0.7835\n",
      "Epoch 67/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5554 - siamese_test_loss: 0.4535 - mlp_test_loss: 0.5554 - mlp_test_binary_accuracy: 0.7038 - mlp_test_auc_15: 0.7829\n",
      "Epoch 68/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5537 - siamese_test_loss: 0.4536 - mlp_test_loss: 0.5537 - mlp_test_binary_accuracy: 0.7080 - mlp_test_auc_15: 0.7843\n",
      "Epoch 69/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5535 - siamese_test_loss: 0.4611 - mlp_test_loss: 0.5535 - mlp_test_binary_accuracy: 0.7105 - mlp_test_auc_15: 0.7854\n",
      "Epoch 70/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5524 - siamese_test_loss: 0.4672 - mlp_test_loss: 0.5524 - mlp_test_binary_accuracy: 0.7119 - mlp_test_auc_15: 0.7868\n",
      "Epoch 71/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5528 - siamese_test_loss: 0.4673 - mlp_test_loss: 0.5528 - mlp_test_binary_accuracy: 0.7146 - mlp_test_auc_15: 0.7852\n",
      "Epoch 72/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5530 - siamese_test_loss: 0.4695 - mlp_test_loss: 0.5530 - mlp_test_binary_accuracy: 0.7073 - mlp_test_auc_15: 0.7853\n",
      "Epoch 73/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5514 - siamese_test_loss: 0.4673 - mlp_test_loss: 0.5514 - mlp_test_binary_accuracy: 0.7112 - mlp_test_auc_15: 0.7876\n",
      "Epoch 74/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5510 - siamese_test_loss: 0.4752 - mlp_test_loss: 0.5510 - mlp_test_binary_accuracy: 0.7144 - mlp_test_auc_15: 0.7878\n",
      "Epoch 75/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5495 - siamese_test_loss: 0.4780 - mlp_test_loss: 0.5495 - mlp_test_binary_accuracy: 0.7102 - mlp_test_auc_15: 0.7890\n",
      "Epoch 76/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5495 - siamese_test_loss: 0.4788 - mlp_test_loss: 0.5495 - mlp_test_binary_accuracy: 0.7116 - mlp_test_auc_15: 0.7884\n",
      "Epoch 77/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5491 - siamese_test_loss: 0.4811 - mlp_test_loss: 0.5491 - mlp_test_binary_accuracy: 0.7174 - mlp_test_auc_15: 0.7902\n",
      "Epoch 78/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5486 - siamese_test_loss: 0.4816 - mlp_test_loss: 0.5486 - mlp_test_binary_accuracy: 0.7135 - mlp_test_auc_15: 0.7899\n",
      "Epoch 79/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5470 - siamese_test_loss: 0.4877 - mlp_test_loss: 0.5470 - mlp_test_binary_accuracy: 0.7137 - mlp_test_auc_15: 0.7911\n",
      "Epoch 80/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5470 - siamese_test_loss: 0.4896 - mlp_test_loss: 0.5470 - mlp_test_binary_accuracy: 0.7233 - mlp_test_auc_15: 0.7922\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5473 - siamese_test_loss: 0.4946 - mlp_test_loss: 0.5473 - mlp_test_binary_accuracy: 0.7165 - mlp_test_auc_15: 0.7923\n",
      "Epoch 82/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5485 - siamese_test_loss: 0.4957 - mlp_test_loss: 0.5485 - mlp_test_binary_accuracy: 0.7148 - mlp_test_auc_15: 0.7903\n",
      "Epoch 83/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5465 - siamese_test_loss: 0.4952 - mlp_test_loss: 0.5465 - mlp_test_binary_accuracy: 0.7146 - mlp_test_auc_15: 0.7918\n",
      "Epoch 84/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5474 - siamese_test_loss: 0.4957 - mlp_test_loss: 0.5474 - mlp_test_binary_accuracy: 0.7151 - mlp_test_auc_15: 0.7927\n",
      "Epoch 85/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5469 - siamese_test_loss: 0.4957 - mlp_test_loss: 0.5469 - mlp_test_binary_accuracy: 0.7165 - mlp_test_auc_15: 0.7919\n",
      "Epoch 86/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5460 - siamese_test_loss: 0.4953 - mlp_test_loss: 0.5460 - mlp_test_binary_accuracy: 0.7197 - mlp_test_auc_15: 0.7928\n",
      "Epoch 87/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5444 - siamese_test_loss: 0.5018 - mlp_test_loss: 0.5444 - mlp_test_binary_accuracy: 0.7158 - mlp_test_auc_15: 0.7941\n",
      "Epoch 88/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5446 - siamese_test_loss: 0.5053 - mlp_test_loss: 0.5446 - mlp_test_binary_accuracy: 0.7206 - mlp_test_auc_15: 0.7949\n",
      "Epoch 89/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5458 - siamese_test_loss: 0.5073 - mlp_test_loss: 0.5458 - mlp_test_binary_accuracy: 0.7144 - mlp_test_auc_15: 0.7927\n",
      "Epoch 90/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5443 - siamese_test_loss: 0.5055 - mlp_test_loss: 0.5443 - mlp_test_binary_accuracy: 0.7130 - mlp_test_auc_15: 0.7947\n",
      "Epoch 91/200\n",
      "136/136 [==============================] - 0s 989us/step - loss: 0.5448 - siamese_test_loss: 0.5054 - mlp_test_loss: 0.5448 - mlp_test_binary_accuracy: 0.7146 - mlp_test_auc_15: 0.7946\n",
      "Epoch 92/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5434 - siamese_test_loss: 0.5132 - mlp_test_loss: 0.5434 - mlp_test_binary_accuracy: 0.7197 - mlp_test_auc_15: 0.7959\n",
      "Epoch 93/200\n",
      "136/136 [==============================] - 0s 964us/step - loss: 0.5421 - siamese_test_loss: 0.5173 - mlp_test_loss: 0.5421 - mlp_test_binary_accuracy: 0.7208 - mlp_test_auc_15: 0.7974\n",
      "Epoch 94/200\n",
      "136/136 [==============================] - 0s 923us/step - loss: 0.5422 - siamese_test_loss: 0.5103 - mlp_test_loss: 0.5422 - mlp_test_binary_accuracy: 0.7165 - mlp_test_auc_15: 0.7957\n",
      "Epoch 95/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5408 - siamese_test_loss: 0.5190 - mlp_test_loss: 0.5408 - mlp_test_binary_accuracy: 0.7236 - mlp_test_auc_15: 0.7983\n",
      "Epoch 96/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5396 - siamese_test_loss: 0.5181 - mlp_test_loss: 0.5396 - mlp_test_binary_accuracy: 0.7224 - mlp_test_auc_15: 0.7988\n",
      "Epoch 97/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5404 - siamese_test_loss: 0.5172 - mlp_test_loss: 0.5404 - mlp_test_binary_accuracy: 0.7273 - mlp_test_auc_15: 0.7992\n",
      "Epoch 98/200\n",
      "136/136 [==============================] - 0s 997us/step - loss: 0.5376 - siamese_test_loss: 0.5233 - mlp_test_loss: 0.5376 - mlp_test_binary_accuracy: 0.7252 - mlp_test_auc_15: 0.8013\n",
      "Epoch 99/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5399 - siamese_test_loss: 0.5224 - mlp_test_loss: 0.5399 - mlp_test_binary_accuracy: 0.7220 - mlp_test_auc_15: 0.7991\n",
      "Epoch 100/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5395 - siamese_test_loss: 0.5273 - mlp_test_loss: 0.5395 - mlp_test_binary_accuracy: 0.7194 - mlp_test_auc_15: 0.8000\n",
      "Epoch 101/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5382 - siamese_test_loss: 0.5260 - mlp_test_loss: 0.5382 - mlp_test_binary_accuracy: 0.7243 - mlp_test_auc_15: 0.8001\n",
      "Epoch 102/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5384 - siamese_test_loss: 0.5268 - mlp_test_loss: 0.5384 - mlp_test_binary_accuracy: 0.7217 - mlp_test_auc_15: 0.8003\n",
      "Epoch 103/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5380 - siamese_test_loss: 0.5301 - mlp_test_loss: 0.5380 - mlp_test_binary_accuracy: 0.7206 - mlp_test_auc_15: 0.8005\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002185CD66558> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 988us/step - loss: 0.5441 - siamese_test_loss: 0.5375 - mlp_test_loss: 0.5441 - mlp_test_binary_accuracy: 0.7190 - mlp_test_auc_15: 0.7980\n",
      "------ mlp_test_binary_accuracy: 71.90%\t ----- mlp_test_auc_15: 79.80%\n",
      "concat_fea的shape： (None, 16)\n",
      "-------------------------------------Kfold: 3 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6872 - siamese_test_loss: 0.4022 - mlp_test_loss: 0.6872 - mlp_test_binary_accuracy: 0.5471 - mlp_test_auc_16: 0.5686\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6629 - siamese_test_loss: 0.3485 - mlp_test_loss: 0.6629 - mlp_test_binary_accuracy: 0.6140 - mlp_test_auc_16: 0.6681\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 972us/step - loss: 0.6370 - siamese_test_loss: 0.3396 - mlp_test_loss: 0.6370 - mlp_test_binary_accuracy: 0.6443 - mlp_test_auc_16: 0.6970\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 993us/step - loss: 0.6208 - siamese_test_loss: 0.3420 - mlp_test_loss: 0.6208 - mlp_test_binary_accuracy: 0.6482 - mlp_test_auc_16: 0.7141\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 998us/step - loss: 0.6113 - siamese_test_loss: 0.3487 - mlp_test_loss: 0.6113 - mlp_test_binary_accuracy: 0.6590 - mlp_test_auc_16: 0.7223\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6056 - siamese_test_loss: 0.3503 - mlp_test_loss: 0.6056 - mlp_test_binary_accuracy: 0.6638 - mlp_test_auc_16: 0.7281\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6021 - siamese_test_loss: 0.3532 - mlp_test_loss: 0.6021 - mlp_test_binary_accuracy: 0.6631 - mlp_test_auc_16: 0.7312\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5997 - siamese_test_loss: 0.3529 - mlp_test_loss: 0.5997 - mlp_test_binary_accuracy: 0.6719 - mlp_test_auc_16: 0.7352\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5957 - siamese_test_loss: 0.3515 - mlp_test_loss: 0.5957 - mlp_test_binary_accuracy: 0.6710 - mlp_test_auc_16: 0.7404\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5959 - siamese_test_loss: 0.3511 - mlp_test_loss: 0.5959 - mlp_test_binary_accuracy: 0.6689 - mlp_test_auc_16: 0.7396\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5940 - siamese_test_loss: 0.3512 - mlp_test_loss: 0.5940 - mlp_test_binary_accuracy: 0.6758 - mlp_test_auc_16: 0.7418\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 994us/step - loss: 0.5917 - siamese_test_loss: 0.3489 - mlp_test_loss: 0.5917 - mlp_test_binary_accuracy: 0.6776 - mlp_test_auc_16: 0.7445\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5919 - siamese_test_loss: 0.3476 - mlp_test_loss: 0.5919 - mlp_test_binary_accuracy: 0.6776 - mlp_test_auc_16: 0.7446\n",
      "Epoch 14/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5901 - siamese_test_loss: 0.3460 - mlp_test_loss: 0.5901 - mlp_test_binary_accuracy: 0.6827 - mlp_test_auc_16: 0.7465\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5893 - siamese_test_loss: 0.3461 - mlp_test_loss: 0.5893 - mlp_test_binary_accuracy: 0.6783 - mlp_test_auc_16: 0.7472\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5883 - siamese_test_loss: 0.3446 - mlp_test_loss: 0.5883 - mlp_test_binary_accuracy: 0.6769 - mlp_test_auc_16: 0.7483\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5887 - siamese_test_loss: 0.3442 - mlp_test_loss: 0.5887 - mlp_test_binary_accuracy: 0.6827 - mlp_test_auc_16: 0.7482\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 938us/step - loss: 0.5877 - siamese_test_loss: 0.3447 - mlp_test_loss: 0.5877 - mlp_test_binary_accuracy: 0.6806 - mlp_test_auc_16: 0.7493\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 948us/step - loss: 0.5859 - siamese_test_loss: 0.3441 - mlp_test_loss: 0.5859 - mlp_test_binary_accuracy: 0.6850 - mlp_test_auc_16: 0.7524\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 932us/step - loss: 0.5851 - siamese_test_loss: 0.3425 - mlp_test_loss: 0.5851 - mlp_test_binary_accuracy: 0.6868 - mlp_test_auc_16: 0.7532\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5851 - siamese_test_loss: 0.3392 - mlp_test_loss: 0.5851 - mlp_test_binary_accuracy: 0.6873 - mlp_test_auc_16: 0.7519\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5854 - siamese_test_loss: 0.3426 - mlp_test_loss: 0.5854 - mlp_test_binary_accuracy: 0.6799 - mlp_test_auc_16: 0.7529\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 937us/step - loss: 0.5835 - siamese_test_loss: 0.3421 - mlp_test_loss: 0.5835 - mlp_test_binary_accuracy: 0.6818 - mlp_test_auc_16: 0.7538\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5824 - siamese_test_loss: 0.3419 - mlp_test_loss: 0.5824 - mlp_test_binary_accuracy: 0.6870 - mlp_test_auc_16: 0.7551\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5835 - siamese_test_loss: 0.3413 - mlp_test_loss: 0.5835 - mlp_test_binary_accuracy: 0.6861 - mlp_test_auc_16: 0.7547\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 923us/step - loss: 0.5832 - siamese_test_loss: 0.3433 - mlp_test_loss: 0.5832 - mlp_test_binary_accuracy: 0.6788 - mlp_test_auc_16: 0.7554\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 929us/step - loss: 0.5813 - siamese_test_loss: 0.3421 - mlp_test_loss: 0.5813 - mlp_test_binary_accuracy: 0.6847 - mlp_test_auc_16: 0.7569\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5812 - siamese_test_loss: 0.3397 - mlp_test_loss: 0.5812 - mlp_test_binary_accuracy: 0.6859 - mlp_test_auc_16: 0.7572\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5810 - siamese_test_loss: 0.3416 - mlp_test_loss: 0.5810 - mlp_test_binary_accuracy: 0.6850 - mlp_test_auc_16: 0.7576\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5788 - siamese_test_loss: 0.3427 - mlp_test_loss: 0.5788 - mlp_test_binary_accuracy: 0.6818 - mlp_test_auc_16: 0.7601\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5786 - siamese_test_loss: 0.3440 - mlp_test_loss: 0.5786 - mlp_test_binary_accuracy: 0.6880 - mlp_test_auc_16: 0.7603\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5777 - siamese_test_loss: 0.3429 - mlp_test_loss: 0.5777 - mlp_test_binary_accuracy: 0.6859 - mlp_test_auc_16: 0.7613\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 930us/step - loss: 0.5755 - siamese_test_loss: 0.3444 - mlp_test_loss: 0.5755 - mlp_test_binary_accuracy: 0.6946 - mlp_test_auc_16: 0.7656\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5749 - siamese_test_loss: 0.3422 - mlp_test_loss: 0.5749 - mlp_test_binary_accuracy: 0.6850 - mlp_test_auc_16: 0.7643\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5755 - siamese_test_loss: 0.3475 - mlp_test_loss: 0.5755 - mlp_test_binary_accuracy: 0.6827 - mlp_test_auc_16: 0.7635\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5731 - siamese_test_loss: 0.3505 - mlp_test_loss: 0.5731 - mlp_test_binary_accuracy: 0.6875 - mlp_test_auc_16: 0.7652\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5729 - siamese_test_loss: 0.3506 - mlp_test_loss: 0.5729 - mlp_test_binary_accuracy: 0.6850 - mlp_test_auc_16: 0.7654\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 929us/step - loss: 0.5720 - siamese_test_loss: 0.3524 - mlp_test_loss: 0.5720 - mlp_test_binary_accuracy: 0.6896 - mlp_test_auc_16: 0.7667\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 941us/step - loss: 0.5709 - siamese_test_loss: 0.3585 - mlp_test_loss: 0.5709 - mlp_test_binary_accuracy: 0.6893 - mlp_test_auc_16: 0.7677\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 928us/step - loss: 0.5696 - siamese_test_loss: 0.3576 - mlp_test_loss: 0.5696 - mlp_test_binary_accuracy: 0.6898 - mlp_test_auc_16: 0.7697\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5685 - siamese_test_loss: 0.3629 - mlp_test_loss: 0.5685 - mlp_test_binary_accuracy: 0.6884 - mlp_test_auc_16: 0.7704\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5676 - siamese_test_loss: 0.3626 - mlp_test_loss: 0.5676 - mlp_test_binary_accuracy: 0.6932 - mlp_test_auc_16: 0.7716\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5668 - siamese_test_loss: 0.3639 - mlp_test_loss: 0.5668 - mlp_test_binary_accuracy: 0.6889 - mlp_test_auc_16: 0.7716\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 932us/step - loss: 0.5657 - siamese_test_loss: 0.3672 - mlp_test_loss: 0.5657 - mlp_test_binary_accuracy: 0.6965 - mlp_test_auc_16: 0.7725\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5625 - siamese_test_loss: 0.3705 - mlp_test_loss: 0.5625 - mlp_test_binary_accuracy: 0.6958 - mlp_test_auc_16: 0.7765\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5651 - siamese_test_loss: 0.3724 - mlp_test_loss: 0.5651 - mlp_test_binary_accuracy: 0.6921 - mlp_test_auc_16: 0.7718\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5637 - siamese_test_loss: 0.3759 - mlp_test_loss: 0.5637 - mlp_test_binary_accuracy: 0.6916 - mlp_test_auc_16: 0.7744\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5638 - siamese_test_loss: 0.3780 - mlp_test_loss: 0.5638 - mlp_test_binary_accuracy: 0.6974 - mlp_test_auc_16: 0.7750\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5622 - siamese_test_loss: 0.3764 - mlp_test_loss: 0.5622 - mlp_test_binary_accuracy: 0.6893 - mlp_test_auc_16: 0.7747\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5605 - siamese_test_loss: 0.3777 - mlp_test_loss: 0.5605 - mlp_test_binary_accuracy: 0.6972 - mlp_test_auc_16: 0.7779\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5602 - siamese_test_loss: 0.3815 - mlp_test_loss: 0.5602 - mlp_test_binary_accuracy: 0.6999 - mlp_test_auc_16: 0.7780\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5604 - siamese_test_loss: 0.3824 - mlp_test_loss: 0.5604 - mlp_test_binary_accuracy: 0.6937 - mlp_test_auc_16: 0.7771\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5580 - siamese_test_loss: 0.3866 - mlp_test_loss: 0.5580 - mlp_test_binary_accuracy: 0.6994 - mlp_test_auc_16: 0.7794\n",
      "Epoch 54/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5580 - siamese_test_loss: 0.3848 - mlp_test_loss: 0.5580 - mlp_test_binary_accuracy: 0.6997 - mlp_test_auc_16: 0.7800\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5582 - siamese_test_loss: 0.3901 - mlp_test_loss: 0.5582 - mlp_test_binary_accuracy: 0.7031 - mlp_test_auc_16: 0.7796\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5561 - siamese_test_loss: 0.3939 - mlp_test_loss: 0.5561 - mlp_test_binary_accuracy: 0.7001 - mlp_test_auc_16: 0.7811\n",
      "Epoch 57/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5548 - siamese_test_loss: 0.3911 - mlp_test_loss: 0.5548 - mlp_test_binary_accuracy: 0.7050 - mlp_test_auc_16: 0.7838\n",
      "Epoch 58/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5548 - siamese_test_loss: 0.3922 - mlp_test_loss: 0.5548 - mlp_test_binary_accuracy: 0.7031 - mlp_test_auc_16: 0.7835\n",
      "Epoch 59/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5551 - siamese_test_loss: 0.3929 - mlp_test_loss: 0.5551 - mlp_test_binary_accuracy: 0.7027 - mlp_test_auc_16: 0.7834\n",
      "Epoch 60/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5543 - siamese_test_loss: 0.3927 - mlp_test_loss: 0.5543 - mlp_test_binary_accuracy: 0.7038 - mlp_test_auc_16: 0.7830\n",
      "Epoch 61/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5536 - siamese_test_loss: 0.3928 - mlp_test_loss: 0.5536 - mlp_test_binary_accuracy: 0.7061 - mlp_test_auc_16: 0.7844\n",
      "Epoch 62/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5525 - siamese_test_loss: 0.3968 - mlp_test_loss: 0.5525 - mlp_test_binary_accuracy: 0.7059 - mlp_test_auc_16: 0.7855\n",
      "Epoch 63/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5526 - siamese_test_loss: 0.3970 - mlp_test_loss: 0.5526 - mlp_test_binary_accuracy: 0.7080 - mlp_test_auc_16: 0.7857\n",
      "Epoch 64/200\n",
      "136/136 [==============================] - 0s 971us/step - loss: 0.5512 - siamese_test_loss: 0.3991 - mlp_test_loss: 0.5512 - mlp_test_binary_accuracy: 0.7052 - mlp_test_auc_16: 0.7849\n",
      "Epoch 65/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5494 - siamese_test_loss: 0.3998 - mlp_test_loss: 0.5494 - mlp_test_binary_accuracy: 0.7050 - mlp_test_auc_16: 0.7890\n",
      "Epoch 66/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5498 - siamese_test_loss: 0.4031 - mlp_test_loss: 0.5498 - mlp_test_binary_accuracy: 0.7070 - mlp_test_auc_16: 0.7872\n",
      "Epoch 67/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5485 - siamese_test_loss: 0.4077 - mlp_test_loss: 0.5485 - mlp_test_binary_accuracy: 0.7116 - mlp_test_auc_16: 0.7885\n",
      "Epoch 68/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5476 - siamese_test_loss: 0.4044 - mlp_test_loss: 0.5476 - mlp_test_binary_accuracy: 0.7132 - mlp_test_auc_16: 0.7900\n",
      "Epoch 69/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5458 - siamese_test_loss: 0.4080 - mlp_test_loss: 0.5458 - mlp_test_binary_accuracy: 0.7162 - mlp_test_auc_16: 0.7932\n",
      "Epoch 70/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5452 - siamese_test_loss: 0.4116 - mlp_test_loss: 0.5452 - mlp_test_binary_accuracy: 0.7128 - mlp_test_auc_16: 0.7919\n",
      "Epoch 71/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5455 - siamese_test_loss: 0.4120 - mlp_test_loss: 0.5455 - mlp_test_binary_accuracy: 0.7112 - mlp_test_auc_16: 0.7918\n",
      "Epoch 72/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5442 - siamese_test_loss: 0.4131 - mlp_test_loss: 0.5442 - mlp_test_binary_accuracy: 0.7132 - mlp_test_auc_16: 0.7933\n",
      "Epoch 73/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5441 - siamese_test_loss: 0.4161 - mlp_test_loss: 0.5441 - mlp_test_binary_accuracy: 0.7167 - mlp_test_auc_16: 0.7938\n",
      "Epoch 74/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5426 - siamese_test_loss: 0.4153 - mlp_test_loss: 0.5426 - mlp_test_binary_accuracy: 0.7144 - mlp_test_auc_16: 0.7956\n",
      "Epoch 75/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5421 - siamese_test_loss: 0.4203 - mlp_test_loss: 0.5421 - mlp_test_binary_accuracy: 0.7146 - mlp_test_auc_16: 0.7947\n",
      "Epoch 76/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5423 - siamese_test_loss: 0.4225 - mlp_test_loss: 0.5423 - mlp_test_binary_accuracy: 0.7144 - mlp_test_auc_16: 0.7950\n",
      "Epoch 77/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5405 - siamese_test_loss: 0.4187 - mlp_test_loss: 0.5405 - mlp_test_binary_accuracy: 0.7160 - mlp_test_auc_16: 0.7959\n",
      "Epoch 78/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5411 - siamese_test_loss: 0.4186 - mlp_test_loss: 0.5411 - mlp_test_binary_accuracy: 0.7158 - mlp_test_auc_16: 0.7957\n",
      "Epoch 79/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5399 - siamese_test_loss: 0.4201 - mlp_test_loss: 0.5399 - mlp_test_binary_accuracy: 0.7162 - mlp_test_auc_16: 0.7966\n",
      "Epoch 80/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5389 - siamese_test_loss: 0.4229 - mlp_test_loss: 0.5389 - mlp_test_binary_accuracy: 0.7167 - mlp_test_auc_16: 0.7981\n",
      "Epoch 81/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5395 - siamese_test_loss: 0.4207 - mlp_test_loss: 0.5395 - mlp_test_binary_accuracy: 0.7151 - mlp_test_auc_16: 0.7967\n",
      "Epoch 82/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5384 - siamese_test_loss: 0.4241 - mlp_test_loss: 0.5384 - mlp_test_binary_accuracy: 0.7208 - mlp_test_auc_16: 0.7989\n",
      "Epoch 83/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5364 - siamese_test_loss: 0.4203 - mlp_test_loss: 0.5364 - mlp_test_binary_accuracy: 0.7206 - mlp_test_auc_16: 0.8002\n",
      "Epoch 84/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5378 - siamese_test_loss: 0.4257 - mlp_test_loss: 0.5378 - mlp_test_binary_accuracy: 0.7181 - mlp_test_auc_16: 0.7997\n",
      "Epoch 85/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5372 - siamese_test_loss: 0.4296 - mlp_test_loss: 0.5372 - mlp_test_binary_accuracy: 0.7176 - mlp_test_auc_16: 0.7997\n",
      "Epoch 86/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5360 - siamese_test_loss: 0.4211 - mlp_test_loss: 0.5360 - mlp_test_binary_accuracy: 0.7151 - mlp_test_auc_16: 0.7999\n",
      "Epoch 87/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5363 - siamese_test_loss: 0.4275 - mlp_test_loss: 0.5363 - mlp_test_binary_accuracy: 0.7199 - mlp_test_auc_16: 0.8003\n",
      "Epoch 88/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5365 - siamese_test_loss: 0.4265 - mlp_test_loss: 0.5365 - mlp_test_binary_accuracy: 0.7169 - mlp_test_auc_16: 0.8005\n",
      "Epoch 89/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5355 - siamese_test_loss: 0.4286 - mlp_test_loss: 0.5355 - mlp_test_binary_accuracy: 0.7183 - mlp_test_auc_16: 0.8008\n",
      "Epoch 90/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5354 - siamese_test_loss: 0.4286 - mlp_test_loss: 0.5354 - mlp_test_binary_accuracy: 0.7153 - mlp_test_auc_16: 0.8006\n",
      "Epoch 91/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5352 - siamese_test_loss: 0.4356 - mlp_test_loss: 0.5352 - mlp_test_binary_accuracy: 0.7215 - mlp_test_auc_16: 0.8012\n",
      "Epoch 92/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5324 - siamese_test_loss: 0.4336 - mlp_test_loss: 0.5324 - mlp_test_binary_accuracy: 0.7231 - mlp_test_auc_16: 0.8041\n",
      "Epoch 93/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5323 - siamese_test_loss: 0.4308 - mlp_test_loss: 0.5323 - mlp_test_binary_accuracy: 0.7245 - mlp_test_auc_16: 0.8034\n",
      "Epoch 94/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5332 - siamese_test_loss: 0.4327 - mlp_test_loss: 0.5332 - mlp_test_binary_accuracy: 0.7222 - mlp_test_auc_16: 0.8035\n",
      "Epoch 95/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5326 - siamese_test_loss: 0.4330 - mlp_test_loss: 0.5326 - mlp_test_binary_accuracy: 0.7254 - mlp_test_auc_16: 0.8041\n",
      "Epoch 96/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5315 - siamese_test_loss: 0.4296 - mlp_test_loss: 0.5315 - mlp_test_binary_accuracy: 0.7231 - mlp_test_auc_16: 0.8043\n",
      "Epoch 97/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5317 - siamese_test_loss: 0.4291 - mlp_test_loss: 0.5317 - mlp_test_binary_accuracy: 0.7231 - mlp_test_auc_16: 0.8046\n",
      "Epoch 98/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5312 - siamese_test_loss: 0.4345 - mlp_test_loss: 0.5312 - mlp_test_binary_accuracy: 0.7233 - mlp_test_auc_16: 0.8046\n",
      "Epoch 99/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5300 - siamese_test_loss: 0.4350 - mlp_test_loss: 0.5300 - mlp_test_binary_accuracy: 0.7266 - mlp_test_auc_16: 0.8057\n",
      "Epoch 100/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5309 - siamese_test_loss: 0.4363 - mlp_test_loss: 0.5309 - mlp_test_binary_accuracy: 0.7245 - mlp_test_auc_16: 0.8056\n",
      "Epoch 101/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5284 - siamese_test_loss: 0.4368 - mlp_test_loss: 0.5284 - mlp_test_binary_accuracy: 0.7247 - mlp_test_auc_16: 0.8073\n",
      "Epoch 102/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5277 - siamese_test_loss: 0.4374 - mlp_test_loss: 0.5277 - mlp_test_binary_accuracy: 0.7256 - mlp_test_auc_16: 0.8087\n",
      "Epoch 103/200\n",
      "136/136 [==============================] - 0s 941us/step - loss: 0.5303 - siamese_test_loss: 0.4378 - mlp_test_loss: 0.5303 - mlp_test_binary_accuracy: 0.7316 - mlp_test_auc_16: 0.8061\n",
      "Epoch 104/200\n",
      "136/136 [==============================] - 0s 950us/step - loss: 0.5285 - siamese_test_loss: 0.4414 - mlp_test_loss: 0.5285 - mlp_test_binary_accuracy: 0.7261 - mlp_test_auc_16: 0.8073\n",
      "Epoch 105/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5281 - siamese_test_loss: 0.4436 - mlp_test_loss: 0.5281 - mlp_test_binary_accuracy: 0.7266 - mlp_test_auc_16: 0.8075\n",
      "Epoch 106/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5274 - siamese_test_loss: 0.4434 - mlp_test_loss: 0.5274 - mlp_test_binary_accuracy: 0.7279 - mlp_test_auc_16: 0.8085\n",
      "Epoch 107/200\n",
      "136/136 [==============================] - 0s 930us/step - loss: 0.5270 - siamese_test_loss: 0.4470 - mlp_test_loss: 0.5270 - mlp_test_binary_accuracy: 0.7279 - mlp_test_auc_16: 0.8085\n",
      "Epoch 108/200\n",
      "136/136 [==============================] - 0s 928us/step - loss: 0.5272 - siamese_test_loss: 0.4482 - mlp_test_loss: 0.5272 - mlp_test_binary_accuracy: 0.7289 - mlp_test_auc_16: 0.8086\n",
      "Epoch 109/200\n",
      "136/136 [==============================] - 0s 925us/step - loss: 0.5273 - siamese_test_loss: 0.4439 - mlp_test_loss: 0.5273 - mlp_test_binary_accuracy: 0.7293 - mlp_test_auc_16: 0.8084\n",
      "Epoch 110/200\n",
      "136/136 [==============================] - 0s 921us/step - loss: 0.5247 - siamese_test_loss: 0.4503 - mlp_test_loss: 0.5247 - mlp_test_binary_accuracy: 0.7256 - mlp_test_auc_16: 0.8106\n",
      "Epoch 111/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5251 - siamese_test_loss: 0.4492 - mlp_test_loss: 0.5251 - mlp_test_binary_accuracy: 0.7247 - mlp_test_auc_16: 0.8108\n",
      "Epoch 112/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5244 - siamese_test_loss: 0.4453 - mlp_test_loss: 0.5244 - mlp_test_binary_accuracy: 0.7325 - mlp_test_auc_16: 0.8111\n",
      "Epoch 113/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5241 - siamese_test_loss: 0.4498 - mlp_test_loss: 0.5241 - mlp_test_binary_accuracy: 0.7325 - mlp_test_auc_16: 0.8111\n",
      "Epoch 114/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5225 - siamese_test_loss: 0.4491 - mlp_test_loss: 0.5225 - mlp_test_binary_accuracy: 0.7362 - mlp_test_auc_16: 0.8133\n",
      "Epoch 115/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5236 - siamese_test_loss: 0.4486 - mlp_test_loss: 0.5236 - mlp_test_binary_accuracy: 0.7335 - mlp_test_auc_16: 0.8126\n",
      "Epoch 116/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5233 - siamese_test_loss: 0.4501 - mlp_test_loss: 0.5233 - mlp_test_binary_accuracy: 0.7316 - mlp_test_auc_16: 0.8117\n",
      "Epoch 117/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5239 - siamese_test_loss: 0.4518 - mlp_test_loss: 0.5239 - mlp_test_binary_accuracy: 0.7293 - mlp_test_auc_16: 0.8116\n",
      "Epoch 118/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5224 - siamese_test_loss: 0.4507 - mlp_test_loss: 0.5224 - mlp_test_binary_accuracy: 0.7330 - mlp_test_auc_16: 0.8131\n",
      "Epoch 119/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5218 - siamese_test_loss: 0.4516 - mlp_test_loss: 0.5218 - mlp_test_binary_accuracy: 0.7335 - mlp_test_auc_16: 0.8135\n",
      "Epoch 120/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5218 - siamese_test_loss: 0.4535 - mlp_test_loss: 0.5218 - mlp_test_binary_accuracy: 0.7270 - mlp_test_auc_16: 0.8130\n",
      "Epoch 121/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5221 - siamese_test_loss: 0.4545 - mlp_test_loss: 0.5221 - mlp_test_binary_accuracy: 0.7302 - mlp_test_auc_16: 0.8130\n",
      "Epoch 122/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5199 - siamese_test_loss: 0.4604 - mlp_test_loss: 0.5199 - mlp_test_binary_accuracy: 0.7394 - mlp_test_auc_16: 0.8148\n",
      "Epoch 123/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5198 - siamese_test_loss: 0.4548 - mlp_test_loss: 0.5198 - mlp_test_binary_accuracy: 0.7383 - mlp_test_auc_16: 0.8151\n",
      "Epoch 124/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5207 - siamese_test_loss: 0.4615 - mlp_test_loss: 0.5207 - mlp_test_binary_accuracy: 0.7339 - mlp_test_auc_16: 0.8141\n",
      "Epoch 125/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5200 - siamese_test_loss: 0.4591 - mlp_test_loss: 0.5200 - mlp_test_binary_accuracy: 0.7346 - mlp_test_auc_16: 0.8153\n",
      "Epoch 126/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5204 - siamese_test_loss: 0.4590 - mlp_test_loss: 0.5204 - mlp_test_binary_accuracy: 0.7312 - mlp_test_auc_16: 0.8142\n",
      "Epoch 127/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5195 - siamese_test_loss: 0.4606 - mlp_test_loss: 0.5195 - mlp_test_binary_accuracy: 0.7321 - mlp_test_auc_16: 0.8152\n",
      "Epoch 128/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5189 - siamese_test_loss: 0.4634 - mlp_test_loss: 0.5189 - mlp_test_binary_accuracy: 0.7392 - mlp_test_auc_16: 0.8160\n",
      "Epoch 129/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5182 - siamese_test_loss: 0.4628 - mlp_test_loss: 0.5182 - mlp_test_binary_accuracy: 0.7374 - mlp_test_auc_16: 0.8162\n",
      "Epoch 130/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5178 - siamese_test_loss: 0.4650 - mlp_test_loss: 0.5178 - mlp_test_binary_accuracy: 0.7353 - mlp_test_auc_16: 0.8170\n",
      "Epoch 131/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5188 - siamese_test_loss: 0.4656 - mlp_test_loss: 0.5188 - mlp_test_binary_accuracy: 0.7369 - mlp_test_auc_16: 0.8163\n",
      "Epoch 132/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5176 - siamese_test_loss: 0.4669 - mlp_test_loss: 0.5176 - mlp_test_binary_accuracy: 0.7316 - mlp_test_auc_16: 0.8165\n",
      "Epoch 133/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5179 - siamese_test_loss: 0.4660 - mlp_test_loss: 0.5179 - mlp_test_binary_accuracy: 0.7339 - mlp_test_auc_16: 0.8161\n",
      "Epoch 134/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5177 - siamese_test_loss: 0.4653 - mlp_test_loss: 0.5177 - mlp_test_binary_accuracy: 0.7367 - mlp_test_auc_16: 0.8172\n",
      "Epoch 135/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5158 - siamese_test_loss: 0.4705 - mlp_test_loss: 0.5158 - mlp_test_binary_accuracy: 0.7369 - mlp_test_auc_16: 0.8179\n",
      "Epoch 136/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5144 - siamese_test_loss: 0.4637 - mlp_test_loss: 0.5144 - mlp_test_binary_accuracy: 0.7362 - mlp_test_auc_16: 0.8196\n",
      "Epoch 137/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5161 - siamese_test_loss: 0.4714 - mlp_test_loss: 0.5161 - mlp_test_binary_accuracy: 0.7341 - mlp_test_auc_16: 0.8182\n",
      "Epoch 138/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5147 - siamese_test_loss: 0.4732 - mlp_test_loss: 0.5147 - mlp_test_binary_accuracy: 0.7369 - mlp_test_auc_16: 0.8194\n",
      "Epoch 139/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5143 - siamese_test_loss: 0.4700 - mlp_test_loss: 0.5143 - mlp_test_binary_accuracy: 0.7429 - mlp_test_auc_16: 0.8199\n",
      "Epoch 140/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5151 - siamese_test_loss: 0.4730 - mlp_test_loss: 0.5151 - mlp_test_binary_accuracy: 0.7369 - mlp_test_auc_16: 0.8186\n",
      "Epoch 141/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5134 - siamese_test_loss: 0.4710 - mlp_test_loss: 0.5134 - mlp_test_binary_accuracy: 0.7436 - mlp_test_auc_16: 0.8204\n",
      "Epoch 142/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5125 - siamese_test_loss: 0.4705 - mlp_test_loss: 0.5125 - mlp_test_binary_accuracy: 0.7406 - mlp_test_auc_16: 0.8208\n",
      "Epoch 143/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5153 - siamese_test_loss: 0.4727 - mlp_test_loss: 0.5153 - mlp_test_binary_accuracy: 0.7351 - mlp_test_auc_16: 0.8189\n",
      "Epoch 144/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5145 - siamese_test_loss: 0.4799 - mlp_test_loss: 0.5145 - mlp_test_binary_accuracy: 0.7406 - mlp_test_auc_16: 0.8198\n",
      "Epoch 145/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5125 - siamese_test_loss: 0.4789 - mlp_test_loss: 0.5125 - mlp_test_binary_accuracy: 0.7417 - mlp_test_auc_16: 0.8213\n",
      "Epoch 146/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5114 - siamese_test_loss: 0.4766 - mlp_test_loss: 0.5114 - mlp_test_binary_accuracy: 0.7371 - mlp_test_auc_16: 0.8220\n",
      "Epoch 147/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5115 - siamese_test_loss: 0.4752 - mlp_test_loss: 0.5115 - mlp_test_binary_accuracy: 0.7403 - mlp_test_auc_16: 0.8218\n",
      "Epoch 148/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5104 - siamese_test_loss: 0.4771 - mlp_test_loss: 0.5104 - mlp_test_binary_accuracy: 0.7433 - mlp_test_auc_16: 0.8228\n",
      "Epoch 149/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5120 - siamese_test_loss: 0.4816 - mlp_test_loss: 0.5120 - mlp_test_binary_accuracy: 0.7417 - mlp_test_auc_16: 0.8215\n",
      "Epoch 150/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5117 - siamese_test_loss: 0.4813 - mlp_test_loss: 0.5117 - mlp_test_binary_accuracy: 0.7422 - mlp_test_auc_16: 0.8215\n",
      "Epoch 151/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5128 - siamese_test_loss: 0.4791 - mlp_test_loss: 0.5128 - mlp_test_binary_accuracy: 0.7335 - mlp_test_auc_16: 0.8204\n",
      "Epoch 152/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5103 - siamese_test_loss: 0.4825 - mlp_test_loss: 0.5103 - mlp_test_binary_accuracy: 0.7452 - mlp_test_auc_16: 0.8235\n",
      "Epoch 153/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5090 - siamese_test_loss: 0.4838 - mlp_test_loss: 0.5090 - mlp_test_binary_accuracy: 0.7399 - mlp_test_auc_16: 0.8241\n",
      "Epoch 154/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5105 - siamese_test_loss: 0.4837 - mlp_test_loss: 0.5105 - mlp_test_binary_accuracy: 0.7454 - mlp_test_auc_16: 0.8232\n",
      "Epoch 155/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5097 - siamese_test_loss: 0.4829 - mlp_test_loss: 0.5097 - mlp_test_binary_accuracy: 0.7397 - mlp_test_auc_16: 0.8233\n",
      "Epoch 156/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5101 - siamese_test_loss: 0.4849 - mlp_test_loss: 0.5101 - mlp_test_binary_accuracy: 0.7410 - mlp_test_auc_16: 0.8231\n",
      "Epoch 157/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5094 - siamese_test_loss: 0.4841 - mlp_test_loss: 0.5094 - mlp_test_binary_accuracy: 0.7426 - mlp_test_auc_16: 0.8235\n",
      "Epoch 158/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5089 - siamese_test_loss: 0.4855 - mlp_test_loss: 0.5089 - mlp_test_binary_accuracy: 0.7397 - mlp_test_auc_16: 0.8239\n",
      "Epoch 159/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5094 - siamese_test_loss: 0.4844 - mlp_test_loss: 0.5094 - mlp_test_binary_accuracy: 0.7397 - mlp_test_auc_16: 0.8238\n",
      "Epoch 160/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5080 - siamese_test_loss: 0.4820 - mlp_test_loss: 0.5080 - mlp_test_binary_accuracy: 0.7417 - mlp_test_auc_16: 0.8248\n",
      "Epoch 161/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5084 - siamese_test_loss: 0.4871 - mlp_test_loss: 0.5084 - mlp_test_binary_accuracy: 0.7420 - mlp_test_auc_16: 0.8240\n",
      "Epoch 162/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5080 - siamese_test_loss: 0.4829 - mlp_test_loss: 0.5080 - mlp_test_binary_accuracy: 0.7484 - mlp_test_auc_16: 0.8254\n",
      "Epoch 163/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5060 - siamese_test_loss: 0.4844 - mlp_test_loss: 0.5060 - mlp_test_binary_accuracy: 0.7468 - mlp_test_auc_16: 0.8266\n",
      "Epoch 164/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5067 - siamese_test_loss: 0.4927 - mlp_test_loss: 0.5067 - mlp_test_binary_accuracy: 0.7454 - mlp_test_auc_16: 0.8259\n",
      "Epoch 165/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5068 - siamese_test_loss: 0.4892 - mlp_test_loss: 0.5068 - mlp_test_binary_accuracy: 0.7413 - mlp_test_auc_16: 0.8258\n",
      "Epoch 166/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5056 - siamese_test_loss: 0.4958 - mlp_test_loss: 0.5056 - mlp_test_binary_accuracy: 0.7433 - mlp_test_auc_16: 0.8268\n",
      "Epoch 167/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5071 - siamese_test_loss: 0.4959 - mlp_test_loss: 0.5071 - mlp_test_binary_accuracy: 0.7431 - mlp_test_auc_16: 0.8252\n",
      "Epoch 168/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5062 - siamese_test_loss: 0.4946 - mlp_test_loss: 0.5062 - mlp_test_binary_accuracy: 0.7424 - mlp_test_auc_16: 0.8262\n",
      "Epoch 169/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5052 - siamese_test_loss: 0.4961 - mlp_test_loss: 0.5052 - mlp_test_binary_accuracy: 0.7447 - mlp_test_auc_16: 0.8269\n",
      "Epoch 170/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5077 - siamese_test_loss: 0.4990 - mlp_test_loss: 0.5077 - mlp_test_binary_accuracy: 0.7383 - mlp_test_auc_16: 0.8250\n",
      "Epoch 171/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5045 - siamese_test_loss: 0.4969 - mlp_test_loss: 0.5045 - mlp_test_binary_accuracy: 0.7477 - mlp_test_auc_16: 0.8278\n",
      "Epoch 172/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5047 - siamese_test_loss: 0.4968 - mlp_test_loss: 0.5047 - mlp_test_binary_accuracy: 0.7403 - mlp_test_auc_16: 0.8275\n",
      "Epoch 173/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5054 - siamese_test_loss: 0.4987 - mlp_test_loss: 0.5054 - mlp_test_binary_accuracy: 0.7456 - mlp_test_auc_16: 0.8266\n",
      "Epoch 174/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5037 - siamese_test_loss: 0.4986 - mlp_test_loss: 0.5037 - mlp_test_binary_accuracy: 0.7482 - mlp_test_auc_16: 0.8287\n",
      "Epoch 175/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5042 - siamese_test_loss: 0.5026 - mlp_test_loss: 0.5042 - mlp_test_binary_accuracy: 0.7449 - mlp_test_auc_16: 0.8278\n",
      "Epoch 176/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5044 - siamese_test_loss: 0.5065 - mlp_test_loss: 0.5044 - mlp_test_binary_accuracy: 0.7426 - mlp_test_auc_16: 0.8271\n",
      "Epoch 177/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5036 - siamese_test_loss: 0.5051 - mlp_test_loss: 0.5036 - mlp_test_binary_accuracy: 0.7420 - mlp_test_auc_16: 0.8276\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5046 - siamese_test_loss: 0.5046 - mlp_test_loss: 0.5046 - mlp_test_binary_accuracy: 0.7429 - mlp_test_auc_16: 0.8281\n",
      "Epoch 179/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5039 - siamese_test_loss: 0.5088 - mlp_test_loss: 0.5039 - mlp_test_binary_accuracy: 0.7440 - mlp_test_auc_16: 0.8273\n",
      "Epoch 180/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5029 - siamese_test_loss: 0.5138 - mlp_test_loss: 0.5029 - mlp_test_binary_accuracy: 0.7479 - mlp_test_auc_16: 0.8295\n",
      "Epoch 181/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5020 - siamese_test_loss: 0.5098 - mlp_test_loss: 0.5020 - mlp_test_binary_accuracy: 0.7452 - mlp_test_auc_16: 0.8294\n",
      "Epoch 182/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5007 - siamese_test_loss: 0.5123 - mlp_test_loss: 0.5007 - mlp_test_binary_accuracy: 0.7523 - mlp_test_auc_16: 0.8309\n",
      "Epoch 183/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5029 - siamese_test_loss: 0.5165 - mlp_test_loss: 0.5029 - mlp_test_binary_accuracy: 0.7468 - mlp_test_auc_16: 0.8294\n",
      "Epoch 184/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5028 - siamese_test_loss: 0.5155 - mlp_test_loss: 0.5028 - mlp_test_binary_accuracy: 0.7491 - mlp_test_auc_16: 0.8293\n",
      "Epoch 185/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5023 - siamese_test_loss: 0.5040 - mlp_test_loss: 0.5023 - mlp_test_binary_accuracy: 0.7456 - mlp_test_auc_16: 0.8289\n",
      "Epoch 186/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5014 - siamese_test_loss: 0.5178 - mlp_test_loss: 0.5014 - mlp_test_binary_accuracy: 0.7470 - mlp_test_auc_16: 0.8300\n",
      "Epoch 187/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5032 - siamese_test_loss: 0.5149 - mlp_test_loss: 0.5032 - mlp_test_binary_accuracy: 0.7475 - mlp_test_auc_16: 0.8288\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002186B974DC8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5999 - siamese_test_loss: 0.5125 - mlp_test_loss: 0.5999 - mlp_test_binary_accuracy: 0.6983 - mlp_test_auc_16: 0.7624\n",
      "------ mlp_test_binary_accuracy: 69.83%\t ----- mlp_test_auc_16: 76.24%\n",
      "concat_fea的shape： (None, 16)\n",
      "-------------------------------------Kfold: 4 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6907 - siamese_test_loss: 0.4319 - mlp_test_loss: 0.6907 - mlp_test_binary_accuracy: 0.5460 - mlp_test_auc_17: 0.5559\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6739 - siamese_test_loss: 0.3567 - mlp_test_loss: 0.6739 - mlp_test_binary_accuracy: 0.6108 - mlp_test_auc_17: 0.6598\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6490 - siamese_test_loss: 0.3622 - mlp_test_loss: 0.6490 - mlp_test_binary_accuracy: 0.6255 - mlp_test_auc_17: 0.6828\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6306 - siamese_test_loss: 0.3754 - mlp_test_loss: 0.6306 - mlp_test_binary_accuracy: 0.6425 - mlp_test_auc_17: 0.7033\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6185 - siamese_test_loss: 0.3793 - mlp_test_loss: 0.6185 - mlp_test_binary_accuracy: 0.6514 - mlp_test_auc_17: 0.7143\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6122 - siamese_test_loss: 0.3769 - mlp_test_loss: 0.6122 - mlp_test_binary_accuracy: 0.6588 - mlp_test_auc_17: 0.7213\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6086 - siamese_test_loss: 0.3753 - mlp_test_loss: 0.6086 - mlp_test_binary_accuracy: 0.6629 - mlp_test_auc_17: 0.7256\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6062 - siamese_test_loss: 0.3737 - mlp_test_loss: 0.6062 - mlp_test_binary_accuracy: 0.6581 - mlp_test_auc_17: 0.7266\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6032 - siamese_test_loss: 0.3664 - mlp_test_loss: 0.6032 - mlp_test_binary_accuracy: 0.6638 - mlp_test_auc_17: 0.7315\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6024 - siamese_test_loss: 0.3621 - mlp_test_loss: 0.6024 - mlp_test_binary_accuracy: 0.6728 - mlp_test_auc_17: 0.7309\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6001 - siamese_test_loss: 0.3618 - mlp_test_loss: 0.6001 - mlp_test_binary_accuracy: 0.6654 - mlp_test_auc_17: 0.7351\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5986 - siamese_test_loss: 0.3586 - mlp_test_loss: 0.5986 - mlp_test_binary_accuracy: 0.6682 - mlp_test_auc_17: 0.7363\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5975 - siamese_test_loss: 0.3592 - mlp_test_loss: 0.5975 - mlp_test_binary_accuracy: 0.6751 - mlp_test_auc_17: 0.7374\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5962 - siamese_test_loss: 0.3590 - mlp_test_loss: 0.5962 - mlp_test_binary_accuracy: 0.6735 - mlp_test_auc_17: 0.7397\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5961 - siamese_test_loss: 0.3565 - mlp_test_loss: 0.5961 - mlp_test_binary_accuracy: 0.6707 - mlp_test_auc_17: 0.7385\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5937 - siamese_test_loss: 0.3537 - mlp_test_loss: 0.5937 - mlp_test_binary_accuracy: 0.6749 - mlp_test_auc_17: 0.7412\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5942 - siamese_test_loss: 0.3604 - mlp_test_loss: 0.5942 - mlp_test_binary_accuracy: 0.6765 - mlp_test_auc_17: 0.7411\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5937 - siamese_test_loss: 0.3521 - mlp_test_loss: 0.5937 - mlp_test_binary_accuracy: 0.6790 - mlp_test_auc_17: 0.7415\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5918 - siamese_test_loss: 0.3558 - mlp_test_loss: 0.5918 - mlp_test_binary_accuracy: 0.6783 - mlp_test_auc_17: 0.7423\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5916 - siamese_test_loss: 0.3563 - mlp_test_loss: 0.5916 - mlp_test_binary_accuracy: 0.6730 - mlp_test_auc_17: 0.7430\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5910 - siamese_test_loss: 0.3561 - mlp_test_loss: 0.5910 - mlp_test_binary_accuracy: 0.6772 - mlp_test_auc_17: 0.7441\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5915 - siamese_test_loss: 0.3568 - mlp_test_loss: 0.5915 - mlp_test_binary_accuracy: 0.6758 - mlp_test_auc_17: 0.7432\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5906 - siamese_test_loss: 0.3536 - mlp_test_loss: 0.5906 - mlp_test_binary_accuracy: 0.6744 - mlp_test_auc_17: 0.7426\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5905 - siamese_test_loss: 0.3584 - mlp_test_loss: 0.5905 - mlp_test_binary_accuracy: 0.6710 - mlp_test_auc_17: 0.7439\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5896 - siamese_test_loss: 0.3550 - mlp_test_loss: 0.5896 - mlp_test_binary_accuracy: 0.6818 - mlp_test_auc_17: 0.7469\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5895 - siamese_test_loss: 0.3584 - mlp_test_loss: 0.5895 - mlp_test_binary_accuracy: 0.6735 - mlp_test_auc_17: 0.7448\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5883 - siamese_test_loss: 0.3560 - mlp_test_loss: 0.5883 - mlp_test_binary_accuracy: 0.6824 - mlp_test_auc_17: 0.7457\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5891 - siamese_test_loss: 0.3590 - mlp_test_loss: 0.5891 - mlp_test_binary_accuracy: 0.6781 - mlp_test_auc_17: 0.7456\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5884 - siamese_test_loss: 0.3602 - mlp_test_loss: 0.5884 - mlp_test_binary_accuracy: 0.6728 - mlp_test_auc_17: 0.7451\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5882 - siamese_test_loss: 0.3624 - mlp_test_loss: 0.5882 - mlp_test_binary_accuracy: 0.6804 - mlp_test_auc_17: 0.7475\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5887 - siamese_test_loss: 0.3561 - mlp_test_loss: 0.5887 - mlp_test_binary_accuracy: 0.6806 - mlp_test_auc_17: 0.7460\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5877 - siamese_test_loss: 0.3555 - mlp_test_loss: 0.5877 - mlp_test_binary_accuracy: 0.6781 - mlp_test_auc_17: 0.7468\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5855 - siamese_test_loss: 0.3574 - mlp_test_loss: 0.5855 - mlp_test_binary_accuracy: 0.6742 - mlp_test_auc_17: 0.7498\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5876 - siamese_test_loss: 0.3602 - mlp_test_loss: 0.5876 - mlp_test_binary_accuracy: 0.6808 - mlp_test_auc_17: 0.7479\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5876 - siamese_test_loss: 0.3586 - mlp_test_loss: 0.5876 - mlp_test_binary_accuracy: 0.6815 - mlp_test_auc_17: 0.7472\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5879 - siamese_test_loss: 0.3582 - mlp_test_loss: 0.5879 - mlp_test_binary_accuracy: 0.6769 - mlp_test_auc_17: 0.7474\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5864 - siamese_test_loss: 0.3571 - mlp_test_loss: 0.5864 - mlp_test_binary_accuracy: 0.6808 - mlp_test_auc_17: 0.7491\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5854 - siamese_test_loss: 0.3593 - mlp_test_loss: 0.5854 - mlp_test_binary_accuracy: 0.6815 - mlp_test_auc_17: 0.7503\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5871 - siamese_test_loss: 0.3567 - mlp_test_loss: 0.5871 - mlp_test_binary_accuracy: 0.6778 - mlp_test_auc_17: 0.7479\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5859 - siamese_test_loss: 0.3577 - mlp_test_loss: 0.5859 - mlp_test_binary_accuracy: 0.6815 - mlp_test_auc_17: 0.7497\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5853 - siamese_test_loss: 0.3570 - mlp_test_loss: 0.5853 - mlp_test_binary_accuracy: 0.6811 - mlp_test_auc_17: 0.7496\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5843 - siamese_test_loss: 0.3550 - mlp_test_loss: 0.5843 - mlp_test_binary_accuracy: 0.6822 - mlp_test_auc_17: 0.7516\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5857 - siamese_test_loss: 0.3575 - mlp_test_loss: 0.5857 - mlp_test_binary_accuracy: 0.6790 - mlp_test_auc_17: 0.7495\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5845 - siamese_test_loss: 0.3571 - mlp_test_loss: 0.5845 - mlp_test_binary_accuracy: 0.6808 - mlp_test_auc_17: 0.7498\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5840 - siamese_test_loss: 0.3617 - mlp_test_loss: 0.5840 - mlp_test_binary_accuracy: 0.6850 - mlp_test_auc_17: 0.7535\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5837 - siamese_test_loss: 0.3594 - mlp_test_loss: 0.5837 - mlp_test_binary_accuracy: 0.6829 - mlp_test_auc_17: 0.7517\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5833 - siamese_test_loss: 0.3590 - mlp_test_loss: 0.5833 - mlp_test_binary_accuracy: 0.6749 - mlp_test_auc_17: 0.7506\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5840 - siamese_test_loss: 0.3611 - mlp_test_loss: 0.5840 - mlp_test_binary_accuracy: 0.6790 - mlp_test_auc_17: 0.7509\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5831 - siamese_test_loss: 0.3555 - mlp_test_loss: 0.5831 - mlp_test_binary_accuracy: 0.6790 - mlp_test_auc_17: 0.7518\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5823 - siamese_test_loss: 0.3634 - mlp_test_loss: 0.5823 - mlp_test_binary_accuracy: 0.6820 - mlp_test_auc_17: 0.7532\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5823 - siamese_test_loss: 0.3598 - mlp_test_loss: 0.5823 - mlp_test_binary_accuracy: 0.6733 - mlp_test_auc_17: 0.7532\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5825 - siamese_test_loss: 0.3624 - mlp_test_loss: 0.5825 - mlp_test_binary_accuracy: 0.6808 - mlp_test_auc_17: 0.7534\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5809 - siamese_test_loss: 0.3620 - mlp_test_loss: 0.5809 - mlp_test_binary_accuracy: 0.6758 - mlp_test_auc_17: 0.7534\n",
      "Epoch 54/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5808 - siamese_test_loss: 0.3625 - mlp_test_loss: 0.5808 - mlp_test_binary_accuracy: 0.6852 - mlp_test_auc_17: 0.7548\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5801 - siamese_test_loss: 0.3617 - mlp_test_loss: 0.5801 - mlp_test_binary_accuracy: 0.6841 - mlp_test_auc_17: 0.7550\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5795 - siamese_test_loss: 0.3661 - mlp_test_loss: 0.5795 - mlp_test_binary_accuracy: 0.6857 - mlp_test_auc_17: 0.7570\n",
      "Epoch 57/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5788 - siamese_test_loss: 0.3648 - mlp_test_loss: 0.5788 - mlp_test_binary_accuracy: 0.6875 - mlp_test_auc_17: 0.7577\n",
      "Epoch 58/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5775 - siamese_test_loss: 0.3653 - mlp_test_loss: 0.5775 - mlp_test_binary_accuracy: 0.6795 - mlp_test_auc_17: 0.7578\n",
      "Epoch 59/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5786 - siamese_test_loss: 0.3702 - mlp_test_loss: 0.5786 - mlp_test_binary_accuracy: 0.6847 - mlp_test_auc_17: 0.7576\n",
      "Epoch 60/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5770 - siamese_test_loss: 0.3687 - mlp_test_loss: 0.5770 - mlp_test_binary_accuracy: 0.6868 - mlp_test_auc_17: 0.7591\n",
      "Epoch 61/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5770 - siamese_test_loss: 0.3722 - mlp_test_loss: 0.5770 - mlp_test_binary_accuracy: 0.6905 - mlp_test_auc_17: 0.7598\n",
      "Epoch 62/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5759 - siamese_test_loss: 0.3725 - mlp_test_loss: 0.5759 - mlp_test_binary_accuracy: 0.6836 - mlp_test_auc_17: 0.7595\n",
      "Epoch 63/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5765 - siamese_test_loss: 0.3733 - mlp_test_loss: 0.5765 - mlp_test_binary_accuracy: 0.6873 - mlp_test_auc_17: 0.7598\n",
      "Epoch 64/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5750 - siamese_test_loss: 0.3757 - mlp_test_loss: 0.5750 - mlp_test_binary_accuracy: 0.6893 - mlp_test_auc_17: 0.7615\n",
      "Epoch 65/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5735 - siamese_test_loss: 0.3745 - mlp_test_loss: 0.5735 - mlp_test_binary_accuracy: 0.6942 - mlp_test_auc_17: 0.7640\n",
      "Epoch 66/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5744 - siamese_test_loss: 0.3816 - mlp_test_loss: 0.5744 - mlp_test_binary_accuracy: 0.6882 - mlp_test_auc_17: 0.7624\n",
      "Epoch 67/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5733 - siamese_test_loss: 0.3750 - mlp_test_loss: 0.5733 - mlp_test_binary_accuracy: 0.6914 - mlp_test_auc_17: 0.7633\n",
      "Epoch 68/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5729 - siamese_test_loss: 0.3850 - mlp_test_loss: 0.5729 - mlp_test_binary_accuracy: 0.6861 - mlp_test_auc_17: 0.7647\n",
      "Epoch 69/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5719 - siamese_test_loss: 0.3784 - mlp_test_loss: 0.5719 - mlp_test_binary_accuracy: 0.6852 - mlp_test_auc_17: 0.7642\n",
      "Epoch 70/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5719 - siamese_test_loss: 0.3794 - mlp_test_loss: 0.5719 - mlp_test_binary_accuracy: 0.6880 - mlp_test_auc_17: 0.7646\n",
      "Epoch 71/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5708 - siamese_test_loss: 0.3833 - mlp_test_loss: 0.5708 - mlp_test_binary_accuracy: 0.6909 - mlp_test_auc_17: 0.7662\n",
      "Epoch 72/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5701 - siamese_test_loss: 0.3801 - mlp_test_loss: 0.5701 - mlp_test_binary_accuracy: 0.6870 - mlp_test_auc_17: 0.7670\n",
      "Epoch 73/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5690 - siamese_test_loss: 0.3849 - mlp_test_loss: 0.5690 - mlp_test_binary_accuracy: 0.6953 - mlp_test_auc_17: 0.7693\n",
      "Epoch 74/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5697 - siamese_test_loss: 0.3805 - mlp_test_loss: 0.5697 - mlp_test_binary_accuracy: 0.6854 - mlp_test_auc_17: 0.7684\n",
      "Epoch 75/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5689 - siamese_test_loss: 0.3822 - mlp_test_loss: 0.5689 - mlp_test_binary_accuracy: 0.6928 - mlp_test_auc_17: 0.7692\n",
      "Epoch 76/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5667 - siamese_test_loss: 0.3856 - mlp_test_loss: 0.5667 - mlp_test_binary_accuracy: 0.6965 - mlp_test_auc_17: 0.7708\n",
      "Epoch 77/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5664 - siamese_test_loss: 0.3826 - mlp_test_loss: 0.5664 - mlp_test_binary_accuracy: 0.6921 - mlp_test_auc_17: 0.7729\n",
      "Epoch 78/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5670 - siamese_test_loss: 0.3873 - mlp_test_loss: 0.5670 - mlp_test_binary_accuracy: 0.6953 - mlp_test_auc_17: 0.7715\n",
      "Epoch 79/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5646 - siamese_test_loss: 0.3846 - mlp_test_loss: 0.5646 - mlp_test_binary_accuracy: 0.6953 - mlp_test_auc_17: 0.7737\n",
      "Epoch 80/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5642 - siamese_test_loss: 0.3872 - mlp_test_loss: 0.5642 - mlp_test_binary_accuracy: 0.6926 - mlp_test_auc_17: 0.7741\n",
      "Epoch 81/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5652 - siamese_test_loss: 0.3892 - mlp_test_loss: 0.5652 - mlp_test_binary_accuracy: 0.6944 - mlp_test_auc_17: 0.7730\n",
      "Epoch 82/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5637 - siamese_test_loss: 0.3875 - mlp_test_loss: 0.5637 - mlp_test_binary_accuracy: 0.7001 - mlp_test_auc_17: 0.7750\n",
      "Epoch 83/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5635 - siamese_test_loss: 0.3900 - mlp_test_loss: 0.5635 - mlp_test_binary_accuracy: 0.6969 - mlp_test_auc_17: 0.7759\n",
      "Epoch 84/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5622 - siamese_test_loss: 0.3897 - mlp_test_loss: 0.5622 - mlp_test_binary_accuracy: 0.6999 - mlp_test_auc_17: 0.7764\n",
      "Epoch 85/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5611 - siamese_test_loss: 0.3923 - mlp_test_loss: 0.5611 - mlp_test_binary_accuracy: 0.6997 - mlp_test_auc_17: 0.7765\n",
      "Epoch 86/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5614 - siamese_test_loss: 0.4018 - mlp_test_loss: 0.5614 - mlp_test_binary_accuracy: 0.6997 - mlp_test_auc_17: 0.7762\n",
      "Epoch 87/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5598 - siamese_test_loss: 0.3947 - mlp_test_loss: 0.5598 - mlp_test_binary_accuracy: 0.7020 - mlp_test_auc_17: 0.7786\n",
      "Epoch 88/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5600 - siamese_test_loss: 0.3969 - mlp_test_loss: 0.5600 - mlp_test_binary_accuracy: 0.7029 - mlp_test_auc_17: 0.7780\n",
      "Epoch 89/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5584 - siamese_test_loss: 0.3960 - mlp_test_loss: 0.5584 - mlp_test_binary_accuracy: 0.7024 - mlp_test_auc_17: 0.7808\n",
      "Epoch 90/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5580 - siamese_test_loss: 0.3989 - mlp_test_loss: 0.5580 - mlp_test_binary_accuracy: 0.6990 - mlp_test_auc_17: 0.7804\n",
      "Epoch 91/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5583 - siamese_test_loss: 0.4060 - mlp_test_loss: 0.5583 - mlp_test_binary_accuracy: 0.6999 - mlp_test_auc_17: 0.7800\n",
      "Epoch 92/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5560 - siamese_test_loss: 0.4036 - mlp_test_loss: 0.5560 - mlp_test_binary_accuracy: 0.7036 - mlp_test_auc_17: 0.7825\n",
      "Epoch 93/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5562 - siamese_test_loss: 0.4107 - mlp_test_loss: 0.5562 - mlp_test_binary_accuracy: 0.6997 - mlp_test_auc_17: 0.7823\n",
      "Epoch 94/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5549 - siamese_test_loss: 0.4162 - mlp_test_loss: 0.5549 - mlp_test_binary_accuracy: 0.7022 - mlp_test_auc_17: 0.7830\n",
      "Epoch 95/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5537 - siamese_test_loss: 0.4147 - mlp_test_loss: 0.5537 - mlp_test_binary_accuracy: 0.7047 - mlp_test_auc_17: 0.7844\n",
      "Epoch 96/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5522 - siamese_test_loss: 0.4165 - mlp_test_loss: 0.5522 - mlp_test_binary_accuracy: 0.7082 - mlp_test_auc_17: 0.7865\n",
      "Epoch 97/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5540 - siamese_test_loss: 0.4223 - mlp_test_loss: 0.5540 - mlp_test_binary_accuracy: 0.7086 - mlp_test_auc_17: 0.7852\n",
      "Epoch 98/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5519 - siamese_test_loss: 0.4249 - mlp_test_loss: 0.5519 - mlp_test_binary_accuracy: 0.7022 - mlp_test_auc_17: 0.7859\n",
      "Epoch 99/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5516 - siamese_test_loss: 0.4234 - mlp_test_loss: 0.5516 - mlp_test_binary_accuracy: 0.7054 - mlp_test_auc_17: 0.7867\n",
      "Epoch 100/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5515 - siamese_test_loss: 0.4331 - mlp_test_loss: 0.5515 - mlp_test_binary_accuracy: 0.7070 - mlp_test_auc_17: 0.7865\n",
      "Epoch 101/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5504 - siamese_test_loss: 0.4338 - mlp_test_loss: 0.5504 - mlp_test_binary_accuracy: 0.7091 - mlp_test_auc_17: 0.7877\n",
      "Epoch 102/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5503 - siamese_test_loss: 0.4418 - mlp_test_loss: 0.5503 - mlp_test_binary_accuracy: 0.7038 - mlp_test_auc_17: 0.7885\n",
      "Epoch 103/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5491 - siamese_test_loss: 0.4488 - mlp_test_loss: 0.5491 - mlp_test_binary_accuracy: 0.7100 - mlp_test_auc_17: 0.7897\n",
      "Epoch 104/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5489 - siamese_test_loss: 0.4482 - mlp_test_loss: 0.5489 - mlp_test_binary_accuracy: 0.7052 - mlp_test_auc_17: 0.7896\n",
      "Epoch 105/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5475 - siamese_test_loss: 0.4469 - mlp_test_loss: 0.5475 - mlp_test_binary_accuracy: 0.7070 - mlp_test_auc_17: 0.7898\n",
      "Epoch 106/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5472 - siamese_test_loss: 0.4575 - mlp_test_loss: 0.5472 - mlp_test_binary_accuracy: 0.7070 - mlp_test_auc_17: 0.7903\n",
      "Epoch 107/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5460 - siamese_test_loss: 0.4600 - mlp_test_loss: 0.5460 - mlp_test_binary_accuracy: 0.7105 - mlp_test_auc_17: 0.7921\n",
      "Epoch 108/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5463 - siamese_test_loss: 0.4567 - mlp_test_loss: 0.5463 - mlp_test_binary_accuracy: 0.7121 - mlp_test_auc_17: 0.7911\n",
      "Epoch 109/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5458 - siamese_test_loss: 0.4684 - mlp_test_loss: 0.5458 - mlp_test_binary_accuracy: 0.7128 - mlp_test_auc_17: 0.7930\n",
      "Epoch 110/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5432 - siamese_test_loss: 0.4650 - mlp_test_loss: 0.5432 - mlp_test_binary_accuracy: 0.7190 - mlp_test_auc_17: 0.7939\n",
      "Epoch 111/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5428 - siamese_test_loss: 0.4660 - mlp_test_loss: 0.5428 - mlp_test_binary_accuracy: 0.7144 - mlp_test_auc_17: 0.7947\n",
      "Epoch 112/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5436 - siamese_test_loss: 0.4690 - mlp_test_loss: 0.5436 - mlp_test_binary_accuracy: 0.7137 - mlp_test_auc_17: 0.7948\n",
      "Epoch 113/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5431 - siamese_test_loss: 0.4724 - mlp_test_loss: 0.5431 - mlp_test_binary_accuracy: 0.7160 - mlp_test_auc_17: 0.7945\n",
      "Epoch 114/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5418 - siamese_test_loss: 0.4797 - mlp_test_loss: 0.5418 - mlp_test_binary_accuracy: 0.7114 - mlp_test_auc_17: 0.7952\n",
      "Epoch 115/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5418 - siamese_test_loss: 0.4752 - mlp_test_loss: 0.5418 - mlp_test_binary_accuracy: 0.7171 - mlp_test_auc_17: 0.7962\n",
      "Epoch 116/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5402 - siamese_test_loss: 0.4796 - mlp_test_loss: 0.5402 - mlp_test_binary_accuracy: 0.7162 - mlp_test_auc_17: 0.7977\n",
      "Epoch 117/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5399 - siamese_test_loss: 0.4822 - mlp_test_loss: 0.5399 - mlp_test_binary_accuracy: 0.7169 - mlp_test_auc_17: 0.7984\n",
      "Epoch 118/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5386 - siamese_test_loss: 0.4888 - mlp_test_loss: 0.5386 - mlp_test_binary_accuracy: 0.7194 - mlp_test_auc_17: 0.7997\n",
      "Epoch 119/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5387 - siamese_test_loss: 0.5013 - mlp_test_loss: 0.5387 - mlp_test_binary_accuracy: 0.7176 - mlp_test_auc_17: 0.7994\n",
      "Epoch 120/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5376 - siamese_test_loss: 0.4954 - mlp_test_loss: 0.5376 - mlp_test_binary_accuracy: 0.7188 - mlp_test_auc_17: 0.7993\n",
      "Epoch 121/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5369 - siamese_test_loss: 0.5082 - mlp_test_loss: 0.5369 - mlp_test_binary_accuracy: 0.7208 - mlp_test_auc_17: 0.8005\n",
      "Epoch 122/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5364 - siamese_test_loss: 0.5026 - mlp_test_loss: 0.5364 - mlp_test_binary_accuracy: 0.7192 - mlp_test_auc_17: 0.8010\n",
      "Epoch 123/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5357 - siamese_test_loss: 0.5042 - mlp_test_loss: 0.5357 - mlp_test_binary_accuracy: 0.7227 - mlp_test_auc_17: 0.8010\n",
      "Epoch 124/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5356 - siamese_test_loss: 0.5101 - mlp_test_loss: 0.5356 - mlp_test_binary_accuracy: 0.7197 - mlp_test_auc_17: 0.8021\n",
      "Epoch 125/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5350 - siamese_test_loss: 0.5148 - mlp_test_loss: 0.5350 - mlp_test_binary_accuracy: 0.7178 - mlp_test_auc_17: 0.8018\n",
      "Epoch 126/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5340 - siamese_test_loss: 0.5170 - mlp_test_loss: 0.5340 - mlp_test_binary_accuracy: 0.7165 - mlp_test_auc_17: 0.8017\n",
      "Epoch 127/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5309 - siamese_test_loss: 0.5179 - mlp_test_loss: 0.5309 - mlp_test_binary_accuracy: 0.7252 - mlp_test_auc_17: 0.8056\n",
      "Epoch 128/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5328 - siamese_test_loss: 0.5193 - mlp_test_loss: 0.5328 - mlp_test_binary_accuracy: 0.7233 - mlp_test_auc_17: 0.8036\n",
      "Epoch 129/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5323 - siamese_test_loss: 0.5241 - mlp_test_loss: 0.5323 - mlp_test_binary_accuracy: 0.7254 - mlp_test_auc_17: 0.8039\n",
      "Epoch 130/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5318 - siamese_test_loss: 0.5251 - mlp_test_loss: 0.5318 - mlp_test_binary_accuracy: 0.7236 - mlp_test_auc_17: 0.8048\n",
      "Epoch 131/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5312 - siamese_test_loss: 0.5261 - mlp_test_loss: 0.5312 - mlp_test_binary_accuracy: 0.7250 - mlp_test_auc_17: 0.8055\n",
      "Epoch 132/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5309 - siamese_test_loss: 0.5313 - mlp_test_loss: 0.5309 - mlp_test_binary_accuracy: 0.7245 - mlp_test_auc_17: 0.8051\n",
      "Epoch 133/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5290 - siamese_test_loss: 0.5324 - mlp_test_loss: 0.5290 - mlp_test_binary_accuracy: 0.7233 - mlp_test_auc_17: 0.8068\n",
      "Epoch 134/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5295 - siamese_test_loss: 0.5346 - mlp_test_loss: 0.5295 - mlp_test_binary_accuracy: 0.7275 - mlp_test_auc_17: 0.8063\n",
      "Epoch 135/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5291 - siamese_test_loss: 0.5367 - mlp_test_loss: 0.5291 - mlp_test_binary_accuracy: 0.7275 - mlp_test_auc_17: 0.8070\n",
      "Epoch 136/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5279 - siamese_test_loss: 0.5371 - mlp_test_loss: 0.5279 - mlp_test_binary_accuracy: 0.7247 - mlp_test_auc_17: 0.8073\n",
      "Epoch 137/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5272 - siamese_test_loss: 0.5413 - mlp_test_loss: 0.5272 - mlp_test_binary_accuracy: 0.7286 - mlp_test_auc_17: 0.8091\n",
      "Epoch 138/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5272 - siamese_test_loss: 0.5426 - mlp_test_loss: 0.5272 - mlp_test_binary_accuracy: 0.7256 - mlp_test_auc_17: 0.8089\n",
      "Epoch 139/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5270 - siamese_test_loss: 0.5445 - mlp_test_loss: 0.5270 - mlp_test_binary_accuracy: 0.7240 - mlp_test_auc_17: 0.8077\n",
      "Epoch 140/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5261 - siamese_test_loss: 0.5453 - mlp_test_loss: 0.5261 - mlp_test_binary_accuracy: 0.7312 - mlp_test_auc_17: 0.8097\n",
      "Epoch 141/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5258 - siamese_test_loss: 0.5429 - mlp_test_loss: 0.5258 - mlp_test_binary_accuracy: 0.7222 - mlp_test_auc_17: 0.8092\n",
      "Epoch 142/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5259 - siamese_test_loss: 0.5488 - mlp_test_loss: 0.5259 - mlp_test_binary_accuracy: 0.7325 - mlp_test_auc_17: 0.8090\n",
      "Epoch 143/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5260 - siamese_test_loss: 0.5547 - mlp_test_loss: 0.5260 - mlp_test_binary_accuracy: 0.7305 - mlp_test_auc_17: 0.8091\n",
      "Epoch 144/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5245 - siamese_test_loss: 0.5552 - mlp_test_loss: 0.5245 - mlp_test_binary_accuracy: 0.7312 - mlp_test_auc_17: 0.8113\n",
      "Epoch 145/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5251 - siamese_test_loss: 0.5574 - mlp_test_loss: 0.5251 - mlp_test_binary_accuracy: 0.7286 - mlp_test_auc_17: 0.8101\n",
      "Epoch 146/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5229 - siamese_test_loss: 0.5554 - mlp_test_loss: 0.5229 - mlp_test_binary_accuracy: 0.7330 - mlp_test_auc_17: 0.8116\n",
      "Epoch 147/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5236 - siamese_test_loss: 0.5586 - mlp_test_loss: 0.5236 - mlp_test_binary_accuracy: 0.7358 - mlp_test_auc_17: 0.8115\n",
      "Epoch 148/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5241 - siamese_test_loss: 0.5618 - mlp_test_loss: 0.5241 - mlp_test_binary_accuracy: 0.7291 - mlp_test_auc_17: 0.8110\n",
      "Epoch 149/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5239 - siamese_test_loss: 0.5595 - mlp_test_loss: 0.5239 - mlp_test_binary_accuracy: 0.7312 - mlp_test_auc_17: 0.8113\n",
      "Epoch 150/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5211 - siamese_test_loss: 0.5668 - mlp_test_loss: 0.5211 - mlp_test_binary_accuracy: 0.7374 - mlp_test_auc_17: 0.8140\n",
      "Epoch 151/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5213 - siamese_test_loss: 0.5668 - mlp_test_loss: 0.5213 - mlp_test_binary_accuracy: 0.7337 - mlp_test_auc_17: 0.8129\n",
      "Epoch 152/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5220 - siamese_test_loss: 0.5713 - mlp_test_loss: 0.5220 - mlp_test_binary_accuracy: 0.7328 - mlp_test_auc_17: 0.8124\n",
      "Epoch 153/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5208 - siamese_test_loss: 0.5677 - mlp_test_loss: 0.5208 - mlp_test_binary_accuracy: 0.7364 - mlp_test_auc_17: 0.8134\n",
      "Epoch 154/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5217 - siamese_test_loss: 0.5691 - mlp_test_loss: 0.5217 - mlp_test_binary_accuracy: 0.7353 - mlp_test_auc_17: 0.8130\n",
      "Epoch 155/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5194 - siamese_test_loss: 0.5816 - mlp_test_loss: 0.5194 - mlp_test_binary_accuracy: 0.7360 - mlp_test_auc_17: 0.8151\n",
      "Epoch 156/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5196 - siamese_test_loss: 0.5798 - mlp_test_loss: 0.5196 - mlp_test_binary_accuracy: 0.7376 - mlp_test_auc_17: 0.8145\n",
      "Epoch 157/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5201 - siamese_test_loss: 0.5791 - mlp_test_loss: 0.5201 - mlp_test_binary_accuracy: 0.7335 - mlp_test_auc_17: 0.8132\n",
      "Epoch 158/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5210 - siamese_test_loss: 0.5795 - mlp_test_loss: 0.5210 - mlp_test_binary_accuracy: 0.7328 - mlp_test_auc_17: 0.8133\n",
      "Epoch 159/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5195 - siamese_test_loss: 0.5790 - mlp_test_loss: 0.5195 - mlp_test_binary_accuracy: 0.7355 - mlp_test_auc_17: 0.8138\n",
      "Epoch 160/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5167 - siamese_test_loss: 0.5780 - mlp_test_loss: 0.5167 - mlp_test_binary_accuracy: 0.7344 - mlp_test_auc_17: 0.8168\n",
      "Epoch 161/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5184 - siamese_test_loss: 0.5783 - mlp_test_loss: 0.5184 - mlp_test_binary_accuracy: 0.7369 - mlp_test_auc_17: 0.8158\n",
      "Epoch 162/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5176 - siamese_test_loss: 0.5818 - mlp_test_loss: 0.5176 - mlp_test_binary_accuracy: 0.7374 - mlp_test_auc_17: 0.8151\n",
      "Epoch 163/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5150 - siamese_test_loss: 0.5873 - mlp_test_loss: 0.5150 - mlp_test_binary_accuracy: 0.7387 - mlp_test_auc_17: 0.8189\n",
      "Epoch 164/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5182 - siamese_test_loss: 0.5856 - mlp_test_loss: 0.5182 - mlp_test_binary_accuracy: 0.7362 - mlp_test_auc_17: 0.8161\n",
      "Epoch 165/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5171 - siamese_test_loss: 0.5885 - mlp_test_loss: 0.5171 - mlp_test_binary_accuracy: 0.7371 - mlp_test_auc_17: 0.8163\n",
      "Epoch 166/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5172 - siamese_test_loss: 0.5809 - mlp_test_loss: 0.5172 - mlp_test_binary_accuracy: 0.7378 - mlp_test_auc_17: 0.8166\n",
      "Epoch 167/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5163 - siamese_test_loss: 0.5837 - mlp_test_loss: 0.5163 - mlp_test_binary_accuracy: 0.7417 - mlp_test_auc_17: 0.8178\n",
      "Epoch 168/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5166 - siamese_test_loss: 0.5877 - mlp_test_loss: 0.5166 - mlp_test_binary_accuracy: 0.7367 - mlp_test_auc_17: 0.8170\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002185CF56048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5713 - siamese_test_loss: 0.6169 - mlp_test_loss: 0.5713 - mlp_test_binary_accuracy: 0.7128 - mlp_test_auc_17: 0.7786\n",
      "------ mlp_test_binary_accuracy: 71.28%\t ----- mlp_test_auc_17: 77.86%\n",
      "concat_fea的shape： (None, 16)\n",
      "-------------------------------------Kfold: 5 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6862 - siamese_test_loss: 0.4268 - mlp_test_loss: 0.6862 - mlp_test_binary_accuracy: 0.5485 - mlp_test_auc_18: 0.5774\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6634 - siamese_test_loss: 0.3561 - mlp_test_loss: 0.6634 - mlp_test_binary_accuracy: 0.6050 - mlp_test_auc_18: 0.6518\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6451 - siamese_test_loss: 0.3468 - mlp_test_loss: 0.6451 - mlp_test_binary_accuracy: 0.6342 - mlp_test_auc_18: 0.6762\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6313 - siamese_test_loss: 0.3510 - mlp_test_loss: 0.6313 - mlp_test_binary_accuracy: 0.6376 - mlp_test_auc_18: 0.6956\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6193 - siamese_test_loss: 0.3594 - mlp_test_loss: 0.6193 - mlp_test_binary_accuracy: 0.6441 - mlp_test_auc_18: 0.7107\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6123 - siamese_test_loss: 0.3738 - mlp_test_loss: 0.6123 - mlp_test_binary_accuracy: 0.6523 - mlp_test_auc_18: 0.7183\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6089 - siamese_test_loss: 0.3727 - mlp_test_loss: 0.6089 - mlp_test_binary_accuracy: 0.6556 - mlp_test_auc_18: 0.7224\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6051 - siamese_test_loss: 0.3689 - mlp_test_loss: 0.6051 - mlp_test_binary_accuracy: 0.6592 - mlp_test_auc_18: 0.7270\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6018 - siamese_test_loss: 0.3753 - mlp_test_loss: 0.6018 - mlp_test_binary_accuracy: 0.6684 - mlp_test_auc_18: 0.7318\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6010 - siamese_test_loss: 0.3625 - mlp_test_loss: 0.6010 - mlp_test_binary_accuracy: 0.6645 - mlp_test_auc_18: 0.7309\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5997 - siamese_test_loss: 0.3702 - mlp_test_loss: 0.5997 - mlp_test_binary_accuracy: 0.6645 - mlp_test_auc_18: 0.7321\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5977 - siamese_test_loss: 0.3657 - mlp_test_loss: 0.5977 - mlp_test_binary_accuracy: 0.6737 - mlp_test_auc_18: 0.7362\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5974 - siamese_test_loss: 0.3705 - mlp_test_loss: 0.5974 - mlp_test_binary_accuracy: 0.6638 - mlp_test_auc_18: 0.7352\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5956 - siamese_test_loss: 0.3677 - mlp_test_loss: 0.5956 - mlp_test_binary_accuracy: 0.6728 - mlp_test_auc_18: 0.7380\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5962 - siamese_test_loss: 0.3647 - mlp_test_loss: 0.5962 - mlp_test_binary_accuracy: 0.6735 - mlp_test_auc_18: 0.7380\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5946 - siamese_test_loss: 0.3687 - mlp_test_loss: 0.5946 - mlp_test_binary_accuracy: 0.6700 - mlp_test_auc_18: 0.7405\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5950 - siamese_test_loss: 0.3680 - mlp_test_loss: 0.5950 - mlp_test_binary_accuracy: 0.6661 - mlp_test_auc_18: 0.7376\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5948 - siamese_test_loss: 0.3697 - mlp_test_loss: 0.5948 - mlp_test_binary_accuracy: 0.6742 - mlp_test_auc_18: 0.7402\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5939 - siamese_test_loss: 0.3690 - mlp_test_loss: 0.5939 - mlp_test_binary_accuracy: 0.6710 - mlp_test_auc_18: 0.7409\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5926 - siamese_test_loss: 0.3668 - mlp_test_loss: 0.5926 - mlp_test_binary_accuracy: 0.6716 - mlp_test_auc_18: 0.7424\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5927 - siamese_test_loss: 0.3657 - mlp_test_loss: 0.5927 - mlp_test_binary_accuracy: 0.6746 - mlp_test_auc_18: 0.7425\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5931 - siamese_test_loss: 0.3700 - mlp_test_loss: 0.5931 - mlp_test_binary_accuracy: 0.6744 - mlp_test_auc_18: 0.7410\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5919 - siamese_test_loss: 0.3668 - mlp_test_loss: 0.5919 - mlp_test_binary_accuracy: 0.6723 - mlp_test_auc_18: 0.7421\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5916 - siamese_test_loss: 0.3684 - mlp_test_loss: 0.5916 - mlp_test_binary_accuracy: 0.6733 - mlp_test_auc_18: 0.7426\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5907 - siamese_test_loss: 0.3680 - mlp_test_loss: 0.5907 - mlp_test_binary_accuracy: 0.6756 - mlp_test_auc_18: 0.7450\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5900 - siamese_test_loss: 0.3687 - mlp_test_loss: 0.5900 - mlp_test_binary_accuracy: 0.6728 - mlp_test_auc_18: 0.7450\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5901 - siamese_test_loss: 0.3658 - mlp_test_loss: 0.5901 - mlp_test_binary_accuracy: 0.6737 - mlp_test_auc_18: 0.7441\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5898 - siamese_test_loss: 0.3693 - mlp_test_loss: 0.5898 - mlp_test_binary_accuracy: 0.6726 - mlp_test_auc_18: 0.7443\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5897 - siamese_test_loss: 0.3661 - mlp_test_loss: 0.5897 - mlp_test_binary_accuracy: 0.6733 - mlp_test_auc_18: 0.7455\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5886 - siamese_test_loss: 0.3677 - mlp_test_loss: 0.5886 - mlp_test_binary_accuracy: 0.6765 - mlp_test_auc_18: 0.7473\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5890 - siamese_test_loss: 0.3659 - mlp_test_loss: 0.5890 - mlp_test_binary_accuracy: 0.6815 - mlp_test_auc_18: 0.7455\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5887 - siamese_test_loss: 0.3665 - mlp_test_loss: 0.5887 - mlp_test_binary_accuracy: 0.6804 - mlp_test_auc_18: 0.7469\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5870 - siamese_test_loss: 0.3621 - mlp_test_loss: 0.5870 - mlp_test_binary_accuracy: 0.6797 - mlp_test_auc_18: 0.7501\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5870 - siamese_test_loss: 0.3661 - mlp_test_loss: 0.5870 - mlp_test_binary_accuracy: 0.6781 - mlp_test_auc_18: 0.7482\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5872 - siamese_test_loss: 0.3661 - mlp_test_loss: 0.5872 - mlp_test_binary_accuracy: 0.6797 - mlp_test_auc_18: 0.7487\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5872 - siamese_test_loss: 0.3657 - mlp_test_loss: 0.5872 - mlp_test_binary_accuracy: 0.6760 - mlp_test_auc_18: 0.7481\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5862 - siamese_test_loss: 0.3652 - mlp_test_loss: 0.5862 - mlp_test_binary_accuracy: 0.6762 - mlp_test_auc_18: 0.7494\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5854 - siamese_test_loss: 0.3655 - mlp_test_loss: 0.5854 - mlp_test_binary_accuracy: 0.6772 - mlp_test_auc_18: 0.7498\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5847 - siamese_test_loss: 0.3686 - mlp_test_loss: 0.5847 - mlp_test_binary_accuracy: 0.6774 - mlp_test_auc_18: 0.7507\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5853 - siamese_test_loss: 0.3677 - mlp_test_loss: 0.5853 - mlp_test_binary_accuracy: 0.6762 - mlp_test_auc_18: 0.7497\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5840 - siamese_test_loss: 0.3649 - mlp_test_loss: 0.5840 - mlp_test_binary_accuracy: 0.6758 - mlp_test_auc_18: 0.7521\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5827 - siamese_test_loss: 0.3692 - mlp_test_loss: 0.5827 - mlp_test_binary_accuracy: 0.6813 - mlp_test_auc_18: 0.7538\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5828 - siamese_test_loss: 0.3668 - mlp_test_loss: 0.5828 - mlp_test_binary_accuracy: 0.6797 - mlp_test_auc_18: 0.7528\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5819 - siamese_test_loss: 0.3713 - mlp_test_loss: 0.5819 - mlp_test_binary_accuracy: 0.6811 - mlp_test_auc_18: 0.7537\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5795 - siamese_test_loss: 0.3701 - mlp_test_loss: 0.5795 - mlp_test_binary_accuracy: 0.6769 - mlp_test_auc_18: 0.7558\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5804 - siamese_test_loss: 0.3702 - mlp_test_loss: 0.5804 - mlp_test_binary_accuracy: 0.6783 - mlp_test_auc_18: 0.7553\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5791 - siamese_test_loss: 0.3715 - mlp_test_loss: 0.5791 - mlp_test_binary_accuracy: 0.6841 - mlp_test_auc_18: 0.7579\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5792 - siamese_test_loss: 0.3753 - mlp_test_loss: 0.5792 - mlp_test_binary_accuracy: 0.6845 - mlp_test_auc_18: 0.7577\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5761 - siamese_test_loss: 0.3743 - mlp_test_loss: 0.5761 - mlp_test_binary_accuracy: 0.6893 - mlp_test_auc_18: 0.7603\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5761 - siamese_test_loss: 0.3870 - mlp_test_loss: 0.5761 - mlp_test_binary_accuracy: 0.6866 - mlp_test_auc_18: 0.7609\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5759 - siamese_test_loss: 0.3839 - mlp_test_loss: 0.5759 - mlp_test_binary_accuracy: 0.6845 - mlp_test_auc_18: 0.7616\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5747 - siamese_test_loss: 0.3865 - mlp_test_loss: 0.5747 - mlp_test_binary_accuracy: 0.6880 - mlp_test_auc_18: 0.7628\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5734 - siamese_test_loss: 0.3905 - mlp_test_loss: 0.5734 - mlp_test_binary_accuracy: 0.6850 - mlp_test_auc_18: 0.7646\n",
      "Epoch 54/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5720 - siamese_test_loss: 0.3892 - mlp_test_loss: 0.5720 - mlp_test_binary_accuracy: 0.6909 - mlp_test_auc_18: 0.7655\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5725 - siamese_test_loss: 0.3955 - mlp_test_loss: 0.5725 - mlp_test_binary_accuracy: 0.6928 - mlp_test_auc_18: 0.7669\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5698 - siamese_test_loss: 0.4032 - mlp_test_loss: 0.5698 - mlp_test_binary_accuracy: 0.6951 - mlp_test_auc_18: 0.7681\n",
      "Epoch 57/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5698 - siamese_test_loss: 0.4043 - mlp_test_loss: 0.5698 - mlp_test_binary_accuracy: 0.6889 - mlp_test_auc_18: 0.7683\n",
      "Epoch 58/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5674 - siamese_test_loss: 0.4062 - mlp_test_loss: 0.5674 - mlp_test_binary_accuracy: 0.6923 - mlp_test_auc_18: 0.7709\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5694 - siamese_test_loss: 0.4105 - mlp_test_loss: 0.5694 - mlp_test_binary_accuracy: 0.6962 - mlp_test_auc_18: 0.7691\n",
      "Epoch 60/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5676 - siamese_test_loss: 0.4061 - mlp_test_loss: 0.5676 - mlp_test_binary_accuracy: 0.6900 - mlp_test_auc_18: 0.7714\n",
      "Epoch 61/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5673 - siamese_test_loss: 0.4112 - mlp_test_loss: 0.5673 - mlp_test_binary_accuracy: 0.6930 - mlp_test_auc_18: 0.7715\n",
      "Epoch 62/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5665 - siamese_test_loss: 0.4135 - mlp_test_loss: 0.5665 - mlp_test_binary_accuracy: 0.6928 - mlp_test_auc_18: 0.7715\n",
      "Epoch 63/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5647 - siamese_test_loss: 0.4206 - mlp_test_loss: 0.5647 - mlp_test_binary_accuracy: 0.6992 - mlp_test_auc_18: 0.7739\n",
      "Epoch 64/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5649 - siamese_test_loss: 0.4210 - mlp_test_loss: 0.5649 - mlp_test_binary_accuracy: 0.6953 - mlp_test_auc_18: 0.7738\n",
      "Epoch 65/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5644 - siamese_test_loss: 0.4199 - mlp_test_loss: 0.5644 - mlp_test_binary_accuracy: 0.6946 - mlp_test_auc_18: 0.7739\n",
      "Epoch 66/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5640 - siamese_test_loss: 0.4294 - mlp_test_loss: 0.5640 - mlp_test_binary_accuracy: 0.6914 - mlp_test_auc_18: 0.7728\n",
      "Epoch 67/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5629 - siamese_test_loss: 0.4329 - mlp_test_loss: 0.5629 - mlp_test_binary_accuracy: 0.7001 - mlp_test_auc_18: 0.7767\n",
      "Epoch 68/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5620 - siamese_test_loss: 0.4344 - mlp_test_loss: 0.5620 - mlp_test_binary_accuracy: 0.7013 - mlp_test_auc_18: 0.7775\n",
      "Epoch 69/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5609 - siamese_test_loss: 0.4283 - mlp_test_loss: 0.5609 - mlp_test_binary_accuracy: 0.7004 - mlp_test_auc_18: 0.7775\n",
      "Epoch 70/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5614 - siamese_test_loss: 0.4336 - mlp_test_loss: 0.5614 - mlp_test_binary_accuracy: 0.6967 - mlp_test_auc_18: 0.7786\n",
      "Epoch 71/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5610 - siamese_test_loss: 0.4398 - mlp_test_loss: 0.5610 - mlp_test_binary_accuracy: 0.6983 - mlp_test_auc_18: 0.7769\n",
      "Epoch 72/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5608 - siamese_test_loss: 0.4364 - mlp_test_loss: 0.5608 - mlp_test_binary_accuracy: 0.6988 - mlp_test_auc_18: 0.7772\n",
      "Epoch 73/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5600 - siamese_test_loss: 0.4359 - mlp_test_loss: 0.5600 - mlp_test_binary_accuracy: 0.6958 - mlp_test_auc_18: 0.7784\n",
      "Epoch 74/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5594 - siamese_test_loss: 0.4419 - mlp_test_loss: 0.5594 - mlp_test_binary_accuracy: 0.6953 - mlp_test_auc_18: 0.7782\n",
      "Epoch 75/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5583 - siamese_test_loss: 0.4372 - mlp_test_loss: 0.5583 - mlp_test_binary_accuracy: 0.7024 - mlp_test_auc_18: 0.7811\n",
      "Epoch 76/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5578 - siamese_test_loss: 0.4453 - mlp_test_loss: 0.5578 - mlp_test_binary_accuracy: 0.7040 - mlp_test_auc_18: 0.7826\n",
      "Epoch 77/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5569 - siamese_test_loss: 0.4462 - mlp_test_loss: 0.5569 - mlp_test_binary_accuracy: 0.6960 - mlp_test_auc_18: 0.7828\n",
      "Epoch 78/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5576 - siamese_test_loss: 0.4489 - mlp_test_loss: 0.5576 - mlp_test_binary_accuracy: 0.7089 - mlp_test_auc_18: 0.7831\n",
      "Epoch 79/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5566 - siamese_test_loss: 0.4509 - mlp_test_loss: 0.5566 - mlp_test_binary_accuracy: 0.7052 - mlp_test_auc_18: 0.7835\n",
      "Epoch 80/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5567 - siamese_test_loss: 0.4530 - mlp_test_loss: 0.5567 - mlp_test_binary_accuracy: 0.7038 - mlp_test_auc_18: 0.7826\n",
      "Epoch 81/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5559 - siamese_test_loss: 0.4543 - mlp_test_loss: 0.5559 - mlp_test_binary_accuracy: 0.7043 - mlp_test_auc_18: 0.7838\n",
      "Epoch 82/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5558 - siamese_test_loss: 0.4546 - mlp_test_loss: 0.5558 - mlp_test_binary_accuracy: 0.7040 - mlp_test_auc_18: 0.7845\n",
      "Epoch 83/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5533 - siamese_test_loss: 0.4546 - mlp_test_loss: 0.5533 - mlp_test_binary_accuracy: 0.7038 - mlp_test_auc_18: 0.7860\n",
      "Epoch 84/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5539 - siamese_test_loss: 0.4573 - mlp_test_loss: 0.5539 - mlp_test_binary_accuracy: 0.7063 - mlp_test_auc_18: 0.7854\n",
      "Epoch 85/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5529 - siamese_test_loss: 0.4635 - mlp_test_loss: 0.5529 - mlp_test_binary_accuracy: 0.7086 - mlp_test_auc_18: 0.7874\n",
      "Epoch 86/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5526 - siamese_test_loss: 0.4622 - mlp_test_loss: 0.5526 - mlp_test_binary_accuracy: 0.7116 - mlp_test_auc_18: 0.7881\n",
      "Epoch 87/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5518 - siamese_test_loss: 0.4584 - mlp_test_loss: 0.5518 - mlp_test_binary_accuracy: 0.7075 - mlp_test_auc_18: 0.7878\n",
      "Epoch 88/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5524 - siamese_test_loss: 0.4627 - mlp_test_loss: 0.5524 - mlp_test_binary_accuracy: 0.7114 - mlp_test_auc_18: 0.7887\n",
      "Epoch 89/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5515 - siamese_test_loss: 0.4681 - mlp_test_loss: 0.5515 - mlp_test_binary_accuracy: 0.7080 - mlp_test_auc_18: 0.7883\n",
      "Epoch 90/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5519 - siamese_test_loss: 0.4609 - mlp_test_loss: 0.5519 - mlp_test_binary_accuracy: 0.7073 - mlp_test_auc_18: 0.7876\n",
      "Epoch 91/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5504 - siamese_test_loss: 0.4720 - mlp_test_loss: 0.5504 - mlp_test_binary_accuracy: 0.7105 - mlp_test_auc_18: 0.7906\n",
      "Epoch 92/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5496 - siamese_test_loss: 0.4681 - mlp_test_loss: 0.5496 - mlp_test_binary_accuracy: 0.7093 - mlp_test_auc_18: 0.7909\n",
      "Epoch 93/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5489 - siamese_test_loss: 0.4658 - mlp_test_loss: 0.5489 - mlp_test_binary_accuracy: 0.7107 - mlp_test_auc_18: 0.7914\n",
      "Epoch 94/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5490 - siamese_test_loss: 0.4687 - mlp_test_loss: 0.5490 - mlp_test_binary_accuracy: 0.7096 - mlp_test_auc_18: 0.7914\n",
      "Epoch 95/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5476 - siamese_test_loss: 0.4772 - mlp_test_loss: 0.5476 - mlp_test_binary_accuracy: 0.7158 - mlp_test_auc_18: 0.7928\n",
      "Epoch 96/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5453 - siamese_test_loss: 0.4732 - mlp_test_loss: 0.5453 - mlp_test_binary_accuracy: 0.7165 - mlp_test_auc_18: 0.7951\n",
      "Epoch 97/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5472 - siamese_test_loss: 0.4734 - mlp_test_loss: 0.5472 - mlp_test_binary_accuracy: 0.7165 - mlp_test_auc_18: 0.7940\n",
      "Epoch 98/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5466 - siamese_test_loss: 0.4775 - mlp_test_loss: 0.5466 - mlp_test_binary_accuracy: 0.7112 - mlp_test_auc_18: 0.7927\n",
      "Epoch 99/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5459 - siamese_test_loss: 0.4784 - mlp_test_loss: 0.5459 - mlp_test_binary_accuracy: 0.7160 - mlp_test_auc_18: 0.7946\n",
      "Epoch 100/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5467 - siamese_test_loss: 0.4810 - mlp_test_loss: 0.5467 - mlp_test_binary_accuracy: 0.7171 - mlp_test_auc_18: 0.7943\n",
      "Epoch 101/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5454 - siamese_test_loss: 0.4801 - mlp_test_loss: 0.5454 - mlp_test_binary_accuracy: 0.7165 - mlp_test_auc_18: 0.7958\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002186B355438> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5940 - siamese_test_loss: 0.4759 - mlp_test_loss: 0.5940 - mlp_test_binary_accuracy: 0.6694 - mlp_test_auc_18: 0.7419\n",
      "------ mlp_test_binary_accuracy: 66.94%\t ----- mlp_test_auc_18: 74.19%\n",
      "concat_fea的shape： (None, 16)\n",
      "-------------------------------------Kfold: 6 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6872 - siamese_test_loss: 0.4076 - mlp_test_loss: 0.6872 - mlp_test_binary_accuracy: 0.5687 - mlp_test_auc_19: 0.5963\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6669 - siamese_test_loss: 0.3275 - mlp_test_loss: 0.6669 - mlp_test_binary_accuracy: 0.6183 - mlp_test_auc_19: 0.6650\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6457 - siamese_test_loss: 0.3189 - mlp_test_loss: 0.6457 - mlp_test_binary_accuracy: 0.6245 - mlp_test_auc_19: 0.6849\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6321 - siamese_test_loss: 0.3287 - mlp_test_loss: 0.6321 - mlp_test_binary_accuracy: 0.6381 - mlp_test_auc_19: 0.6992\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 981us/step - loss: 0.6219 - siamese_test_loss: 0.3366 - mlp_test_loss: 0.6219 - mlp_test_binary_accuracy: 0.6471 - mlp_test_auc_19: 0.7094\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6149 - siamese_test_loss: 0.3501 - mlp_test_loss: 0.6149 - mlp_test_binary_accuracy: 0.6512 - mlp_test_auc_19: 0.7170\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6100 - siamese_test_loss: 0.3594 - mlp_test_loss: 0.6100 - mlp_test_binary_accuracy: 0.6540 - mlp_test_auc_19: 0.7214\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6069 - siamese_test_loss: 0.3679 - mlp_test_loss: 0.6069 - mlp_test_binary_accuracy: 0.6606 - mlp_test_auc_19: 0.7242\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6026 - siamese_test_loss: 0.3698 - mlp_test_loss: 0.6026 - mlp_test_binary_accuracy: 0.6650 - mlp_test_auc_19: 0.7302\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 938us/step - loss: 0.6022 - siamese_test_loss: 0.3703 - mlp_test_loss: 0.6022 - mlp_test_binary_accuracy: 0.6627 - mlp_test_auc_19: 0.7298\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6008 - siamese_test_loss: 0.3753 - mlp_test_loss: 0.6008 - mlp_test_binary_accuracy: 0.6597 - mlp_test_auc_19: 0.7325\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5974 - siamese_test_loss: 0.3748 - mlp_test_loss: 0.5974 - mlp_test_binary_accuracy: 0.6700 - mlp_test_auc_19: 0.7359\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5980 - siamese_test_loss: 0.3714 - mlp_test_loss: 0.5980 - mlp_test_binary_accuracy: 0.6714 - mlp_test_auc_19: 0.7357\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5959 - siamese_test_loss: 0.3685 - mlp_test_loss: 0.5959 - mlp_test_binary_accuracy: 0.6661 - mlp_test_auc_19: 0.7383\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5949 - siamese_test_loss: 0.3714 - mlp_test_loss: 0.5949 - mlp_test_binary_accuracy: 0.6696 - mlp_test_auc_19: 0.7386\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5927 - siamese_test_loss: 0.3669 - mlp_test_loss: 0.5927 - mlp_test_binary_accuracy: 0.6753 - mlp_test_auc_19: 0.7421\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5943 - siamese_test_loss: 0.3700 - mlp_test_loss: 0.5943 - mlp_test_binary_accuracy: 0.6698 - mlp_test_auc_19: 0.7396\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5926 - siamese_test_loss: 0.3746 - mlp_test_loss: 0.5926 - mlp_test_binary_accuracy: 0.6730 - mlp_test_auc_19: 0.7416\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5917 - siamese_test_loss: 0.3684 - mlp_test_loss: 0.5917 - mlp_test_binary_accuracy: 0.6730 - mlp_test_auc_19: 0.7437\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5915 - siamese_test_loss: 0.3705 - mlp_test_loss: 0.5915 - mlp_test_binary_accuracy: 0.6739 - mlp_test_auc_19: 0.7450\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5914 - siamese_test_loss: 0.3685 - mlp_test_loss: 0.5914 - mlp_test_binary_accuracy: 0.6733 - mlp_test_auc_19: 0.7437\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5918 - siamese_test_loss: 0.3751 - mlp_test_loss: 0.5918 - mlp_test_binary_accuracy: 0.6790 - mlp_test_auc_19: 0.7439\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5893 - siamese_test_loss: 0.3727 - mlp_test_loss: 0.5893 - mlp_test_binary_accuracy: 0.6774 - mlp_test_auc_19: 0.7465\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5905 - siamese_test_loss: 0.3730 - mlp_test_loss: 0.5905 - mlp_test_binary_accuracy: 0.6811 - mlp_test_auc_19: 0.7447\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5880 - siamese_test_loss: 0.3759 - mlp_test_loss: 0.5880 - mlp_test_binary_accuracy: 0.6788 - mlp_test_auc_19: 0.7475\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5891 - siamese_test_loss: 0.3727 - mlp_test_loss: 0.5891 - mlp_test_binary_accuracy: 0.6774 - mlp_test_auc_19: 0.7466\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5887 - siamese_test_loss: 0.3746 - mlp_test_loss: 0.5887 - mlp_test_binary_accuracy: 0.6714 - mlp_test_auc_19: 0.7464\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5874 - siamese_test_loss: 0.3738 - mlp_test_loss: 0.5874 - mlp_test_binary_accuracy: 0.6790 - mlp_test_auc_19: 0.7490\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5881 - siamese_test_loss: 0.3769 - mlp_test_loss: 0.5881 - mlp_test_binary_accuracy: 0.6772 - mlp_test_auc_19: 0.7473\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5875 - siamese_test_loss: 0.3788 - mlp_test_loss: 0.5875 - mlp_test_binary_accuracy: 0.6751 - mlp_test_auc_19: 0.7479\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 919us/step - loss: 0.5873 - siamese_test_loss: 0.3762 - mlp_test_loss: 0.5873 - mlp_test_binary_accuracy: 0.6739 - mlp_test_auc_19: 0.7473\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5875 - siamese_test_loss: 0.3776 - mlp_test_loss: 0.5875 - mlp_test_binary_accuracy: 0.6756 - mlp_test_auc_19: 0.7482\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5854 - siamese_test_loss: 0.3774 - mlp_test_loss: 0.5854 - mlp_test_binary_accuracy: 0.6792 - mlp_test_auc_19: 0.7506\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5862 - siamese_test_loss: 0.3826 - mlp_test_loss: 0.5862 - mlp_test_binary_accuracy: 0.6797 - mlp_test_auc_19: 0.7500\n",
      "Epoch 35/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5852 - siamese_test_loss: 0.3838 - mlp_test_loss: 0.5852 - mlp_test_binary_accuracy: 0.6813 - mlp_test_auc_19: 0.7509\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5861 - siamese_test_loss: 0.3820 - mlp_test_loss: 0.5861 - mlp_test_binary_accuracy: 0.6815 - mlp_test_auc_19: 0.7497\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5847 - siamese_test_loss: 0.3855 - mlp_test_loss: 0.5847 - mlp_test_binary_accuracy: 0.6788 - mlp_test_auc_19: 0.7511\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5853 - siamese_test_loss: 0.3833 - mlp_test_loss: 0.5853 - mlp_test_binary_accuracy: 0.6785 - mlp_test_auc_19: 0.7511\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5843 - siamese_test_loss: 0.3905 - mlp_test_loss: 0.5843 - mlp_test_binary_accuracy: 0.6813 - mlp_test_auc_19: 0.7517\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5840 - siamese_test_loss: 0.3937 - mlp_test_loss: 0.5840 - mlp_test_binary_accuracy: 0.6843 - mlp_test_auc_19: 0.7523\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5823 - siamese_test_loss: 0.3903 - mlp_test_loss: 0.5823 - mlp_test_binary_accuracy: 0.6822 - mlp_test_auc_19: 0.7541\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5820 - siamese_test_loss: 0.3918 - mlp_test_loss: 0.5820 - mlp_test_binary_accuracy: 0.6815 - mlp_test_auc_19: 0.7541\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5827 - siamese_test_loss: 0.3970 - mlp_test_loss: 0.5827 - mlp_test_binary_accuracy: 0.6792 - mlp_test_auc_19: 0.7538\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5813 - siamese_test_loss: 0.3902 - mlp_test_loss: 0.5813 - mlp_test_binary_accuracy: 0.6808 - mlp_test_auc_19: 0.7551\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5791 - siamese_test_loss: 0.3933 - mlp_test_loss: 0.5791 - mlp_test_binary_accuracy: 0.6852 - mlp_test_auc_19: 0.7571\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5806 - siamese_test_loss: 0.3969 - mlp_test_loss: 0.5806 - mlp_test_binary_accuracy: 0.6831 - mlp_test_auc_19: 0.7549\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5781 - siamese_test_loss: 0.3982 - mlp_test_loss: 0.5781 - mlp_test_binary_accuracy: 0.6896 - mlp_test_auc_19: 0.7597\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5787 - siamese_test_loss: 0.3970 - mlp_test_loss: 0.5787 - mlp_test_binary_accuracy: 0.6799 - mlp_test_auc_19: 0.7575\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5772 - siamese_test_loss: 0.3938 - mlp_test_loss: 0.5772 - mlp_test_binary_accuracy: 0.6838 - mlp_test_auc_19: 0.7593\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5759 - siamese_test_loss: 0.4007 - mlp_test_loss: 0.5759 - mlp_test_binary_accuracy: 0.6861 - mlp_test_auc_19: 0.7609\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5759 - siamese_test_loss: 0.4034 - mlp_test_loss: 0.5759 - mlp_test_binary_accuracy: 0.6875 - mlp_test_auc_19: 0.7616\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5741 - siamese_test_loss: 0.4023 - mlp_test_loss: 0.5741 - mlp_test_binary_accuracy: 0.6864 - mlp_test_auc_19: 0.7625\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5731 - siamese_test_loss: 0.4097 - mlp_test_loss: 0.5731 - mlp_test_binary_accuracy: 0.6932 - mlp_test_auc_19: 0.7648\n",
      "Epoch 54/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5728 - siamese_test_loss: 0.4133 - mlp_test_loss: 0.5728 - mlp_test_binary_accuracy: 0.6893 - mlp_test_auc_19: 0.7634\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5707 - siamese_test_loss: 0.4206 - mlp_test_loss: 0.5707 - mlp_test_binary_accuracy: 0.6928 - mlp_test_auc_19: 0.7677\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5694 - siamese_test_loss: 0.4265 - mlp_test_loss: 0.5694 - mlp_test_binary_accuracy: 0.6930 - mlp_test_auc_19: 0.7690\n",
      "Epoch 57/200\n",
      "136/136 [==============================] - 0s 990us/step - loss: 0.5694 - siamese_test_loss: 0.4315 - mlp_test_loss: 0.5694 - mlp_test_binary_accuracy: 0.6926 - mlp_test_auc_19: 0.7680\n",
      "Epoch 58/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5670 - siamese_test_loss: 0.4337 - mlp_test_loss: 0.5670 - mlp_test_binary_accuracy: 0.6965 - mlp_test_auc_19: 0.7708\n",
      "Epoch 59/200\n",
      "136/136 [==============================] - 0s 970us/step - loss: 0.5674 - siamese_test_loss: 0.4366 - mlp_test_loss: 0.5674 - mlp_test_binary_accuracy: 0.6962 - mlp_test_auc_19: 0.7689\n",
      "Epoch 60/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5661 - siamese_test_loss: 0.4417 - mlp_test_loss: 0.5661 - mlp_test_binary_accuracy: 0.6997 - mlp_test_auc_19: 0.7727\n",
      "Epoch 61/200\n",
      "136/136 [==============================] - 0s 976us/step - loss: 0.5648 - siamese_test_loss: 0.4451 - mlp_test_loss: 0.5648 - mlp_test_binary_accuracy: 0.6958 - mlp_test_auc_19: 0.7734\n",
      "Epoch 62/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5639 - siamese_test_loss: 0.4422 - mlp_test_loss: 0.5639 - mlp_test_binary_accuracy: 0.6985 - mlp_test_auc_19: 0.7743\n",
      "Epoch 63/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5621 - siamese_test_loss: 0.4505 - mlp_test_loss: 0.5621 - mlp_test_binary_accuracy: 0.7011 - mlp_test_auc_19: 0.7766\n",
      "Epoch 64/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5628 - siamese_test_loss: 0.4506 - mlp_test_loss: 0.5628 - mlp_test_binary_accuracy: 0.6990 - mlp_test_auc_19: 0.7755\n",
      "Epoch 65/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5609 - siamese_test_loss: 0.4502 - mlp_test_loss: 0.5609 - mlp_test_binary_accuracy: 0.6972 - mlp_test_auc_19: 0.7778\n",
      "Epoch 66/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5601 - siamese_test_loss: 0.4513 - mlp_test_loss: 0.5601 - mlp_test_binary_accuracy: 0.7054 - mlp_test_auc_19: 0.7783\n",
      "Epoch 67/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5600 - siamese_test_loss: 0.4575 - mlp_test_loss: 0.5600 - mlp_test_binary_accuracy: 0.7054 - mlp_test_auc_19: 0.7794\n",
      "Epoch 68/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5588 - siamese_test_loss: 0.4620 - mlp_test_loss: 0.5588 - mlp_test_binary_accuracy: 0.7043 - mlp_test_auc_19: 0.7804\n",
      "Epoch 69/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5584 - siamese_test_loss: 0.4603 - mlp_test_loss: 0.5584 - mlp_test_binary_accuracy: 0.7024 - mlp_test_auc_19: 0.7805\n",
      "Epoch 70/200\n",
      "136/136 [==============================] - 0s 998us/step - loss: 0.5577 - siamese_test_loss: 0.4640 - mlp_test_loss: 0.5577 - mlp_test_binary_accuracy: 0.6990 - mlp_test_auc_19: 0.7816\n",
      "Epoch 71/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5569 - siamese_test_loss: 0.4664 - mlp_test_loss: 0.5569 - mlp_test_binary_accuracy: 0.7004 - mlp_test_auc_19: 0.7820\n",
      "Epoch 72/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5568 - siamese_test_loss: 0.4717 - mlp_test_loss: 0.5568 - mlp_test_binary_accuracy: 0.7022 - mlp_test_auc_19: 0.7826\n",
      "Epoch 73/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5547 - siamese_test_loss: 0.4720 - mlp_test_loss: 0.5547 - mlp_test_binary_accuracy: 0.6994 - mlp_test_auc_19: 0.7836\n",
      "Epoch 74/200\n",
      "136/136 [==============================] - 0s 971us/step - loss: 0.5557 - siamese_test_loss: 0.4818 - mlp_test_loss: 0.5557 - mlp_test_binary_accuracy: 0.6976 - mlp_test_auc_19: 0.7831\n",
      "Epoch 75/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5544 - siamese_test_loss: 0.4790 - mlp_test_loss: 0.5544 - mlp_test_binary_accuracy: 0.7070 - mlp_test_auc_19: 0.7844\n",
      "Epoch 76/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5533 - siamese_test_loss: 0.4858 - mlp_test_loss: 0.5533 - mlp_test_binary_accuracy: 0.7066 - mlp_test_auc_19: 0.7854\n",
      "Epoch 77/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5529 - siamese_test_loss: 0.4873 - mlp_test_loss: 0.5529 - mlp_test_binary_accuracy: 0.7061 - mlp_test_auc_19: 0.7861\n",
      "Epoch 78/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5524 - siamese_test_loss: 0.4884 - mlp_test_loss: 0.5524 - mlp_test_binary_accuracy: 0.7084 - mlp_test_auc_19: 0.7864\n",
      "Epoch 79/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5504 - siamese_test_loss: 0.4908 - mlp_test_loss: 0.5504 - mlp_test_binary_accuracy: 0.7119 - mlp_test_auc_19: 0.7886\n",
      "Epoch 80/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5501 - siamese_test_loss: 0.4979 - mlp_test_loss: 0.5501 - mlp_test_binary_accuracy: 0.7116 - mlp_test_auc_19: 0.7897\n",
      "Epoch 81/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5510 - siamese_test_loss: 0.5022 - mlp_test_loss: 0.5510 - mlp_test_binary_accuracy: 0.7057 - mlp_test_auc_19: 0.7877\n",
      "Epoch 82/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5493 - siamese_test_loss: 0.5017 - mlp_test_loss: 0.5493 - mlp_test_binary_accuracy: 0.7109 - mlp_test_auc_19: 0.7892\n",
      "Epoch 83/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5496 - siamese_test_loss: 0.5105 - mlp_test_loss: 0.5496 - mlp_test_binary_accuracy: 0.7137 - mlp_test_auc_19: 0.7899\n",
      "Epoch 84/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5486 - siamese_test_loss: 0.5134 - mlp_test_loss: 0.5486 - mlp_test_binary_accuracy: 0.7123 - mlp_test_auc_19: 0.7904\n",
      "Epoch 85/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5483 - siamese_test_loss: 0.5142 - mlp_test_loss: 0.5483 - mlp_test_binary_accuracy: 0.7057 - mlp_test_auc_19: 0.7899\n",
      "Epoch 86/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.5445 - siamese_test_loss: 0.5079 - mlp_test_loss: 0.5445 - mlp_test_binary_accuracy: 0.7169 - mlp_test_auc_19: 0.79 - 0s 1ms/step - loss: 0.5475 - siamese_test_loss: 0.5127 - mlp_test_loss: 0.5475 - mlp_test_binary_accuracy: 0.7114 - mlp_test_auc_19: 0.7915\n",
      "Epoch 87/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5475 - siamese_test_loss: 0.5189 - mlp_test_loss: 0.5475 - mlp_test_binary_accuracy: 0.7114 - mlp_test_auc_19: 0.7918\n",
      "Epoch 88/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5475 - siamese_test_loss: 0.5246 - mlp_test_loss: 0.5475 - mlp_test_binary_accuracy: 0.7093 - mlp_test_auc_19: 0.7922\n",
      "Epoch 89/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5466 - siamese_test_loss: 0.5263 - mlp_test_loss: 0.5466 - mlp_test_binary_accuracy: 0.7093 - mlp_test_auc_19: 0.7913\n",
      "Epoch 90/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5471 - siamese_test_loss: 0.5311 - mlp_test_loss: 0.5471 - mlp_test_binary_accuracy: 0.7105 - mlp_test_auc_19: 0.7915\n",
      "Epoch 91/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5463 - siamese_test_loss: 0.5331 - mlp_test_loss: 0.5463 - mlp_test_binary_accuracy: 0.7123 - mlp_test_auc_19: 0.7928\n",
      "Epoch 92/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5434 - siamese_test_loss: 0.5347 - mlp_test_loss: 0.5434 - mlp_test_binary_accuracy: 0.7119 - mlp_test_auc_19: 0.7950\n",
      "Epoch 93/200\n",
      "136/136 [==============================] - 0s 992us/step - loss: 0.5438 - siamese_test_loss: 0.5329 - mlp_test_loss: 0.5438 - mlp_test_binary_accuracy: 0.7148 - mlp_test_auc_19: 0.7953\n",
      "Epoch 94/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5423 - siamese_test_loss: 0.5401 - mlp_test_loss: 0.5423 - mlp_test_binary_accuracy: 0.7119 - mlp_test_auc_19: 0.7966\n",
      "Epoch 95/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5426 - siamese_test_loss: 0.5510 - mlp_test_loss: 0.5426 - mlp_test_binary_accuracy: 0.7174 - mlp_test_auc_19: 0.7958\n",
      "Epoch 96/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5416 - siamese_test_loss: 0.5448 - mlp_test_loss: 0.5416 - mlp_test_binary_accuracy: 0.7144 - mlp_test_auc_19: 0.7965\n",
      "Epoch 97/200\n",
      "136/136 [==============================] - 0s 919us/step - loss: 0.5417 - siamese_test_loss: 0.5445 - mlp_test_loss: 0.5417 - mlp_test_binary_accuracy: 0.7185 - mlp_test_auc_19: 0.7971\n",
      "Epoch 98/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5407 - siamese_test_loss: 0.5478 - mlp_test_loss: 0.5407 - mlp_test_binary_accuracy: 0.7112 - mlp_test_auc_19: 0.7976\n",
      "Epoch 99/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5414 - siamese_test_loss: 0.5524 - mlp_test_loss: 0.5414 - mlp_test_binary_accuracy: 0.7183 - mlp_test_auc_19: 0.7973\n",
      "Epoch 100/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5399 - siamese_test_loss: 0.5599 - mlp_test_loss: 0.5399 - mlp_test_binary_accuracy: 0.7176 - mlp_test_auc_19: 0.7993\n",
      "Epoch 101/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5387 - siamese_test_loss: 0.5555 - mlp_test_loss: 0.5387 - mlp_test_binary_accuracy: 0.7155 - mlp_test_auc_19: 0.7996\n",
      "Epoch 102/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5377 - siamese_test_loss: 0.5609 - mlp_test_loss: 0.5377 - mlp_test_binary_accuracy: 0.7224 - mlp_test_auc_19: 0.8003\n",
      "Epoch 103/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5374 - siamese_test_loss: 0.5643 - mlp_test_loss: 0.5374 - mlp_test_binary_accuracy: 0.7192 - mlp_test_auc_19: 0.8008\n",
      "Epoch 104/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5381 - siamese_test_loss: 0.5623 - mlp_test_loss: 0.5381 - mlp_test_binary_accuracy: 0.7197 - mlp_test_auc_19: 0.8007\n",
      "Epoch 105/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5355 - siamese_test_loss: 0.5619 - mlp_test_loss: 0.5355 - mlp_test_binary_accuracy: 0.7222 - mlp_test_auc_19: 0.8028\n",
      "Epoch 106/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5352 - siamese_test_loss: 0.5672 - mlp_test_loss: 0.5352 - mlp_test_binary_accuracy: 0.7252 - mlp_test_auc_19: 0.8033\n",
      "Epoch 107/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5349 - siamese_test_loss: 0.5736 - mlp_test_loss: 0.5349 - mlp_test_binary_accuracy: 0.7247 - mlp_test_auc_19: 0.8035\n",
      "Epoch 108/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5343 - siamese_test_loss: 0.5697 - mlp_test_loss: 0.5343 - mlp_test_binary_accuracy: 0.7270 - mlp_test_auc_19: 0.8042\n",
      "Epoch 109/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5340 - siamese_test_loss: 0.5735 - mlp_test_loss: 0.5340 - mlp_test_binary_accuracy: 0.7266 - mlp_test_auc_19: 0.8043\n",
      "Epoch 110/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5343 - siamese_test_loss: 0.5767 - mlp_test_loss: 0.5343 - mlp_test_binary_accuracy: 0.7261 - mlp_test_auc_19: 0.8046\n",
      "Epoch 111/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5330 - siamese_test_loss: 0.5742 - mlp_test_loss: 0.5330 - mlp_test_binary_accuracy: 0.7259 - mlp_test_auc_19: 0.8053\n",
      "Epoch 112/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5323 - siamese_test_loss: 0.5818 - mlp_test_loss: 0.5323 - mlp_test_binary_accuracy: 0.7243 - mlp_test_auc_19: 0.8056\n",
      "Epoch 113/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5318 - siamese_test_loss: 0.5835 - mlp_test_loss: 0.5318 - mlp_test_binary_accuracy: 0.7250 - mlp_test_auc_19: 0.8059\n",
      "Epoch 114/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5312 - siamese_test_loss: 0.5901 - mlp_test_loss: 0.5312 - mlp_test_binary_accuracy: 0.7238 - mlp_test_auc_19: 0.8066\n",
      "Epoch 115/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5298 - siamese_test_loss: 0.5893 - mlp_test_loss: 0.5298 - mlp_test_binary_accuracy: 0.7273 - mlp_test_auc_19: 0.8078\n",
      "Epoch 116/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5297 - siamese_test_loss: 0.5933 - mlp_test_loss: 0.5297 - mlp_test_binary_accuracy: 0.7247 - mlp_test_auc_19: 0.8083\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5290 - siamese_test_loss: 0.6024 - mlp_test_loss: 0.5290 - mlp_test_binary_accuracy: 0.7307 - mlp_test_auc_19: 0.8093\n",
      "Epoch 118/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5286 - siamese_test_loss: 0.5969 - mlp_test_loss: 0.5286 - mlp_test_binary_accuracy: 0.7316 - mlp_test_auc_19: 0.8095\n",
      "Epoch 119/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5273 - siamese_test_loss: 0.6041 - mlp_test_loss: 0.5273 - mlp_test_binary_accuracy: 0.7295 - mlp_test_auc_19: 0.8108\n",
      "Epoch 120/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5272 - siamese_test_loss: 0.6032 - mlp_test_loss: 0.5272 - mlp_test_binary_accuracy: 0.7325 - mlp_test_auc_19: 0.8104\n",
      "Epoch 121/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5265 - siamese_test_loss: 0.6079 - mlp_test_loss: 0.5265 - mlp_test_binary_accuracy: 0.7282 - mlp_test_auc_19: 0.8108\n",
      "Epoch 122/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5264 - siamese_test_loss: 0.6093 - mlp_test_loss: 0.5264 - mlp_test_binary_accuracy: 0.7300 - mlp_test_auc_19: 0.8110\n",
      "Epoch 123/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5251 - siamese_test_loss: 0.6114 - mlp_test_loss: 0.5251 - mlp_test_binary_accuracy: 0.7323 - mlp_test_auc_19: 0.8122\n",
      "Epoch 124/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5264 - siamese_test_loss: 0.6155 - mlp_test_loss: 0.5264 - mlp_test_binary_accuracy: 0.7316 - mlp_test_auc_19: 0.8107\n",
      "Epoch 125/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5247 - siamese_test_loss: 0.6234 - mlp_test_loss: 0.5247 - mlp_test_binary_accuracy: 0.7376 - mlp_test_auc_19: 0.8131\n",
      "Epoch 126/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5250 - siamese_test_loss: 0.6205 - mlp_test_loss: 0.5250 - mlp_test_binary_accuracy: 0.7293 - mlp_test_auc_19: 0.8125\n",
      "Epoch 127/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5240 - siamese_test_loss: 0.6225 - mlp_test_loss: 0.5240 - mlp_test_binary_accuracy: 0.7321 - mlp_test_auc_19: 0.8131\n",
      "Epoch 128/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5240 - siamese_test_loss: 0.6216 - mlp_test_loss: 0.5240 - mlp_test_binary_accuracy: 0.7353 - mlp_test_auc_19: 0.8134\n",
      "Epoch 129/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5227 - siamese_test_loss: 0.6272 - mlp_test_loss: 0.5227 - mlp_test_binary_accuracy: 0.7300 - mlp_test_auc_19: 0.8143\n",
      "Epoch 130/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5242 - siamese_test_loss: 0.6296 - mlp_test_loss: 0.5242 - mlp_test_binary_accuracy: 0.7307 - mlp_test_auc_19: 0.8121\n",
      "Epoch 131/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5220 - siamese_test_loss: 0.6285 - mlp_test_loss: 0.5220 - mlp_test_binary_accuracy: 0.7348 - mlp_test_auc_19: 0.8152\n",
      "Epoch 132/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5221 - siamese_test_loss: 0.6363 - mlp_test_loss: 0.5221 - mlp_test_binary_accuracy: 0.7293 - mlp_test_auc_19: 0.8144\n",
      "Epoch 133/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5216 - siamese_test_loss: 0.6393 - mlp_test_loss: 0.5216 - mlp_test_binary_accuracy: 0.7335 - mlp_test_auc_19: 0.8153\n",
      "Epoch 134/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5224 - siamese_test_loss: 0.6348 - mlp_test_loss: 0.5224 - mlp_test_binary_accuracy: 0.7351 - mlp_test_auc_19: 0.8148\n",
      "Epoch 135/200\n",
      "136/136 [==============================] - 0s 919us/step - loss: 0.5221 - siamese_test_loss: 0.6331 - mlp_test_loss: 0.5221 - mlp_test_binary_accuracy: 0.7346 - mlp_test_auc_19: 0.8146\n",
      "Epoch 136/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5199 - siamese_test_loss: 0.6342 - mlp_test_loss: 0.5199 - mlp_test_binary_accuracy: 0.7305 - mlp_test_auc_19: 0.8162\n",
      "Epoch 137/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5202 - siamese_test_loss: 0.6401 - mlp_test_loss: 0.5202 - mlp_test_binary_accuracy: 0.7358 - mlp_test_auc_19: 0.8171\n",
      "Epoch 138/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5196 - siamese_test_loss: 0.6373 - mlp_test_loss: 0.5196 - mlp_test_binary_accuracy: 0.7339 - mlp_test_auc_19: 0.8169\n",
      "Epoch 139/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5187 - siamese_test_loss: 0.6440 - mlp_test_loss: 0.5187 - mlp_test_binary_accuracy: 0.7399 - mlp_test_auc_19: 0.8176\n",
      "Epoch 140/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5201 - siamese_test_loss: 0.6434 - mlp_test_loss: 0.5201 - mlp_test_binary_accuracy: 0.7341 - mlp_test_auc_19: 0.8164\n",
      "Epoch 141/200\n",
      "136/136 [==============================] - 0s 966us/step - loss: 0.5185 - siamese_test_loss: 0.6467 - mlp_test_loss: 0.5185 - mlp_test_binary_accuracy: 0.7358 - mlp_test_auc_19: 0.8175\n",
      "Epoch 142/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5180 - siamese_test_loss: 0.6486 - mlp_test_loss: 0.5180 - mlp_test_binary_accuracy: 0.7369 - mlp_test_auc_19: 0.8180\n",
      "Epoch 143/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5189 - siamese_test_loss: 0.6462 - mlp_test_loss: 0.5189 - mlp_test_binary_accuracy: 0.7433 - mlp_test_auc_19: 0.8177\n",
      "Epoch 144/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5178 - siamese_test_loss: 0.6577 - mlp_test_loss: 0.5178 - mlp_test_binary_accuracy: 0.7371 - mlp_test_auc_19: 0.8185\n",
      "Epoch 145/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5179 - siamese_test_loss: 0.6484 - mlp_test_loss: 0.5179 - mlp_test_binary_accuracy: 0.7399 - mlp_test_auc_19: 0.8187\n",
      "Epoch 146/200\n",
      "136/136 [==============================] - 0s 982us/step - loss: 0.5160 - siamese_test_loss: 0.6504 - mlp_test_loss: 0.5160 - mlp_test_binary_accuracy: 0.7369 - mlp_test_auc_19: 0.8201\n",
      "Epoch 147/200\n",
      "136/136 [==============================] - 0s 919us/step - loss: 0.5160 - siamese_test_loss: 0.6562 - mlp_test_loss: 0.5160 - mlp_test_binary_accuracy: 0.7341 - mlp_test_auc_19: 0.8194\n",
      "Epoch 148/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5154 - siamese_test_loss: 0.6573 - mlp_test_loss: 0.5154 - mlp_test_binary_accuracy: 0.7433 - mlp_test_auc_19: 0.8208\n",
      "Epoch 149/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5162 - siamese_test_loss: 0.6590 - mlp_test_loss: 0.5162 - mlp_test_binary_accuracy: 0.7401 - mlp_test_auc_19: 0.8195\n",
      "Epoch 150/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5155 - siamese_test_loss: 0.6654 - mlp_test_loss: 0.5155 - mlp_test_binary_accuracy: 0.7394 - mlp_test_auc_19: 0.8204\n",
      "Epoch 151/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5155 - siamese_test_loss: 0.6576 - mlp_test_loss: 0.5155 - mlp_test_binary_accuracy: 0.7397 - mlp_test_auc_19: 0.8200\n",
      "Epoch 152/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5147 - siamese_test_loss: 0.6583 - mlp_test_loss: 0.5147 - mlp_test_binary_accuracy: 0.7406 - mlp_test_auc_19: 0.8210\n",
      "Epoch 153/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5140 - siamese_test_loss: 0.6620 - mlp_test_loss: 0.5140 - mlp_test_binary_accuracy: 0.7387 - mlp_test_auc_19: 0.8221\n",
      "Epoch 154/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5148 - siamese_test_loss: 0.6639 - mlp_test_loss: 0.5148 - mlp_test_binary_accuracy: 0.7429 - mlp_test_auc_19: 0.8203\n",
      "Epoch 155/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5130 - siamese_test_loss: 0.6616 - mlp_test_loss: 0.5130 - mlp_test_binary_accuracy: 0.7399 - mlp_test_auc_19: 0.8222\n",
      "Epoch 156/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5124 - siamese_test_loss: 0.6652 - mlp_test_loss: 0.5124 - mlp_test_binary_accuracy: 0.7420 - mlp_test_auc_19: 0.8227\n",
      "Epoch 157/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5142 - siamese_test_loss: 0.6674 - mlp_test_loss: 0.5142 - mlp_test_binary_accuracy: 0.7344 - mlp_test_auc_19: 0.8210\n",
      "Epoch 158/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5144 - siamese_test_loss: 0.6673 - mlp_test_loss: 0.5144 - mlp_test_binary_accuracy: 0.7374 - mlp_test_auc_19: 0.8210\n",
      "Epoch 159/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5131 - siamese_test_loss: 0.6684 - mlp_test_loss: 0.5131 - mlp_test_binary_accuracy: 0.7408 - mlp_test_auc_19: 0.8225\n",
      "Epoch 160/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5130 - siamese_test_loss: 0.6781 - mlp_test_loss: 0.5130 - mlp_test_binary_accuracy: 0.7403 - mlp_test_auc_19: 0.8217\n",
      "Epoch 161/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5114 - siamese_test_loss: 0.6769 - mlp_test_loss: 0.5114 - mlp_test_binary_accuracy: 0.7413 - mlp_test_auc_19: 0.8232\n",
      "Epoch 162/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5122 - siamese_test_loss: 0.6771 - mlp_test_loss: 0.5122 - mlp_test_binary_accuracy: 0.7420 - mlp_test_auc_19: 0.8234\n",
      "Epoch 163/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5108 - siamese_test_loss: 0.6711 - mlp_test_loss: 0.5108 - mlp_test_binary_accuracy: 0.7429 - mlp_test_auc_19: 0.8245\n",
      "Epoch 164/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5110 - siamese_test_loss: 0.6799 - mlp_test_loss: 0.5110 - mlp_test_binary_accuracy: 0.7424 - mlp_test_auc_19: 0.8236\n",
      "Epoch 165/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5119 - siamese_test_loss: 0.6737 - mlp_test_loss: 0.5119 - mlp_test_binary_accuracy: 0.7461 - mlp_test_auc_19: 0.8233\n",
      "Epoch 166/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5099 - siamese_test_loss: 0.6793 - mlp_test_loss: 0.5099 - mlp_test_binary_accuracy: 0.7445 - mlp_test_auc_19: 0.8243\n",
      "Epoch 167/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5097 - siamese_test_loss: 0.6823 - mlp_test_loss: 0.5097 - mlp_test_binary_accuracy: 0.7463 - mlp_test_auc_19: 0.8251\n",
      "Epoch 168/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5108 - siamese_test_loss: 0.6875 - mlp_test_loss: 0.5108 - mlp_test_binary_accuracy: 0.7456 - mlp_test_auc_19: 0.8240\n",
      "Epoch 169/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5088 - siamese_test_loss: 0.6853 - mlp_test_loss: 0.5088 - mlp_test_binary_accuracy: 0.7452 - mlp_test_auc_19: 0.8256\n",
      "Epoch 170/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5103 - siamese_test_loss: 0.6818 - mlp_test_loss: 0.5103 - mlp_test_binary_accuracy: 0.7399 - mlp_test_auc_19: 0.8242\n",
      "Epoch 171/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5089 - siamese_test_loss: 0.6873 - mlp_test_loss: 0.5089 - mlp_test_binary_accuracy: 0.7443 - mlp_test_auc_19: 0.8255\n",
      "Epoch 172/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5090 - siamese_test_loss: 0.6839 - mlp_test_loss: 0.5090 - mlp_test_binary_accuracy: 0.7470 - mlp_test_auc_19: 0.8254\n",
      "Epoch 173/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5090 - siamese_test_loss: 0.6789 - mlp_test_loss: 0.5090 - mlp_test_binary_accuracy: 0.7470 - mlp_test_auc_19: 0.8253\n",
      "Epoch 174/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5078 - siamese_test_loss: 0.6891 - mlp_test_loss: 0.5078 - mlp_test_binary_accuracy: 0.7470 - mlp_test_auc_19: 0.8266\n",
      "Epoch 175/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5084 - siamese_test_loss: 0.6857 - mlp_test_loss: 0.5084 - mlp_test_binary_accuracy: 0.7426 - mlp_test_auc_19: 0.8259\n",
      "Epoch 176/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5076 - siamese_test_loss: 0.6884 - mlp_test_loss: 0.5076 - mlp_test_binary_accuracy: 0.7447 - mlp_test_auc_19: 0.8262\n",
      "Epoch 177/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5075 - siamese_test_loss: 0.6873 - mlp_test_loss: 0.5075 - mlp_test_binary_accuracy: 0.7463 - mlp_test_auc_19: 0.8267\n",
      "Epoch 178/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5070 - siamese_test_loss: 0.6884 - mlp_test_loss: 0.5070 - mlp_test_binary_accuracy: 0.7463 - mlp_test_auc_19: 0.8272\n",
      "Epoch 179/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5056 - siamese_test_loss: 0.6841 - mlp_test_loss: 0.5056 - mlp_test_binary_accuracy: 0.7461 - mlp_test_auc_19: 0.8277\n",
      "Epoch 180/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5075 - siamese_test_loss: 0.6838 - mlp_test_loss: 0.5075 - mlp_test_binary_accuracy: 0.7475 - mlp_test_auc_19: 0.8268\n",
      "Epoch 181/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5065 - siamese_test_loss: 0.6878 - mlp_test_loss: 0.5065 - mlp_test_binary_accuracy: 0.7489 - mlp_test_auc_19: 0.8273\n",
      "Epoch 182/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5059 - siamese_test_loss: 0.6957 - mlp_test_loss: 0.5059 - mlp_test_binary_accuracy: 0.7477 - mlp_test_auc_19: 0.8279\n",
      "Epoch 183/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5078 - siamese_test_loss: 0.6982 - mlp_test_loss: 0.5078 - mlp_test_binary_accuracy: 0.7479 - mlp_test_auc_19: 0.8259\n",
      "Epoch 184/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5059 - siamese_test_loss: 0.6904 - mlp_test_loss: 0.5059 - mlp_test_binary_accuracy: 0.7470 - mlp_test_auc_19: 0.8275\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002186B3FB0D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5917 - siamese_test_loss: 0.8176 - mlp_test_loss: 0.5917 - mlp_test_binary_accuracy: 0.6860 - mlp_test_auc_19: 0.7655\n",
      "------ mlp_test_binary_accuracy: 68.60%\t ----- mlp_test_auc_19: 76.55%\n",
      "concat_fea的shape： (None, 16)\n",
      "-------------------------------------Kfold: 7 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6907 - siamese_test_loss: 0.4171 - mlp_test_loss: 0.6907 - mlp_test_binary_accuracy: 0.5261 - mlp_test_auc_20: 0.5430\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6714 - siamese_test_loss: 0.3320 - mlp_test_loss: 0.6714 - mlp_test_binary_accuracy: 0.5856 - mlp_test_auc_20: 0.6373\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6493 - siamese_test_loss: 0.3245 - mlp_test_loss: 0.6493 - mlp_test_binary_accuracy: 0.6205 - mlp_test_auc_20: 0.6699\n",
      "Epoch 4/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6351 - siamese_test_loss: 0.3362 - mlp_test_loss: 0.6351 - mlp_test_binary_accuracy: 0.6331 - mlp_test_auc_20: 0.6916\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6252 - siamese_test_loss: 0.3448 - mlp_test_loss: 0.6252 - mlp_test_binary_accuracy: 0.6430 - mlp_test_auc_20: 0.7046\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6141 - siamese_test_loss: 0.3588 - mlp_test_loss: 0.6141 - mlp_test_binary_accuracy: 0.6515 - mlp_test_auc_20: 0.7174\n",
      "Epoch 7/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6106 - siamese_test_loss: 0.3639 - mlp_test_loss: 0.6106 - mlp_test_binary_accuracy: 0.6561 - mlp_test_auc_20: 0.7203\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6043 - siamese_test_loss: 0.3676 - mlp_test_loss: 0.6043 - mlp_test_binary_accuracy: 0.6575 - mlp_test_auc_20: 0.7268\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6026 - siamese_test_loss: 0.3680 - mlp_test_loss: 0.6026 - mlp_test_binary_accuracy: 0.6616 - mlp_test_auc_20: 0.7278\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6002 - siamese_test_loss: 0.3642 - mlp_test_loss: 0.6002 - mlp_test_binary_accuracy: 0.6595 - mlp_test_auc_20: 0.7314\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5987 - siamese_test_loss: 0.3662 - mlp_test_loss: 0.5987 - mlp_test_binary_accuracy: 0.6621 - mlp_test_auc_20: 0.7328\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5978 - siamese_test_loss: 0.3667 - mlp_test_loss: 0.5978 - mlp_test_binary_accuracy: 0.6653 - mlp_test_auc_20: 0.7349\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5963 - siamese_test_loss: 0.3664 - mlp_test_loss: 0.5963 - mlp_test_binary_accuracy: 0.6664 - mlp_test_auc_20: 0.7361\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5938 - siamese_test_loss: 0.3677 - mlp_test_loss: 0.5938 - mlp_test_binary_accuracy: 0.6655 - mlp_test_auc_20: 0.7384\n",
      "Epoch 15/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5940 - siamese_test_loss: 0.3639 - mlp_test_loss: 0.5940 - mlp_test_binary_accuracy: 0.6680 - mlp_test_auc_20: 0.7396\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5939 - siamese_test_loss: 0.3658 - mlp_test_loss: 0.5939 - mlp_test_binary_accuracy: 0.6667 - mlp_test_auc_20: 0.7401\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5921 - siamese_test_loss: 0.3679 - mlp_test_loss: 0.5921 - mlp_test_binary_accuracy: 0.6685 - mlp_test_auc_20: 0.7413\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5920 - siamese_test_loss: 0.3664 - mlp_test_loss: 0.5920 - mlp_test_binary_accuracy: 0.6726 - mlp_test_auc_20: 0.7416\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5915 - siamese_test_loss: 0.3679 - mlp_test_loss: 0.5915 - mlp_test_binary_accuracy: 0.6703 - mlp_test_auc_20: 0.7424\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5892 - siamese_test_loss: 0.3677 - mlp_test_loss: 0.5892 - mlp_test_binary_accuracy: 0.6777 - mlp_test_auc_20: 0.7459\n",
      "Epoch 21/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5913 - siamese_test_loss: 0.3694 - mlp_test_loss: 0.5913 - mlp_test_binary_accuracy: 0.6802 - mlp_test_auc_20: 0.7441\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5907 - siamese_test_loss: 0.3679 - mlp_test_loss: 0.5907 - mlp_test_binary_accuracy: 0.6800 - mlp_test_auc_20: 0.7444\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5900 - siamese_test_loss: 0.3675 - mlp_test_loss: 0.5900 - mlp_test_binary_accuracy: 0.6775 - mlp_test_auc_20: 0.7453\n",
      "Epoch 24/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5903 - siamese_test_loss: 0.3670 - mlp_test_loss: 0.5903 - mlp_test_binary_accuracy: 0.6729 - mlp_test_auc_20: 0.7436\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5897 - siamese_test_loss: 0.3684 - mlp_test_loss: 0.5897 - mlp_test_binary_accuracy: 0.6736 - mlp_test_auc_20: 0.7454\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002186DD73D38> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6404 - siamese_test_loss: 0.3618 - mlp_test_loss: 0.6404 - mlp_test_binary_accuracy: 0.6211 - mlp_test_auc_20: 0.6992\n",
      "------ mlp_test_binary_accuracy: 62.11%\t ----- mlp_test_auc_20: 69.92%\n",
      "concat_fea的shape： (None, 16)\n",
      "-------------------------------------Kfold: 8 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6939 - siamese_test_loss: 0.4114 - mlp_test_loss: 0.6939 - mlp_test_binary_accuracy: 0.5314 - mlp_test_auc_21: 0.5495\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6674 - siamese_test_loss: 0.3299 - mlp_test_loss: 0.6674 - mlp_test_binary_accuracy: 0.6028 - mlp_test_auc_21: 0.6555\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6440 - siamese_test_loss: 0.3275 - mlp_test_loss: 0.6440 - mlp_test_binary_accuracy: 0.6283 - mlp_test_auc_21: 0.6867\n",
      "Epoch 4/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6278 - siamese_test_loss: 0.3403 - mlp_test_loss: 0.6278 - mlp_test_binary_accuracy: 0.6423 - mlp_test_auc_21: 0.7047\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6186 - siamese_test_loss: 0.3497 - mlp_test_loss: 0.6186 - mlp_test_binary_accuracy: 0.6469 - mlp_test_auc_21: 0.7134\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6103 - siamese_test_loss: 0.3573 - mlp_test_loss: 0.6103 - mlp_test_binary_accuracy: 0.6513 - mlp_test_auc_21: 0.7229\n",
      "Epoch 7/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6073 - siamese_test_loss: 0.3593 - mlp_test_loss: 0.6073 - mlp_test_binary_accuracy: 0.6589 - mlp_test_auc_21: 0.7253\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6031 - siamese_test_loss: 0.3594 - mlp_test_loss: 0.6031 - mlp_test_binary_accuracy: 0.6612 - mlp_test_auc_21: 0.7307\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6006 - siamese_test_loss: 0.3624 - mlp_test_loss: 0.6006 - mlp_test_binary_accuracy: 0.6653 - mlp_test_auc_21: 0.7333\n",
      "Epoch 10/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5993 - siamese_test_loss: 0.3611 - mlp_test_loss: 0.5993 - mlp_test_binary_accuracy: 0.6699 - mlp_test_auc_21: 0.7349\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5984 - siamese_test_loss: 0.3675 - mlp_test_loss: 0.5984 - mlp_test_binary_accuracy: 0.6687 - mlp_test_auc_21: 0.7374\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5972 - siamese_test_loss: 0.3641 - mlp_test_loss: 0.5972 - mlp_test_binary_accuracy: 0.6731 - mlp_test_auc_21: 0.7381\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5954 - siamese_test_loss: 0.3662 - mlp_test_loss: 0.5954 - mlp_test_binary_accuracy: 0.6701 - mlp_test_auc_21: 0.7401\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5943 - siamese_test_loss: 0.3650 - mlp_test_loss: 0.5943 - mlp_test_binary_accuracy: 0.6759 - mlp_test_auc_21: 0.7419\n",
      "Epoch 15/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5939 - siamese_test_loss: 0.3684 - mlp_test_loss: 0.5939 - mlp_test_binary_accuracy: 0.6722 - mlp_test_auc_21: 0.7413\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5937 - siamese_test_loss: 0.3660 - mlp_test_loss: 0.5937 - mlp_test_binary_accuracy: 0.6733 - mlp_test_auc_21: 0.7424\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5909 - siamese_test_loss: 0.3686 - mlp_test_loss: 0.5909 - mlp_test_binary_accuracy: 0.6795 - mlp_test_auc_21: 0.7462\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5901 - siamese_test_loss: 0.3674 - mlp_test_loss: 0.5901 - mlp_test_binary_accuracy: 0.6793 - mlp_test_auc_21: 0.7463\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5920 - siamese_test_loss: 0.3612 - mlp_test_loss: 0.5920 - mlp_test_binary_accuracy: 0.6765 - mlp_test_auc_21: 0.7442\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5895 - siamese_test_loss: 0.3652 - mlp_test_loss: 0.5895 - mlp_test_binary_accuracy: 0.6855 - mlp_test_auc_21: 0.7478\n",
      "Epoch 21/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5903 - siamese_test_loss: 0.3650 - mlp_test_loss: 0.5903 - mlp_test_binary_accuracy: 0.6814 - mlp_test_auc_21: 0.7471\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5907 - siamese_test_loss: 0.3663 - mlp_test_loss: 0.5907 - mlp_test_binary_accuracy: 0.6795 - mlp_test_auc_21: 0.7461\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5896 - siamese_test_loss: 0.3657 - mlp_test_loss: 0.5896 - mlp_test_binary_accuracy: 0.6791 - mlp_test_auc_21: 0.7471\n",
      "Epoch 24/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5886 - siamese_test_loss: 0.3661 - mlp_test_loss: 0.5886 - mlp_test_binary_accuracy: 0.6807 - mlp_test_auc_21: 0.7486\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5882 - siamese_test_loss: 0.3650 - mlp_test_loss: 0.5882 - mlp_test_binary_accuracy: 0.6841 - mlp_test_auc_21: 0.7492\n",
      "Epoch 26/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5877 - siamese_test_loss: 0.3650 - mlp_test_loss: 0.5877 - mlp_test_binary_accuracy: 0.6853 - mlp_test_auc_21: 0.7509\n",
      "Epoch 27/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5913 - siamese_test_loss: 0.3625 - mlp_test_loss: 0.5913 - mlp_test_binary_accuracy: 0.6793 - mlp_test_auc_21: 0.7448\n",
      "Epoch 28/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5865 - siamese_test_loss: 0.3634 - mlp_test_loss: 0.5865 - mlp_test_binary_accuracy: 0.6846 - mlp_test_auc_21: 0.7507\n",
      "Epoch 29/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5875 - siamese_test_loss: 0.3679 - mlp_test_loss: 0.5875 - mlp_test_binary_accuracy: 0.6846 - mlp_test_auc_21: 0.7503\n",
      "Epoch 30/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5852 - siamese_test_loss: 0.3660 - mlp_test_loss: 0.5852 - mlp_test_binary_accuracy: 0.6887 - mlp_test_auc_21: 0.7523\n",
      "Epoch 31/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5865 - siamese_test_loss: 0.3638 - mlp_test_loss: 0.5865 - mlp_test_binary_accuracy: 0.6857 - mlp_test_auc_21: 0.7512\n",
      "Epoch 32/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5856 - siamese_test_loss: 0.3704 - mlp_test_loss: 0.5856 - mlp_test_binary_accuracy: 0.6864 - mlp_test_auc_21: 0.7533\n",
      "Epoch 33/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5860 - siamese_test_loss: 0.3650 - mlp_test_loss: 0.5860 - mlp_test_binary_accuracy: 0.6848 - mlp_test_auc_21: 0.7519\n",
      "Epoch 34/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5869 - siamese_test_loss: 0.3673 - mlp_test_loss: 0.5869 - mlp_test_binary_accuracy: 0.6860 - mlp_test_auc_21: 0.7511\n",
      "Epoch 35/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5844 - siamese_test_loss: 0.3674 - mlp_test_loss: 0.5844 - mlp_test_binary_accuracy: 0.6899 - mlp_test_auc_21: 0.7538\n",
      "Epoch 36/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5857 - siamese_test_loss: 0.3664 - mlp_test_loss: 0.5857 - mlp_test_binary_accuracy: 0.6876 - mlp_test_auc_21: 0.7526\n",
      "Epoch 37/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5856 - siamese_test_loss: 0.3662 - mlp_test_loss: 0.5856 - mlp_test_binary_accuracy: 0.6883 - mlp_test_auc_21: 0.7522\n",
      "Epoch 38/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5835 - siamese_test_loss: 0.3679 - mlp_test_loss: 0.5835 - mlp_test_binary_accuracy: 0.6850 - mlp_test_auc_21: 0.7536\n",
      "Epoch 39/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5850 - siamese_test_loss: 0.3688 - mlp_test_loss: 0.5850 - mlp_test_binary_accuracy: 0.6853 - mlp_test_auc_21: 0.7528\n",
      "Epoch 40/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5848 - siamese_test_loss: 0.3684 - mlp_test_loss: 0.5848 - mlp_test_binary_accuracy: 0.6910 - mlp_test_auc_21: 0.7538\n",
      "Epoch 41/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5831 - siamese_test_loss: 0.3720 - mlp_test_loss: 0.5831 - mlp_test_binary_accuracy: 0.6862 - mlp_test_auc_21: 0.7553\n",
      "Epoch 42/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5829 - siamese_test_loss: 0.3717 - mlp_test_loss: 0.5829 - mlp_test_binary_accuracy: 0.6855 - mlp_test_auc_21: 0.7559\n",
      "Epoch 43/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5831 - siamese_test_loss: 0.3739 - mlp_test_loss: 0.5831 - mlp_test_binary_accuracy: 0.6867 - mlp_test_auc_21: 0.7548\n",
      "Epoch 44/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5813 - siamese_test_loss: 0.3768 - mlp_test_loss: 0.5813 - mlp_test_binary_accuracy: 0.6931 - mlp_test_auc_21: 0.7570\n",
      "Epoch 45/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5816 - siamese_test_loss: 0.3773 - mlp_test_loss: 0.5816 - mlp_test_binary_accuracy: 0.6848 - mlp_test_auc_21: 0.7571\n",
      "Epoch 46/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5819 - siamese_test_loss: 0.3773 - mlp_test_loss: 0.5819 - mlp_test_binary_accuracy: 0.6839 - mlp_test_auc_21: 0.7568\n",
      "Epoch 47/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5794 - siamese_test_loss: 0.3777 - mlp_test_loss: 0.5794 - mlp_test_binary_accuracy: 0.6883 - mlp_test_auc_21: 0.7579\n",
      "Epoch 48/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5797 - siamese_test_loss: 0.3788 - mlp_test_loss: 0.5797 - mlp_test_binary_accuracy: 0.6880 - mlp_test_auc_21: 0.7592\n",
      "Epoch 49/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5812 - siamese_test_loss: 0.3816 - mlp_test_loss: 0.5812 - mlp_test_binary_accuracy: 0.6922 - mlp_test_auc_21: 0.7569\n",
      "Epoch 50/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5797 - siamese_test_loss: 0.3865 - mlp_test_loss: 0.5797 - mlp_test_binary_accuracy: 0.6912 - mlp_test_auc_21: 0.7582\n",
      "Epoch 51/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5794 - siamese_test_loss: 0.3849 - mlp_test_loss: 0.5794 - mlp_test_binary_accuracy: 0.6871 - mlp_test_auc_21: 0.7596\n",
      "Epoch 52/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5790 - siamese_test_loss: 0.3908 - mlp_test_loss: 0.5790 - mlp_test_binary_accuracy: 0.6963 - mlp_test_auc_21: 0.7597\n",
      "Epoch 53/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5776 - siamese_test_loss: 0.3894 - mlp_test_loss: 0.5776 - mlp_test_binary_accuracy: 0.6940 - mlp_test_auc_21: 0.7616\n",
      "Epoch 54/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5776 - siamese_test_loss: 0.3929 - mlp_test_loss: 0.5776 - mlp_test_binary_accuracy: 0.6883 - mlp_test_auc_21: 0.7611\n",
      "Epoch 55/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5765 - siamese_test_loss: 0.3960 - mlp_test_loss: 0.5765 - mlp_test_binary_accuracy: 0.6945 - mlp_test_auc_21: 0.7628\n",
      "Epoch 56/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5755 - siamese_test_loss: 0.3990 - mlp_test_loss: 0.5755 - mlp_test_binary_accuracy: 0.6926 - mlp_test_auc_21: 0.7639\n",
      "Epoch 57/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5753 - siamese_test_loss: 0.4007 - mlp_test_loss: 0.5753 - mlp_test_binary_accuracy: 0.6899 - mlp_test_auc_21: 0.7631\n",
      "Epoch 58/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5751 - siamese_test_loss: 0.4068 - mlp_test_loss: 0.5751 - mlp_test_binary_accuracy: 0.6940 - mlp_test_auc_21: 0.7636\n",
      "Epoch 59/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5749 - siamese_test_loss: 0.4070 - mlp_test_loss: 0.5749 - mlp_test_binary_accuracy: 0.6887 - mlp_test_auc_21: 0.7637\n",
      "Epoch 60/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5739 - siamese_test_loss: 0.4072 - mlp_test_loss: 0.5739 - mlp_test_binary_accuracy: 0.6892 - mlp_test_auc_21: 0.7657\n",
      "Epoch 61/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5724 - siamese_test_loss: 0.4089 - mlp_test_loss: 0.5724 - mlp_test_binary_accuracy: 0.6956 - mlp_test_auc_21: 0.7671\n",
      "Epoch 62/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5701 - siamese_test_loss: 0.4134 - mlp_test_loss: 0.5701 - mlp_test_binary_accuracy: 0.7000 - mlp_test_auc_21: 0.7694\n",
      "Epoch 63/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5715 - siamese_test_loss: 0.4086 - mlp_test_loss: 0.5715 - mlp_test_binary_accuracy: 0.6947 - mlp_test_auc_21: 0.7680\n",
      "Epoch 64/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5700 - siamese_test_loss: 0.4090 - mlp_test_loss: 0.5700 - mlp_test_binary_accuracy: 0.6968 - mlp_test_auc_21: 0.7694\n",
      "Epoch 65/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5701 - siamese_test_loss: 0.4089 - mlp_test_loss: 0.5701 - mlp_test_binary_accuracy: 0.7014 - mlp_test_auc_21: 0.7689\n",
      "Epoch 66/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5672 - siamese_test_loss: 0.4164 - mlp_test_loss: 0.5672 - mlp_test_binary_accuracy: 0.6981 - mlp_test_auc_21: 0.7720\n",
      "Epoch 67/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5679 - siamese_test_loss: 0.4137 - mlp_test_loss: 0.5679 - mlp_test_binary_accuracy: 0.7002 - mlp_test_auc_21: 0.7716\n",
      "Epoch 68/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5691 - siamese_test_loss: 0.4202 - mlp_test_loss: 0.5691 - mlp_test_binary_accuracy: 0.6997 - mlp_test_auc_21: 0.7704\n",
      "Epoch 69/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5675 - siamese_test_loss: 0.4152 - mlp_test_loss: 0.5675 - mlp_test_binary_accuracy: 0.6993 - mlp_test_auc_21: 0.7721\n",
      "Epoch 70/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5661 - siamese_test_loss: 0.4250 - mlp_test_loss: 0.5661 - mlp_test_binary_accuracy: 0.7009 - mlp_test_auc_21: 0.7733\n",
      "Epoch 71/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5656 - siamese_test_loss: 0.4244 - mlp_test_loss: 0.5656 - mlp_test_binary_accuracy: 0.7025 - mlp_test_auc_21: 0.7735\n",
      "Epoch 72/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5655 - siamese_test_loss: 0.4221 - mlp_test_loss: 0.5655 - mlp_test_binary_accuracy: 0.7039 - mlp_test_auc_21: 0.7747\n",
      "Epoch 73/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5659 - siamese_test_loss: 0.4258 - mlp_test_loss: 0.5659 - mlp_test_binary_accuracy: 0.6981 - mlp_test_auc_21: 0.7726\n",
      "Epoch 74/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5648 - siamese_test_loss: 0.4296 - mlp_test_loss: 0.5648 - mlp_test_binary_accuracy: 0.7007 - mlp_test_auc_21: 0.7746\n",
      "Epoch 75/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5638 - siamese_test_loss: 0.4277 - mlp_test_loss: 0.5638 - mlp_test_binary_accuracy: 0.6993 - mlp_test_auc_21: 0.7754\n",
      "Epoch 76/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5628 - siamese_test_loss: 0.4238 - mlp_test_loss: 0.5628 - mlp_test_binary_accuracy: 0.7071 - mlp_test_auc_21: 0.7760\n",
      "Epoch 77/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5646 - siamese_test_loss: 0.4373 - mlp_test_loss: 0.5646 - mlp_test_binary_accuracy: 0.7014 - mlp_test_auc_21: 0.7747\n",
      "Epoch 78/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5632 - siamese_test_loss: 0.4294 - mlp_test_loss: 0.5632 - mlp_test_binary_accuracy: 0.7043 - mlp_test_auc_21: 0.7748\n",
      "Epoch 79/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5618 - siamese_test_loss: 0.4285 - mlp_test_loss: 0.5618 - mlp_test_binary_accuracy: 0.6993 - mlp_test_auc_21: 0.7768\n",
      "Epoch 80/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5614 - siamese_test_loss: 0.4267 - mlp_test_loss: 0.5614 - mlp_test_binary_accuracy: 0.7057 - mlp_test_auc_21: 0.7784\n",
      "Epoch 81/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5608 - siamese_test_loss: 0.4361 - mlp_test_loss: 0.5608 - mlp_test_binary_accuracy: 0.7027 - mlp_test_auc_21: 0.7780\n",
      "Epoch 82/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5619 - siamese_test_loss: 0.4293 - mlp_test_loss: 0.5619 - mlp_test_binary_accuracy: 0.6997 - mlp_test_auc_21: 0.7775\n",
      "Epoch 83/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5608 - siamese_test_loss: 0.4275 - mlp_test_loss: 0.5608 - mlp_test_binary_accuracy: 0.7057 - mlp_test_auc_21: 0.7785\n",
      "Epoch 84/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5617 - siamese_test_loss: 0.4310 - mlp_test_loss: 0.5617 - mlp_test_binary_accuracy: 0.6995 - mlp_test_auc_21: 0.7773\n",
      "Epoch 85/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5603 - siamese_test_loss: 0.4313 - mlp_test_loss: 0.5603 - mlp_test_binary_accuracy: 0.7023 - mlp_test_auc_21: 0.7796\n",
      "Epoch 86/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5593 - siamese_test_loss: 0.4324 - mlp_test_loss: 0.5593 - mlp_test_binary_accuracy: 0.7080 - mlp_test_auc_21: 0.7808\n",
      "Epoch 87/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5591 - siamese_test_loss: 0.4362 - mlp_test_loss: 0.5591 - mlp_test_binary_accuracy: 0.7080 - mlp_test_auc_21: 0.7814\n",
      "Epoch 88/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5584 - siamese_test_loss: 0.4251 - mlp_test_loss: 0.5584 - mlp_test_binary_accuracy: 0.7023 - mlp_test_auc_21: 0.7810\n",
      "Epoch 89/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5580 - siamese_test_loss: 0.4265 - mlp_test_loss: 0.5580 - mlp_test_binary_accuracy: 0.7046 - mlp_test_auc_21: 0.7819\n",
      "Epoch 90/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5577 - siamese_test_loss: 0.4330 - mlp_test_loss: 0.5577 - mlp_test_binary_accuracy: 0.7041 - mlp_test_auc_21: 0.7815\n",
      "Epoch 91/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5563 - siamese_test_loss: 0.4287 - mlp_test_loss: 0.5563 - mlp_test_binary_accuracy: 0.7076 - mlp_test_auc_21: 0.7827\n",
      "Epoch 92/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5583 - siamese_test_loss: 0.4308 - mlp_test_loss: 0.5583 - mlp_test_binary_accuracy: 0.7048 - mlp_test_auc_21: 0.7812\n",
      "Epoch 93/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5575 - siamese_test_loss: 0.4300 - mlp_test_loss: 0.5575 - mlp_test_binary_accuracy: 0.7101 - mlp_test_auc_21: 0.7817\n",
      "Epoch 94/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5570 - siamese_test_loss: 0.4340 - mlp_test_loss: 0.5570 - mlp_test_binary_accuracy: 0.7080 - mlp_test_auc_21: 0.7826\n",
      "Epoch 95/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5548 - siamese_test_loss: 0.4302 - mlp_test_loss: 0.5548 - mlp_test_binary_accuracy: 0.7053 - mlp_test_auc_21: 0.7847\n",
      "Epoch 96/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5557 - siamese_test_loss: 0.4376 - mlp_test_loss: 0.5557 - mlp_test_binary_accuracy: 0.7055 - mlp_test_auc_21: 0.7841\n",
      "Epoch 97/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5553 - siamese_test_loss: 0.4328 - mlp_test_loss: 0.5553 - mlp_test_binary_accuracy: 0.7041 - mlp_test_auc_21: 0.7833\n",
      "Epoch 98/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5543 - siamese_test_loss: 0.4328 - mlp_test_loss: 0.5543 - mlp_test_binary_accuracy: 0.7073 - mlp_test_auc_21: 0.7856\n",
      "Epoch 99/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5521 - siamese_test_loss: 0.4342 - mlp_test_loss: 0.5521 - mlp_test_binary_accuracy: 0.7032 - mlp_test_auc_21: 0.7865\n",
      "Epoch 100/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5540 - siamese_test_loss: 0.4302 - mlp_test_loss: 0.5540 - mlp_test_binary_accuracy: 0.7048 - mlp_test_auc_21: 0.7853\n",
      "Epoch 101/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5522 - siamese_test_loss: 0.4336 - mlp_test_loss: 0.5522 - mlp_test_binary_accuracy: 0.7057 - mlp_test_auc_21: 0.7876\n",
      "Epoch 102/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5534 - siamese_test_loss: 0.4296 - mlp_test_loss: 0.5534 - mlp_test_binary_accuracy: 0.7048 - mlp_test_auc_21: 0.7867\n",
      "Epoch 103/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5521 - siamese_test_loss: 0.4332 - mlp_test_loss: 0.5521 - mlp_test_binary_accuracy: 0.7066 - mlp_test_auc_21: 0.7874\n",
      "Epoch 104/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5509 - siamese_test_loss: 0.4410 - mlp_test_loss: 0.5509 - mlp_test_binary_accuracy: 0.7050 - mlp_test_auc_21: 0.7890\n",
      "Epoch 105/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5498 - siamese_test_loss: 0.4372 - mlp_test_loss: 0.5498 - mlp_test_binary_accuracy: 0.7112 - mlp_test_auc_21: 0.7890\n",
      "Epoch 106/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5505 - siamese_test_loss: 0.4449 - mlp_test_loss: 0.5505 - mlp_test_binary_accuracy: 0.7140 - mlp_test_auc_21: 0.7894\n",
      "Epoch 107/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5506 - siamese_test_loss: 0.4404 - mlp_test_loss: 0.5506 - mlp_test_binary_accuracy: 0.7089 - mlp_test_auc_21: 0.7890\n",
      "Epoch 108/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5501 - siamese_test_loss: 0.4369 - mlp_test_loss: 0.5501 - mlp_test_binary_accuracy: 0.7062 - mlp_test_auc_21: 0.7884\n",
      "Epoch 109/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5496 - siamese_test_loss: 0.4425 - mlp_test_loss: 0.5496 - mlp_test_binary_accuracy: 0.7117 - mlp_test_auc_21: 0.7906\n",
      "Epoch 110/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5480 - siamese_test_loss: 0.4414 - mlp_test_loss: 0.5480 - mlp_test_binary_accuracy: 0.7119 - mlp_test_auc_21: 0.7916\n",
      "Epoch 111/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5486 - siamese_test_loss: 0.4340 - mlp_test_loss: 0.5486 - mlp_test_binary_accuracy: 0.7119 - mlp_test_auc_21: 0.7900\n",
      "Epoch 112/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5469 - siamese_test_loss: 0.4403 - mlp_test_loss: 0.5469 - mlp_test_binary_accuracy: 0.7124 - mlp_test_auc_21: 0.7928\n",
      "Epoch 113/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5487 - siamese_test_loss: 0.4405 - mlp_test_loss: 0.5487 - mlp_test_binary_accuracy: 0.7144 - mlp_test_auc_21: 0.7903\n",
      "Epoch 114/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5463 - siamese_test_loss: 0.4455 - mlp_test_loss: 0.5463 - mlp_test_binary_accuracy: 0.7138 - mlp_test_auc_21: 0.7934\n",
      "Epoch 115/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5465 - siamese_test_loss: 0.4500 - mlp_test_loss: 0.5465 - mlp_test_binary_accuracy: 0.7110 - mlp_test_auc_21: 0.7927\n",
      "Epoch 116/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5459 - siamese_test_loss: 0.4520 - mlp_test_loss: 0.5459 - mlp_test_binary_accuracy: 0.7197 - mlp_test_auc_21: 0.7938\n",
      "Epoch 117/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5461 - siamese_test_loss: 0.4533 - mlp_test_loss: 0.5461 - mlp_test_binary_accuracy: 0.7135 - mlp_test_auc_21: 0.7924\n",
      "Epoch 118/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5473 - siamese_test_loss: 0.4552 - mlp_test_loss: 0.5473 - mlp_test_binary_accuracy: 0.7076 - mlp_test_auc_21: 0.7915\n",
      "Epoch 119/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5458 - siamese_test_loss: 0.4581 - mlp_test_loss: 0.5458 - mlp_test_binary_accuracy: 0.7156 - mlp_test_auc_21: 0.7936\n",
      "Epoch 120/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5442 - siamese_test_loss: 0.4571 - mlp_test_loss: 0.5442 - mlp_test_binary_accuracy: 0.7165 - mlp_test_auc_21: 0.7952\n",
      "Epoch 121/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5447 - siamese_test_loss: 0.4585 - mlp_test_loss: 0.5447 - mlp_test_binary_accuracy: 0.7140 - mlp_test_auc_21: 0.7936\n",
      "Epoch 122/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5446 - siamese_test_loss: 0.4681 - mlp_test_loss: 0.5446 - mlp_test_binary_accuracy: 0.7167 - mlp_test_auc_21: 0.7950\n",
      "Epoch 123/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5420 - siamese_test_loss: 0.4613 - mlp_test_loss: 0.5420 - mlp_test_binary_accuracy: 0.7179 - mlp_test_auc_21: 0.7978\n",
      "Epoch 124/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5449 - siamese_test_loss: 0.4630 - mlp_test_loss: 0.5449 - mlp_test_binary_accuracy: 0.7115 - mlp_test_auc_21: 0.7934\n",
      "Epoch 125/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5424 - siamese_test_loss: 0.4628 - mlp_test_loss: 0.5424 - mlp_test_binary_accuracy: 0.7184 - mlp_test_auc_21: 0.7966\n",
      "Epoch 126/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5433 - siamese_test_loss: 0.4650 - mlp_test_loss: 0.5433 - mlp_test_binary_accuracy: 0.7190 - mlp_test_auc_21: 0.7961\n",
      "Epoch 127/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5418 - siamese_test_loss: 0.4716 - mlp_test_loss: 0.5418 - mlp_test_binary_accuracy: 0.7177 - mlp_test_auc_21: 0.7973\n",
      "Epoch 128/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5401 - siamese_test_loss: 0.4690 - mlp_test_loss: 0.5401 - mlp_test_binary_accuracy: 0.7236 - mlp_test_auc_21: 0.7984\n",
      "Epoch 129/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5402 - siamese_test_loss: 0.4699 - mlp_test_loss: 0.5402 - mlp_test_binary_accuracy: 0.7165 - mlp_test_auc_21: 0.7991\n",
      "Epoch 130/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5417 - siamese_test_loss: 0.4753 - mlp_test_loss: 0.5417 - mlp_test_binary_accuracy: 0.7177 - mlp_test_auc_21: 0.7969\n",
      "Epoch 131/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5399 - siamese_test_loss: 0.4803 - mlp_test_loss: 0.5399 - mlp_test_binary_accuracy: 0.7147 - mlp_test_auc_21: 0.7982\n",
      "Epoch 132/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5391 - siamese_test_loss: 0.4815 - mlp_test_loss: 0.5391 - mlp_test_binary_accuracy: 0.7195 - mlp_test_auc_21: 0.7993\n",
      "Epoch 133/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5405 - siamese_test_loss: 0.4805 - mlp_test_loss: 0.5405 - mlp_test_binary_accuracy: 0.7174 - mlp_test_auc_21: 0.7978\n",
      "Epoch 134/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5393 - siamese_test_loss: 0.4764 - mlp_test_loss: 0.5393 - mlp_test_binary_accuracy: 0.7223 - mlp_test_auc_21: 0.7989\n",
      "Epoch 135/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5385 - siamese_test_loss: 0.4751 - mlp_test_loss: 0.5385 - mlp_test_binary_accuracy: 0.7174 - mlp_test_auc_21: 0.7994\n",
      "Epoch 136/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5374 - siamese_test_loss: 0.4801 - mlp_test_loss: 0.5374 - mlp_test_binary_accuracy: 0.7252 - mlp_test_auc_21: 0.8010\n",
      "Epoch 137/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5377 - siamese_test_loss: 0.4823 - mlp_test_loss: 0.5377 - mlp_test_binary_accuracy: 0.7243 - mlp_test_auc_21: 0.8011\n",
      "Epoch 138/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5363 - siamese_test_loss: 0.4837 - mlp_test_loss: 0.5363 - mlp_test_binary_accuracy: 0.7220 - mlp_test_auc_21: 0.8021\n",
      "Epoch 139/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5357 - siamese_test_loss: 0.4850 - mlp_test_loss: 0.5357 - mlp_test_binary_accuracy: 0.7273 - mlp_test_auc_21: 0.8024\n",
      "Epoch 140/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5379 - siamese_test_loss: 0.4844 - mlp_test_loss: 0.5379 - mlp_test_binary_accuracy: 0.7213 - mlp_test_auc_21: 0.8009\n",
      "Epoch 141/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5359 - siamese_test_loss: 0.4850 - mlp_test_loss: 0.5359 - mlp_test_binary_accuracy: 0.7165 - mlp_test_auc_21: 0.8018\n",
      "Epoch 142/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5356 - siamese_test_loss: 0.4887 - mlp_test_loss: 0.5356 - mlp_test_binary_accuracy: 0.7225 - mlp_test_auc_21: 0.8027\n",
      "Epoch 143/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5338 - siamese_test_loss: 0.4935 - mlp_test_loss: 0.5338 - mlp_test_binary_accuracy: 0.7255 - mlp_test_auc_21: 0.8041\n",
      "Epoch 144/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5351 - siamese_test_loss: 0.4918 - mlp_test_loss: 0.5351 - mlp_test_binary_accuracy: 0.7308 - mlp_test_auc_21: 0.8036\n",
      "Epoch 145/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5345 - siamese_test_loss: 0.4881 - mlp_test_loss: 0.5345 - mlp_test_binary_accuracy: 0.7248 - mlp_test_auc_21: 0.8030\n",
      "Epoch 146/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5338 - siamese_test_loss: 0.4910 - mlp_test_loss: 0.5338 - mlp_test_binary_accuracy: 0.7207 - mlp_test_auc_21: 0.8042\n",
      "Epoch 147/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5336 - siamese_test_loss: 0.4915 - mlp_test_loss: 0.5336 - mlp_test_binary_accuracy: 0.7250 - mlp_test_auc_21: 0.8043\n",
      "Epoch 148/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5325 - siamese_test_loss: 0.4901 - mlp_test_loss: 0.5325 - mlp_test_binary_accuracy: 0.7271 - mlp_test_auc_21: 0.8058\n",
      "Epoch 149/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5328 - siamese_test_loss: 0.4955 - mlp_test_loss: 0.5328 - mlp_test_binary_accuracy: 0.7255 - mlp_test_auc_21: 0.8040\n",
      "Epoch 150/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5336 - siamese_test_loss: 0.4973 - mlp_test_loss: 0.5336 - mlp_test_binary_accuracy: 0.7220 - mlp_test_auc_21: 0.8037\n",
      "Epoch 151/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5309 - siamese_test_loss: 0.4952 - mlp_test_loss: 0.5309 - mlp_test_binary_accuracy: 0.7278 - mlp_test_auc_21: 0.8064\n",
      "Epoch 152/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5318 - siamese_test_loss: 0.4959 - mlp_test_loss: 0.5318 - mlp_test_binary_accuracy: 0.7287 - mlp_test_auc_21: 0.8066\n",
      "Epoch 153/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5314 - siamese_test_loss: 0.4926 - mlp_test_loss: 0.5314 - mlp_test_binary_accuracy: 0.7271 - mlp_test_auc_21: 0.8059\n",
      "Epoch 154/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5308 - siamese_test_loss: 0.4980 - mlp_test_loss: 0.5308 - mlp_test_binary_accuracy: 0.7285 - mlp_test_auc_21: 0.8064\n",
      "Epoch 155/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5306 - siamese_test_loss: 0.4981 - mlp_test_loss: 0.5306 - mlp_test_binary_accuracy: 0.7294 - mlp_test_auc_21: 0.8074\n",
      "Epoch 156/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5303 - siamese_test_loss: 0.5001 - mlp_test_loss: 0.5303 - mlp_test_binary_accuracy: 0.7275 - mlp_test_auc_21: 0.8066\n",
      "Epoch 157/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5302 - siamese_test_loss: 0.5018 - mlp_test_loss: 0.5302 - mlp_test_binary_accuracy: 0.7266 - mlp_test_auc_21: 0.8067\n",
      "Epoch 158/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5282 - siamese_test_loss: 0.5024 - mlp_test_loss: 0.5282 - mlp_test_binary_accuracy: 0.7301 - mlp_test_auc_21: 0.8088\n",
      "Epoch 159/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5301 - siamese_test_loss: 0.5026 - mlp_test_loss: 0.5301 - mlp_test_binary_accuracy: 0.7262 - mlp_test_auc_21: 0.8067\n",
      "Epoch 160/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5289 - siamese_test_loss: 0.5114 - mlp_test_loss: 0.5289 - mlp_test_binary_accuracy: 0.7282 - mlp_test_auc_21: 0.8079\n",
      "Epoch 161/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5298 - siamese_test_loss: 0.5125 - mlp_test_loss: 0.5298 - mlp_test_binary_accuracy: 0.7264 - mlp_test_auc_21: 0.8073\n",
      "Epoch 162/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5289 - siamese_test_loss: 0.5117 - mlp_test_loss: 0.5289 - mlp_test_binary_accuracy: 0.7236 - mlp_test_auc_21: 0.8079\n",
      "Epoch 163/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5274 - siamese_test_loss: 0.5115 - mlp_test_loss: 0.5274 - mlp_test_binary_accuracy: 0.7262 - mlp_test_auc_21: 0.8093\n",
      "Epoch 164/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5275 - siamese_test_loss: 0.5081 - mlp_test_loss: 0.5275 - mlp_test_binary_accuracy: 0.7337 - mlp_test_auc_21: 0.8100\n",
      "Epoch 165/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5272 - siamese_test_loss: 0.5165 - mlp_test_loss: 0.5272 - mlp_test_binary_accuracy: 0.7312 - mlp_test_auc_21: 0.8101\n",
      "Epoch 166/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5263 - siamese_test_loss: 0.5106 - mlp_test_loss: 0.5263 - mlp_test_binary_accuracy: 0.7229 - mlp_test_auc_21: 0.8096\n",
      "Epoch 167/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5257 - siamese_test_loss: 0.5189 - mlp_test_loss: 0.5257 - mlp_test_binary_accuracy: 0.7252 - mlp_test_auc_21: 0.8102\n",
      "Epoch 168/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5252 - siamese_test_loss: 0.5205 - mlp_test_loss: 0.5252 - mlp_test_binary_accuracy: 0.7241 - mlp_test_auc_21: 0.8104\n",
      "Epoch 169/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5272 - siamese_test_loss: 0.5211 - mlp_test_loss: 0.5272 - mlp_test_binary_accuracy: 0.7229 - mlp_test_auc_21: 0.8089\n",
      "Epoch 170/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5241 - siamese_test_loss: 0.5195 - mlp_test_loss: 0.5241 - mlp_test_binary_accuracy: 0.7328 - mlp_test_auc_21: 0.8117\n",
      "Epoch 171/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5244 - siamese_test_loss: 0.5238 - mlp_test_loss: 0.5244 - mlp_test_binary_accuracy: 0.7287 - mlp_test_auc_21: 0.8109\n",
      "Epoch 172/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5237 - siamese_test_loss: 0.5239 - mlp_test_loss: 0.5237 - mlp_test_binary_accuracy: 0.7264 - mlp_test_auc_21: 0.8128\n",
      "Epoch 173/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5241 - siamese_test_loss: 0.5315 - mlp_test_loss: 0.5241 - mlp_test_binary_accuracy: 0.7298 - mlp_test_auc_21: 0.8123\n",
      "Epoch 174/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5220 - siamese_test_loss: 0.5234 - mlp_test_loss: 0.5220 - mlp_test_binary_accuracy: 0.7294 - mlp_test_auc_21: 0.8141\n",
      "Epoch 175/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5245 - siamese_test_loss: 0.5315 - mlp_test_loss: 0.5245 - mlp_test_binary_accuracy: 0.7319 - mlp_test_auc_21: 0.8116\n",
      "Epoch 176/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5230 - siamese_test_loss: 0.5383 - mlp_test_loss: 0.5230 - mlp_test_binary_accuracy: 0.7308 - mlp_test_auc_21: 0.8132\n",
      "Epoch 177/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5241 - siamese_test_loss: 0.5379 - mlp_test_loss: 0.5241 - mlp_test_binary_accuracy: 0.7326 - mlp_test_auc_21: 0.8126\n",
      "Epoch 178/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5223 - siamese_test_loss: 0.5397 - mlp_test_loss: 0.5223 - mlp_test_binary_accuracy: 0.7335 - mlp_test_auc_21: 0.8131\n",
      "Epoch 179/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5213 - siamese_test_loss: 0.5378 - mlp_test_loss: 0.5213 - mlp_test_binary_accuracy: 0.7374 - mlp_test_auc_21: 0.8151\n",
      "Epoch 180/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5201 - siamese_test_loss: 0.5320 - mlp_test_loss: 0.5201 - mlp_test_binary_accuracy: 0.7340 - mlp_test_auc_21: 0.8153\n",
      "Epoch 181/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5222 - siamese_test_loss: 0.5390 - mlp_test_loss: 0.5222 - mlp_test_binary_accuracy: 0.7321 - mlp_test_auc_21: 0.8137\n",
      "Epoch 182/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5218 - siamese_test_loss: 0.5384 - mlp_test_loss: 0.5218 - mlp_test_binary_accuracy: 0.7360 - mlp_test_auc_21: 0.8139\n",
      "Epoch 183/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5207 - siamese_test_loss: 0.5422 - mlp_test_loss: 0.5207 - mlp_test_binary_accuracy: 0.7294 - mlp_test_auc_21: 0.8139\n",
      "Epoch 184/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5205 - siamese_test_loss: 0.5477 - mlp_test_loss: 0.5205 - mlp_test_binary_accuracy: 0.7340 - mlp_test_auc_21: 0.8159\n",
      "Epoch 185/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5192 - siamese_test_loss: 0.5415 - mlp_test_loss: 0.5192 - mlp_test_binary_accuracy: 0.7337 - mlp_test_auc_21: 0.8163\n",
      "Epoch 186/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5190 - siamese_test_loss: 0.5475 - mlp_test_loss: 0.5190 - mlp_test_binary_accuracy: 0.7333 - mlp_test_auc_21: 0.8162\n",
      "Epoch 187/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5186 - siamese_test_loss: 0.5445 - mlp_test_loss: 0.5186 - mlp_test_binary_accuracy: 0.7331 - mlp_test_auc_21: 0.8161\n",
      "Epoch 188/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5201 - siamese_test_loss: 0.5506 - mlp_test_loss: 0.5201 - mlp_test_binary_accuracy: 0.7337 - mlp_test_auc_21: 0.8158\n",
      "Epoch 189/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5175 - siamese_test_loss: 0.5450 - mlp_test_loss: 0.5175 - mlp_test_binary_accuracy: 0.7319 - mlp_test_auc_21: 0.8175\n",
      "Epoch 190/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5162 - siamese_test_loss: 0.5447 - mlp_test_loss: 0.5162 - mlp_test_binary_accuracy: 0.7397 - mlp_test_auc_21: 0.8182\n",
      "Epoch 191/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5176 - siamese_test_loss: 0.5390 - mlp_test_loss: 0.5176 - mlp_test_binary_accuracy: 0.7413 - mlp_test_auc_21: 0.8174\n",
      "Epoch 192/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5156 - siamese_test_loss: 0.5442 - mlp_test_loss: 0.5156 - mlp_test_binary_accuracy: 0.7388 - mlp_test_auc_21: 0.8194\n",
      "Epoch 193/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5161 - siamese_test_loss: 0.5465 - mlp_test_loss: 0.5161 - mlp_test_binary_accuracy: 0.7363 - mlp_test_auc_21: 0.8174\n",
      "Epoch 194/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5175 - siamese_test_loss: 0.5529 - mlp_test_loss: 0.5175 - mlp_test_binary_accuracy: 0.7372 - mlp_test_auc_21: 0.8171\n",
      "Epoch 195/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5174 - siamese_test_loss: 0.5521 - mlp_test_loss: 0.5174 - mlp_test_binary_accuracy: 0.7363 - mlp_test_auc_21: 0.8177\n",
      "Epoch 196/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5148 - siamese_test_loss: 0.5502 - mlp_test_loss: 0.5148 - mlp_test_binary_accuracy: 0.7386 - mlp_test_auc_21: 0.8201\n",
      "Epoch 197/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5150 - siamese_test_loss: 0.5480 - mlp_test_loss: 0.5150 - mlp_test_binary_accuracy: 0.7340 - mlp_test_auc_21: 0.8199\n",
      "Epoch 198/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5167 - siamese_test_loss: 0.5467 - mlp_test_loss: 0.5167 - mlp_test_binary_accuracy: 0.7349 - mlp_test_auc_21: 0.8181\n",
      "Epoch 199/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5145 - siamese_test_loss: 0.5512 - mlp_test_loss: 0.5145 - mlp_test_binary_accuracy: 0.7445 - mlp_test_auc_21: 0.8204\n",
      "Epoch 200/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5142 - siamese_test_loss: 0.5545 - mlp_test_loss: 0.5142 - mlp_test_binary_accuracy: 0.7402 - mlp_test_auc_21: 0.8207\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002185C8F00D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 985us/step - loss: 0.5960 - siamese_test_loss: 0.5354 - mlp_test_loss: 0.5960 - mlp_test_binary_accuracy: 0.6853 - mlp_test_auc_21: 0.7546\n",
      "------ mlp_test_binary_accuracy: 68.53%\t ----- mlp_test_auc_21: 75.46%\n",
      "concat_fea的shape： (None, 16)\n",
      "-------------------------------------Kfold: 9 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6932 - siamese_test_loss: 0.4082 - mlp_test_loss: 0.6932 - mlp_test_binary_accuracy: 0.5001 - mlp_test_auc_22: 0.5648\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6817 - siamese_test_loss: 0.3364 - mlp_test_loss: 0.6817 - mlp_test_binary_accuracy: 0.5695 - mlp_test_auc_22: 0.6431\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6707 - siamese_test_loss: 0.3168 - mlp_test_loss: 0.6707 - mlp_test_binary_accuracy: 0.6111 - mlp_test_auc_22: 0.6652\n",
      "Epoch 4/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6611 - siamese_test_loss: 0.3178 - mlp_test_loss: 0.6611 - mlp_test_binary_accuracy: 0.6313 - mlp_test_auc_22: 0.6754\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6524 - siamese_test_loss: 0.3218 - mlp_test_loss: 0.6524 - mlp_test_binary_accuracy: 0.6382 - mlp_test_auc_22: 0.6874\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6410 - siamese_test_loss: 0.3214 - mlp_test_loss: 0.6410 - mlp_test_binary_accuracy: 0.6455 - mlp_test_auc_22: 0.7025\n",
      "Epoch 7/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6327 - siamese_test_loss: 0.3297 - mlp_test_loss: 0.6327 - mlp_test_binary_accuracy: 0.6483 - mlp_test_auc_22: 0.7104\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6228 - siamese_test_loss: 0.3342 - mlp_test_loss: 0.6228 - mlp_test_binary_accuracy: 0.6572 - mlp_test_auc_22: 0.7204\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6149 - siamese_test_loss: 0.3495 - mlp_test_loss: 0.6149 - mlp_test_binary_accuracy: 0.6522 - mlp_test_auc_22: 0.7252\n",
      "Epoch 10/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6099 - siamese_test_loss: 0.3609 - mlp_test_loss: 0.6099 - mlp_test_binary_accuracy: 0.6602 - mlp_test_auc_22: 0.7289\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6054 - siamese_test_loss: 0.3649 - mlp_test_loss: 0.6054 - mlp_test_binary_accuracy: 0.6635 - mlp_test_auc_22: 0.7326\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6025 - siamese_test_loss: 0.3732 - mlp_test_loss: 0.6025 - mlp_test_binary_accuracy: 0.6646 - mlp_test_auc_22: 0.7336\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6006 - siamese_test_loss: 0.3751 - mlp_test_loss: 0.6006 - mlp_test_binary_accuracy: 0.6692 - mlp_test_auc_22: 0.7349\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5983 - siamese_test_loss: 0.3716 - mlp_test_loss: 0.5983 - mlp_test_binary_accuracy: 0.6694 - mlp_test_auc_22: 0.7379\n",
      "Epoch 15/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5972 - siamese_test_loss: 0.3671 - mlp_test_loss: 0.5972 - mlp_test_binary_accuracy: 0.6685 - mlp_test_auc_22: 0.7384\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5963 - siamese_test_loss: 0.3709 - mlp_test_loss: 0.5963 - mlp_test_binary_accuracy: 0.6651 - mlp_test_auc_22: 0.7394\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5936 - siamese_test_loss: 0.3718 - mlp_test_loss: 0.5936 - mlp_test_binary_accuracy: 0.6731 - mlp_test_auc_22: 0.7416\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5939 - siamese_test_loss: 0.3687 - mlp_test_loss: 0.5939 - mlp_test_binary_accuracy: 0.6736 - mlp_test_auc_22: 0.7416\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5926 - siamese_test_loss: 0.3679 - mlp_test_loss: 0.5926 - mlp_test_binary_accuracy: 0.6752 - mlp_test_auc_22: 0.7425\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5918 - siamese_test_loss: 0.3682 - mlp_test_loss: 0.5918 - mlp_test_binary_accuracy: 0.6715 - mlp_test_auc_22: 0.7445\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5933 - siamese_test_loss: 0.3691 - mlp_test_loss: 0.5933 - mlp_test_binary_accuracy: 0.6692 - mlp_test_auc_22: 0.7406\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5917 - siamese_test_loss: 0.3689 - mlp_test_loss: 0.5917 - mlp_test_binary_accuracy: 0.6713 - mlp_test_auc_22: 0.7433\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5919 - siamese_test_loss: 0.3722 - mlp_test_loss: 0.5919 - mlp_test_binary_accuracy: 0.6745 - mlp_test_auc_22: 0.7431\n",
      "Epoch 24/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5919 - siamese_test_loss: 0.3698 - mlp_test_loss: 0.5919 - mlp_test_binary_accuracy: 0.6736 - mlp_test_auc_22: 0.7428\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5917 - siamese_test_loss: 0.3782 - mlp_test_loss: 0.5917 - mlp_test_binary_accuracy: 0.6754 - mlp_test_auc_22: 0.7436\n",
      "Epoch 26/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5911 - siamese_test_loss: 0.3715 - mlp_test_loss: 0.5911 - mlp_test_binary_accuracy: 0.6754 - mlp_test_auc_22: 0.7443\n",
      "Epoch 27/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5918 - siamese_test_loss: 0.3756 - mlp_test_loss: 0.5918 - mlp_test_binary_accuracy: 0.6795 - mlp_test_auc_22: 0.7438\n",
      "Epoch 28/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5898 - siamese_test_loss: 0.3770 - mlp_test_loss: 0.5898 - mlp_test_binary_accuracy: 0.6784 - mlp_test_auc_22: 0.7464\n",
      "Epoch 29/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5909 - siamese_test_loss: 0.3765 - mlp_test_loss: 0.5909 - mlp_test_binary_accuracy: 0.6779 - mlp_test_auc_22: 0.7444\n",
      "Epoch 30/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5886 - siamese_test_loss: 0.3768 - mlp_test_loss: 0.5886 - mlp_test_binary_accuracy: 0.6772 - mlp_test_auc_22: 0.7471\n",
      "Epoch 31/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5895 - siamese_test_loss: 0.3707 - mlp_test_loss: 0.5895 - mlp_test_binary_accuracy: 0.6782 - mlp_test_auc_22: 0.7448\n",
      "Epoch 32/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5886 - siamese_test_loss: 0.3777 - mlp_test_loss: 0.5886 - mlp_test_binary_accuracy: 0.6759 - mlp_test_auc_22: 0.7469\n",
      "Epoch 33/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5888 - siamese_test_loss: 0.3790 - mlp_test_loss: 0.5888 - mlp_test_binary_accuracy: 0.6772 - mlp_test_auc_22: 0.7466\n",
      "Epoch 34/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5887 - siamese_test_loss: 0.3787 - mlp_test_loss: 0.5887 - mlp_test_binary_accuracy: 0.6756 - mlp_test_auc_22: 0.7467\n",
      "Epoch 35/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5879 - siamese_test_loss: 0.3808 - mlp_test_loss: 0.5879 - mlp_test_binary_accuracy: 0.6791 - mlp_test_auc_22: 0.7485\n",
      "Epoch 36/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5895 - siamese_test_loss: 0.3753 - mlp_test_loss: 0.5895 - mlp_test_binary_accuracy: 0.6759 - mlp_test_auc_22: 0.7451\n",
      "Epoch 37/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5887 - siamese_test_loss: 0.3791 - mlp_test_loss: 0.5887 - mlp_test_binary_accuracy: 0.6795 - mlp_test_auc_22: 0.7471\n",
      "Epoch 38/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5884 - siamese_test_loss: 0.3787 - mlp_test_loss: 0.5884 - mlp_test_binary_accuracy: 0.6830 - mlp_test_auc_22: 0.7474\n",
      "Epoch 39/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5873 - siamese_test_loss: 0.3803 - mlp_test_loss: 0.5873 - mlp_test_binary_accuracy: 0.6798 - mlp_test_auc_22: 0.7481\n",
      "Epoch 40/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5877 - siamese_test_loss: 0.3810 - mlp_test_loss: 0.5877 - mlp_test_binary_accuracy: 0.6772 - mlp_test_auc_22: 0.7465\n",
      "Epoch 41/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5868 - siamese_test_loss: 0.3826 - mlp_test_loss: 0.5868 - mlp_test_binary_accuracy: 0.6818 - mlp_test_auc_22: 0.7482\n",
      "Epoch 42/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5875 - siamese_test_loss: 0.3778 - mlp_test_loss: 0.5875 - mlp_test_binary_accuracy: 0.6782 - mlp_test_auc_22: 0.7475\n",
      "Epoch 43/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5865 - siamese_test_loss: 0.3888 - mlp_test_loss: 0.5865 - mlp_test_binary_accuracy: 0.6768 - mlp_test_auc_22: 0.7488\n",
      "Epoch 44/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5863 - siamese_test_loss: 0.3885 - mlp_test_loss: 0.5863 - mlp_test_binary_accuracy: 0.6775 - mlp_test_auc_22: 0.7494\n",
      "Epoch 45/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5866 - siamese_test_loss: 0.3803 - mlp_test_loss: 0.5866 - mlp_test_binary_accuracy: 0.6775 - mlp_test_auc_22: 0.7482\n",
      "Epoch 46/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5859 - siamese_test_loss: 0.3896 - mlp_test_loss: 0.5859 - mlp_test_binary_accuracy: 0.6782 - mlp_test_auc_22: 0.7499\n",
      "Epoch 47/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5856 - siamese_test_loss: 0.3921 - mlp_test_loss: 0.5856 - mlp_test_binary_accuracy: 0.6738 - mlp_test_auc_22: 0.7488\n",
      "Epoch 48/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5845 - siamese_test_loss: 0.3857 - mlp_test_loss: 0.5845 - mlp_test_binary_accuracy: 0.6827 - mlp_test_auc_22: 0.7518\n",
      "Epoch 49/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5850 - siamese_test_loss: 0.3934 - mlp_test_loss: 0.5850 - mlp_test_binary_accuracy: 0.6818 - mlp_test_auc_22: 0.7520\n",
      "Epoch 50/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5859 - siamese_test_loss: 0.3947 - mlp_test_loss: 0.5859 - mlp_test_binary_accuracy: 0.6821 - mlp_test_auc_22: 0.7489\n",
      "Epoch 51/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5853 - siamese_test_loss: 0.3937 - mlp_test_loss: 0.5853 - mlp_test_binary_accuracy: 0.6772 - mlp_test_auc_22: 0.7497\n",
      "Epoch 52/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5855 - siamese_test_loss: 0.3982 - mlp_test_loss: 0.5855 - mlp_test_binary_accuracy: 0.6782 - mlp_test_auc_22: 0.7508\n",
      "Epoch 53/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5854 - siamese_test_loss: 0.3980 - mlp_test_loss: 0.5854 - mlp_test_binary_accuracy: 0.6827 - mlp_test_auc_22: 0.7490\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002186C669558> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 985us/step - loss: 0.6513 - siamese_test_loss: 0.3838 - mlp_test_loss: 0.6513 - mlp_test_binary_accuracy: 0.6190 - mlp_test_auc_22: 0.7236\n",
      "------ mlp_test_binary_accuracy: 61.90%\t ----- mlp_test_auc_22: 72.36%\n",
      "concat_fea的shape： (None, 16)\n",
      "-------------------------------------Kfold: 10 iter-----------------------------------------\n",
      "Epoch 1/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6879 - siamese_test_loss: 0.4175 - mlp_test_loss: 0.6879 - mlp_test_binary_accuracy: 0.5382 - mlp_test_auc_23: 0.5684\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6664 - siamese_test_loss: 0.3580 - mlp_test_loss: 0.6664 - mlp_test_binary_accuracy: 0.6083 - mlp_test_auc_23: 0.6530\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6467 - siamese_test_loss: 0.3445 - mlp_test_loss: 0.6467 - mlp_test_binary_accuracy: 0.6244 - mlp_test_auc_23: 0.6783\n",
      "Epoch 4/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6310 - siamese_test_loss: 0.3441 - mlp_test_loss: 0.6310 - mlp_test_binary_accuracy: 0.6398 - mlp_test_auc_23: 0.6973\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6219 - siamese_test_loss: 0.3463 - mlp_test_loss: 0.6219 - mlp_test_binary_accuracy: 0.6465 - mlp_test_auc_23: 0.7054\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6141 - siamese_test_loss: 0.3456 - mlp_test_loss: 0.6141 - mlp_test_binary_accuracy: 0.6531 - mlp_test_auc_23: 0.7141\n",
      "Epoch 7/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6092 - siamese_test_loss: 0.3428 - mlp_test_loss: 0.6092 - mlp_test_binary_accuracy: 0.6653 - mlp_test_auc_23: 0.7211\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6055 - siamese_test_loss: 0.3416 - mlp_test_loss: 0.6055 - mlp_test_binary_accuracy: 0.6674 - mlp_test_auc_23: 0.7269\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6035 - siamese_test_loss: 0.3421 - mlp_test_loss: 0.6035 - mlp_test_binary_accuracy: 0.6602 - mlp_test_auc_23: 0.7286\n",
      "Epoch 10/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6015 - siamese_test_loss: 0.3427 - mlp_test_loss: 0.6015 - mlp_test_binary_accuracy: 0.6699 - mlp_test_auc_23: 0.7306\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5997 - siamese_test_loss: 0.3407 - mlp_test_loss: 0.5997 - mlp_test_binary_accuracy: 0.6770 - mlp_test_auc_23: 0.7337\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5990 - siamese_test_loss: 0.3425 - mlp_test_loss: 0.5990 - mlp_test_binary_accuracy: 0.6745 - mlp_test_auc_23: 0.7350\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5970 - siamese_test_loss: 0.3430 - mlp_test_loss: 0.5970 - mlp_test_binary_accuracy: 0.6798 - mlp_test_auc_23: 0.7379\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5950 - siamese_test_loss: 0.3434 - mlp_test_loss: 0.5950 - mlp_test_binary_accuracy: 0.6733 - mlp_test_auc_23: 0.7392\n",
      "Epoch 15/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5962 - siamese_test_loss: 0.3433 - mlp_test_loss: 0.5962 - mlp_test_binary_accuracy: 0.6761 - mlp_test_auc_23: 0.7389\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5951 - siamese_test_loss: 0.3438 - mlp_test_loss: 0.5951 - mlp_test_binary_accuracy: 0.6745 - mlp_test_auc_23: 0.7400\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5925 - siamese_test_loss: 0.3415 - mlp_test_loss: 0.5925 - mlp_test_binary_accuracy: 0.6862 - mlp_test_auc_23: 0.7436\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5921 - siamese_test_loss: 0.3419 - mlp_test_loss: 0.5921 - mlp_test_binary_accuracy: 0.6814 - mlp_test_auc_23: 0.7451\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5937 - siamese_test_loss: 0.3410 - mlp_test_loss: 0.5937 - mlp_test_binary_accuracy: 0.6740 - mlp_test_auc_23: 0.7425\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5886 - siamese_test_loss: 0.3412 - mlp_test_loss: 0.5886 - mlp_test_binary_accuracy: 0.6894 - mlp_test_auc_23: 0.7481\n",
      "Epoch 21/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5917 - siamese_test_loss: 0.3393 - mlp_test_loss: 0.5917 - mlp_test_binary_accuracy: 0.6837 - mlp_test_auc_23: 0.7453\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5911 - siamese_test_loss: 0.3401 - mlp_test_loss: 0.5911 - mlp_test_binary_accuracy: 0.6846 - mlp_test_auc_23: 0.7459\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5905 - siamese_test_loss: 0.3395 - mlp_test_loss: 0.5905 - mlp_test_binary_accuracy: 0.6862 - mlp_test_auc_23: 0.7472\n",
      "Epoch 24/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5900 - siamese_test_loss: 0.3349 - mlp_test_loss: 0.5900 - mlp_test_binary_accuracy: 0.6814 - mlp_test_auc_23: 0.7476\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5894 - siamese_test_loss: 0.3410 - mlp_test_loss: 0.5894 - mlp_test_binary_accuracy: 0.6811 - mlp_test_auc_23: 0.7481\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002186B87A318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6082 - siamese_test_loss: 0.3509 - mlp_test_loss: 0.6082 - mlp_test_binary_accuracy: 0.6770 - mlp_test_auc_23: 0.7543\n",
      "------ mlp_test_binary_accuracy: 67.70%\t ----- mlp_test_auc_23: 75.43%\n",
      "十折性能均值：-------- ave_acc: 67.84% (+/- 3.25%)\t ----- ave_auc_mlp:75.32% (+/- 2.61%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics  import roc_curve,auc,roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "mesh_label_A = np.array(mesh_label_A)\n",
    "mesh_label_B = np.array(mesh_label_B)\n",
    "y = np.array(y)\n",
    "\n",
    "cvacc,cvauc_mlp,cvpre,cvpre_top,cvrec,cvauc_sia=[],[],[],[],[],[]\n",
    "i =1\n",
    "\n",
    "y=np.array(y)\n",
    "disA_fea_mat=np.array(mesh_label_A)\n",
    "disB_fea_mat=np.array(mesh_label_B)\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy=1,random_state=11)\n",
    "rus_dfmA,rus_y = rus.fit_resample(X=disA_fea_mat,y=y)\n",
    "rus_dfmB,rus_y = rus.fit_resample(X=disB_fea_mat,y=y)\n",
    "print('rus_y的长度:',len(rus_y))\n",
    "\n",
    "\n",
    "seed = 201202\n",
    "np.random.seed(seed) # This is necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "python_random.seed(seed)# This is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\"\"\"10-fold\"\"\"\n",
    "Kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\n",
    "for train,test in Kfold.split(X=rus_dfmA,y=rus_y):\n",
    "    ###############################################################################################\n",
    "\n",
    "    input_shape=(24)\n",
    "    base_network = create_siamese_test(input_shape)\n",
    "    \n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "    # because we re-use the same instance `base_network`,the weights of the network will be shared across the two branches\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    \n",
    "\n",
    "    eucl_model = create_euclLayer_test(processed_a.shape[-1])\n",
    "    eucl_out = eucl_model([processed_a,processed_b]) \n",
    "      \n",
    "\n",
    "    concat_fea = tf.keras.layers.concatenate([processed_a,processed_b],axis=-1)\n",
    "    print(\"concat_fea的shape：\",concat_fea.shape)\n",
    "    MLP = create_MLP_test(concat_fea.shape[-1])\n",
    "    MLP_out = MLP(concat_fea)\n",
    "    \n",
    "    model = Model(inputs=[input_a, input_b], outputs=[eucl_out,MLP_out])\n",
    "    \n",
    "    \"\"\"L2\"\"\"\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer,'kernel_regularizer'):\n",
    "            layer.kernel_regularizer = tf.keras.regularizers.l2(0.001)\n",
    "        \n",
    "    #####################################################################################################################\n",
    "    \n",
    "    print(\"-------------------------------------Kfold: {} iter-----------------------------------------\".format(i))\n",
    "    i+=1\n",
    "\n",
    "    rms = RMSprop()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=5)\n",
    "    \n",
    "    my_loss = {\"siamese_test\":contrastive_loss,\"mlp_test\":tf.keras.losses.BinaryCrossentropy()}\n",
    "    my_loss_weight = {\"siamese_test\":0,\"mlp_test\":1}\n",
    "    my_metrics ={ \"mlp_test\":[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()]} #tf.keras.metrics.Precision(top_k=len(rus_y)//100)\n",
    "    \n",
    "    model.compile(\n",
    "#         loss= [tfa.losses.ContrastiveLoss(),tf.keras.losses.BinaryCrossentropy()],\n",
    "#         loss_weights=[0.01,1],\n",
    "#         metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()],\n",
    "        \n",
    "        loss= my_loss,\n",
    "        loss_weights=my_loss_weight,\n",
    "        metrics=my_metrics,\n",
    "        \n",
    "        optimizer=rms,\n",
    "    )\n",
    "    \n",
    "    # fit()中 shuffle=True\n",
    "    model.fit([rus_dfmA[train],rus_dfmB[train]],[rus_y[train],rus_y[train]],\n",
    "              batch_size=None,\n",
    "              epochs=200,\n",
    "              callbacks=callback,\n",
    "              shuffle=True)\n",
    "    \n",
    "    scores = model.evaluate([rus_dfmA[test],rus_dfmB[test]],[rus_y[test],rus_y[test]],\n",
    "                            verbose=1,\n",
    "                            batch_size=len(rus_y[test]),\n",
    "                            callbacks=callback)\n",
    "    \n",
    "    cvacc.append(scores[-2] * 100)\n",
    "    cvauc_mlp.append(scores[-1] * 100)\n",
    "    cvauc_sia.append(scores[-3] * 100)\n",
    "    \n",
    "    print(\"------ %s: %.2f%%\\t ----- %s: %.2f%%\" % \n",
    "           (model.metrics_names[-2],scores[-2]*100, model.metrics_names[-1],scores[-1]*100))\n",
    "     \n",
    "#print(\"ave_auc_keras: %.2f%% (+/- %.2f%%)\\t ave_topk_pre: %.2f%% (+/- %.2f%%)\\t ave_auc:%.2f%% (+/- %.2f%%)\" % \n",
    "print(\"十折性能均值：-------- ave_acc: %.2f%% (+/- %.2f%%)\\t ----- ave_auc_mlp:%.2f%% (+/- %.2f%%)\" % \n",
    "      (np.mean(cvacc), np.std(cvacc),np.mean(cvauc_mlp), np.std(cvauc_mlp)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------- ave_acc: 67.84% (+/- 3.25%)\t ----- ave_auc_mlp:75.32% (+/- 2.61%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test4: only Mesh (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35245, 48)\n"
     ]
    }
   ],
   "source": [
    "mesh_label_concat = pd.concat([mesh_label_A.reset_index(drop=True),mesh_label_B.reset_index(drop=True)],axis=1)\n",
    "print(mesh_label_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(mesh_label_concat).to_csv(\"mesh_concat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus_y的长度: 4836\n",
      "--------------Kfold: 1 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6778 - binary_accuracy: 0.5832 - auc_167: 0.6205\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 927us/step - loss: 0.6374 - binary_accuracy: 0.6372 - auc_167: 0.6935\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 943us/step - loss: 0.6108 - binary_accuracy: 0.6620 - auc_167: 0.7234\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 890us/step - loss: 0.5909 - binary_accuracy: 0.6739 - auc_167: 0.7475\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 927us/step - loss: 0.5738 - binary_accuracy: 0.6907 - auc_167: 0.7657\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 941us/step - loss: 0.5622 - binary_accuracy: 0.6967 - auc_167: 0.7776\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 969us/step - loss: 0.5499 - binary_accuracy: 0.7119 - auc_167: 0.7902\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 963us/step - loss: 0.5415 - binary_accuracy: 0.7188 - auc_167: 0.7976\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 948us/step - loss: 0.5334 - binary_accuracy: 0.7261 - auc_167: 0.8056\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5261 - binary_accuracy: 0.7323 - auc_167: 0.8115\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5186 - binary_accuracy: 0.7401 - auc_167: 0.8186\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5144 - binary_accuracy: 0.7387 - auc_167: 0.8226\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 900us/step - loss: 0.5102 - binary_accuracy: 0.7443 - auc_167: 0.8253\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 929us/step - loss: 0.5061 - binary_accuracy: 0.7463 - auc_167: 0.8274\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5003 - binary_accuracy: 0.7534 - auc_167: 0.8332\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4952 - binary_accuracy: 0.7551 - auc_167: 0.8371\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4928 - binary_accuracy: 0.7532 - auc_167: 0.8387\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 889us/step - loss: 0.4895 - binary_accuracy: 0.7601 - auc_167: 0.8413\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 904us/step - loss: 0.4864 - binary_accuracy: 0.7562 - auc_167: 0.8431\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 970us/step - loss: 0.4838 - binary_accuracy: 0.7624 - auc_167: 0.8451\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 937us/step - loss: 0.4792 - binary_accuracy: 0.7656 - auc_167: 0.8492\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 890us/step - loss: 0.4768 - binary_accuracy: 0.7668 - auc_167: 0.8498\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 992us/step - loss: 0.4726 - binary_accuracy: 0.7709 - auc_167: 0.8536\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 793us/step - loss: 0.4691 - binary_accuracy: 0.7693 - auc_167: 0.8556\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 873us/step - loss: 0.4649 - binary_accuracy: 0.7723 - auc_167: 0.8586\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 926us/step - loss: 0.4634 - binary_accuracy: 0.7739 - auc_167: 0.8596\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 926us/step - loss: 0.4578 - binary_accuracy: 0.7750 - auc_167: 0.8628\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 894us/step - loss: 0.4582 - binary_accuracy: 0.7744 - auc_167: 0.8628\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 977us/step - loss: 0.4557 - binary_accuracy: 0.7750 - auc_167: 0.8641\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 904us/step - loss: 0.4527 - binary_accuracy: 0.7806 - auc_167: 0.8662\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 948us/step - loss: 0.4505 - binary_accuracy: 0.7810 - auc_167: 0.8672\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 933us/step - loss: 0.4485 - binary_accuracy: 0.7767 - auc_167: 0.8681\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 882us/step - loss: 0.4461 - binary_accuracy: 0.7824 - auc_167: 0.8705\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4444 - binary_accuracy: 0.7812 - auc_167: 0.8713\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 941us/step - loss: 0.4438 - binary_accuracy: 0.7840 - auc_167: 0.8713\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 948us/step - loss: 0.4405 - binary_accuracy: 0.7911 - auc_167: 0.8737\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 992us/step - loss: 0.4373 - binary_accuracy: 0.7829 - auc_167: 0.8750\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 909us/step - loss: 0.4357 - binary_accuracy: 0.7914 - auc_167: 0.8763\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 992us/step - loss: 0.4340 - binary_accuracy: 0.7856 - auc_167: 0.8767\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 955us/step - loss: 0.4314 - binary_accuracy: 0.7888 - auc_167: 0.8782\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 963us/step - loss: 0.4305 - binary_accuracy: 0.7916 - auc_167: 0.8792\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 999us/step - loss: 0.4289 - binary_accuracy: 0.7845 - auc_167: 0.8804\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 919us/step - loss: 0.4279 - binary_accuracy: 0.7879 - auc_167: 0.8802\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 999us/step - loss: 0.4263 - binary_accuracy: 0.7852 - auc_167: 0.8810\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 911us/step - loss: 0.4251 - binary_accuracy: 0.7877 - auc_167: 0.8818\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 985us/step - loss: 0.4226 - binary_accuracy: 0.7948 - auc_167: 0.8836\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 999us/step - loss: 0.4201 - binary_accuracy: 0.7916 - auc_167: 0.8845\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 892us/step - loss: 0.4202 - binary_accuracy: 0.7932 - auc_167: 0.8843\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 985us/step - loss: 0.4173 - binary_accuracy: 0.7941 - auc_167: 0.8865\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 963us/step - loss: 0.4165 - binary_accuracy: 0.7900 - auc_167: 0.8866\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 884us/step - loss: 0.4172 - binary_accuracy: 0.7950 - auc_167: 0.8868\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 992us/step - loss: 0.4159 - binary_accuracy: 0.7939 - auc_167: 0.8873\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 941us/step - loss: 0.4127 - binary_accuracy: 0.7980 - auc_167: 0.8886\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 904us/step - loss: 0.4123 - binary_accuracy: 0.7957 - auc_167: 0.8886\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 955us/step - loss: 0.4104 - binary_accuracy: 0.7948 - auc_167: 0.8892\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 1000us/step - loss: 0.4092 - binary_accuracy: 0.7985 - auc_167: 0.8909\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 933us/step - loss: 0.4091 - binary_accuracy: 0.7976 - auc_167: 0.8906\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 992us/step - loss: 0.4068 - binary_accuracy: 0.7987 - auc_167: 0.8922\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 882us/step - loss: 0.4075 - binary_accuracy: 0.7980 - auc_167: 0.8913\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4045 - binary_accuracy: 0.7955 - auc_167: 0.8921\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4039 - binary_accuracy: 0.7999 - auc_167: 0.8933\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 794us/step - loss: 0.4011 - binary_accuracy: 0.7932 - auc_167: 0.8940\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 954us/step - loss: 0.4009 - binary_accuracy: 0.7969 - auc_167: 0.8945\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4008 - binary_accuracy: 0.8003 - auc_167: 0.8951\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 892us/step - loss: 0.3999 - binary_accuracy: 0.7973 - auc_167: 0.8941\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 916us/step - loss: 0.3973 - binary_accuracy: 0.7999 - auc_167: 0.8962\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 1000us/step - loss: 0.3975 - binary_accuracy: 0.7987 - auc_167: 0.8964\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 939us/step - loss: 0.3964 - binary_accuracy: 0.8010 - auc_167: 0.8970\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 952us/step - loss: 0.3951 - binary_accuracy: 0.8006 - auc_167: 0.8965\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 999us/step - loss: 0.3945 - binary_accuracy: 0.7973 - auc_167: 0.8966\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 899us/step - loss: 0.3947 - binary_accuracy: 0.7962 - auc_167: 0.8976\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 872us/step - loss: 0.3937 - binary_accuracy: 0.8031 - auc_167: 0.8973\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 0s 909us/step - loss: 0.3913 - binary_accuracy: 0.8035 - auc_167: 0.8995\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 0s 985us/step - loss: 0.3911 - binary_accuracy: 0.8077 - auc_167: 0.8996\n",
      "Epoch 75/100\n",
      "136/136 [==============================] - 0s 899us/step - loss: 0.3904 - binary_accuracy: 0.7999 - auc_167: 0.8993\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 0s 917us/step - loss: 0.3919 - binary_accuracy: 0.8006 - auc_167: 0.8988\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 0s 977us/step - loss: 0.3900 - binary_accuracy: 0.8010 - auc_167: 0.8997\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 0s 895us/step - loss: 0.3896 - binary_accuracy: 0.8061 - auc_167: 0.9003\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 0s 892us/step - loss: 0.3892 - binary_accuracy: 0.7950 - auc_167: 0.8992\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 0s 992us/step - loss: 0.3871 - binary_accuracy: 0.7989 - auc_167: 0.9003\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.3856 - binary_accuracy: 0.8049 - auc_167: 0.9018\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 0s 949us/step - loss: 0.3860 - binary_accuracy: 0.8033 - auc_167: 0.9015\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.3838 - binary_accuracy: 0.8026 - auc_167: 0.9023\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 0s 948us/step - loss: 0.3868 - binary_accuracy: 0.8031 - auc_167: 0.9012\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 0s 896us/step - loss: 0.3833 - binary_accuracy: 0.8081 - auc_167: 0.9034\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 0s 917us/step - loss: 0.3848 - binary_accuracy: 0.8001 - auc_167: 0.9013\n",
      "Epoch 87/100\n",
      "136/136 [==============================] - 0s 977us/step - loss: 0.3843 - binary_accuracy: 0.8045 - auc_167: 0.9024\n",
      "16/16 [==============================] - 0s 875us/step - loss: 0.6745 - binary_accuracy: 0.7376 - auc_167: 0.7859\n",
      " binary_accuracy: 73.76%\t ------ auc_167: 78.59%\t \n",
      "--------------Kfold: 2 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6769 - binary_accuracy: 0.5823 - auc_168: 0.6262\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 958us/step - loss: 0.6390 - binary_accuracy: 0.6441 - auc_168: 0.6950\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6133 - binary_accuracy: 0.6530 - auc_168: 0.7232\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 999us/step - loss: 0.5947 - binary_accuracy: 0.6693 - auc_168: 0.7431\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 888us/step - loss: 0.5777 - binary_accuracy: 0.6854 - auc_168: 0.7623\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 917us/step - loss: 0.5658 - binary_accuracy: 0.7015 - auc_168: 0.7751\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 992us/step - loss: 0.5552 - binary_accuracy: 0.6994 - auc_168: 0.7833\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 951us/step - loss: 0.5461 - binary_accuracy: 0.7197 - auc_168: 0.7925\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5386 - binary_accuracy: 0.7192 - auc_168: 0.7990\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 955us/step - loss: 0.5319 - binary_accuracy: 0.7256 - auc_168: 0.8063\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 890us/step - loss: 0.5249 - binary_accuracy: 0.7325 - auc_168: 0.8118\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 977us/step - loss: 0.5194 - binary_accuracy: 0.7362 - auc_168: 0.8177\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 963us/step - loss: 0.5157 - binary_accuracy: 0.7376 - auc_168: 0.8201\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 905us/step - loss: 0.5090 - binary_accuracy: 0.7415 - auc_168: 0.8254\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 936us/step - loss: 0.5049 - binary_accuracy: 0.7477 - auc_168: 0.8293\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 970us/step - loss: 0.5010 - binary_accuracy: 0.7426 - auc_168: 0.8316\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 955us/step - loss: 0.4966 - binary_accuracy: 0.7484 - auc_168: 0.8354\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 898us/step - loss: 0.4935 - binary_accuracy: 0.7495 - auc_168: 0.8381\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 884us/step - loss: 0.4873 - binary_accuracy: 0.7590 - auc_168: 0.8421\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 916us/step - loss: 0.4838 - binary_accuracy: 0.7624 - auc_168: 0.8450\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 971us/step - loss: 0.4806 - binary_accuracy: 0.7564 - auc_168: 0.8468\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4771 - binary_accuracy: 0.7615 - auc_168: 0.8501\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 990us/step - loss: 0.4736 - binary_accuracy: 0.7642 - auc_168: 0.8526\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 925us/step - loss: 0.4691 - binary_accuracy: 0.7672 - auc_168: 0.8549\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 999us/step - loss: 0.4691 - binary_accuracy: 0.7688 - auc_168: 0.8554\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 999us/step - loss: 0.4665 - binary_accuracy: 0.7688 - auc_168: 0.8569\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 924us/step - loss: 0.4626 - binary_accuracy: 0.7684 - auc_168: 0.8596\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 933us/step - loss: 0.4609 - binary_accuracy: 0.7714 - auc_168: 0.8602\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 789us/step - loss: 0.4591 - binary_accuracy: 0.7732 - auc_168: 0.8622\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 904us/step - loss: 0.4527 - binary_accuracy: 0.7723 - auc_168: 0.8651\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 919us/step - loss: 0.4539 - binary_accuracy: 0.7746 - auc_168: 0.8645\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 911us/step - loss: 0.4517 - binary_accuracy: 0.7767 - auc_168: 0.8665\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 933us/step - loss: 0.4482 - binary_accuracy: 0.7808 - auc_168: 0.8687\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 926us/step - loss: 0.4444 - binary_accuracy: 0.7785 - auc_168: 0.8711\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 911us/step - loss: 0.4446 - binary_accuracy: 0.7778 - auc_168: 0.8706\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 902us/step - loss: 0.4414 - binary_accuracy: 0.7849 - auc_168: 0.8731\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4405 - binary_accuracy: 0.7787 - auc_168: 0.8729\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4373 - binary_accuracy: 0.7803 - auc_168: 0.8752\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4372 - binary_accuracy: 0.7822 - auc_168: 0.8753\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4350 - binary_accuracy: 0.7842 - auc_168: 0.8766\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4327 - binary_accuracy: 0.7884 - auc_168: 0.8778\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 999us/step - loss: 0.4329 - binary_accuracy: 0.7870 - auc_168: 0.8777\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 889us/step - loss: 0.4306 - binary_accuracy: 0.7854 - auc_168: 0.8790\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 904us/step - loss: 0.4282 - binary_accuracy: 0.7877 - auc_168: 0.8806\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 911us/step - loss: 0.4287 - binary_accuracy: 0.7911 - auc_168: 0.8798\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 911us/step - loss: 0.4263 - binary_accuracy: 0.7902 - auc_168: 0.8813\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 882us/step - loss: 0.4255 - binary_accuracy: 0.7868 - auc_168: 0.8812\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 892us/step - loss: 0.4232 - binary_accuracy: 0.7939 - auc_168: 0.8831\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 942us/step - loss: 0.4230 - binary_accuracy: 0.7891 - auc_168: 0.8835\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 955us/step - loss: 0.4212 - binary_accuracy: 0.7888 - auc_168: 0.8841\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 904us/step - loss: 0.4191 - binary_accuracy: 0.7895 - auc_168: 0.8849\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 926us/step - loss: 0.4185 - binary_accuracy: 0.7969 - auc_168: 0.8854\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 926us/step - loss: 0.4153 - binary_accuracy: 0.7992 - auc_168: 0.8875\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 906us/step - loss: 0.4154 - binary_accuracy: 0.7976 - auc_168: 0.8874\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 947us/step - loss: 0.4130 - binary_accuracy: 0.7957 - auc_168: 0.8888\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 920us/step - loss: 0.4109 - binary_accuracy: 0.7955 - auc_168: 0.8893\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 987us/step - loss: 0.4101 - binary_accuracy: 0.7983 - auc_168: 0.8900\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 975us/step - loss: 0.4082 - binary_accuracy: 0.8003 - auc_168: 0.8914 0s - loss: 0.3966 - binary_accuracy: 0.7980 - auc_168: 0.89\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 840us/step - loss: 0.4085 - binary_accuracy: 0.8003 - auc_168: 0.8911\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 856us/step - loss: 0.4054 - binary_accuracy: 0.8017 - auc_168: 0.8930\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 939us/step - loss: 0.4045 - binary_accuracy: 0.7976 - auc_168: 0.8932\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 863us/step - loss: 0.4057 - binary_accuracy: 0.7957 - auc_168: 0.8926\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 843us/step - loss: 0.4045 - binary_accuracy: 0.7989 - auc_168: 0.8933\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 825us/step - loss: 0.4034 - binary_accuracy: 0.8001 - auc_168: 0.8939\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 837us/step - loss: 0.4028 - binary_accuracy: 0.7999 - auc_168: 0.8941\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 824us/step - loss: 0.4011 - binary_accuracy: 0.8077 - auc_168: 0.8949\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 826us/step - loss: 0.3998 - binary_accuracy: 0.8026 - auc_168: 0.8959\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 823us/step - loss: 0.3985 - binary_accuracy: 0.7989 - auc_168: 0.8964\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 950us/step - loss: 0.4008 - binary_accuracy: 0.8006 - auc_168: 0.8951\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 832us/step - loss: 0.3990 - binary_accuracy: 0.8100 - auc_168: 0.8966\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6738 - binary_accuracy: 0.6839 - auc_168: 0.7573\n",
      " binary_accuracy: 68.39%\t ------ auc_168: 75.73%\t \n",
      "--------------Kfold: 3 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 846us/step - loss: 0.6790 - binary_accuracy: 0.5786 - auc_169: 0.6087\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 847us/step - loss: 0.6413 - binary_accuracy: 0.6218 - auc_169: 0.6813\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 812us/step - loss: 0.6153 - binary_accuracy: 0.6459 - auc_169: 0.7155\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 874us/step - loss: 0.5974 - binary_accuracy: 0.6696 - auc_169: 0.7360\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 895us/step - loss: 0.5806 - binary_accuracy: 0.6811 - auc_169: 0.7564\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 822us/step - loss: 0.5687 - binary_accuracy: 0.6932 - auc_169: 0.7691\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 894us/step - loss: 0.5596 - binary_accuracy: 0.7057 - auc_169: 0.7797\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 923us/step - loss: 0.5515 - binary_accuracy: 0.7075 - auc_169: 0.7872\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 910us/step - loss: 0.5438 - binary_accuracy: 0.7183 - auc_169: 0.7954\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 903us/step - loss: 0.5375 - binary_accuracy: 0.7210 - auc_169: 0.8009\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 918us/step - loss: 0.5287 - binary_accuracy: 0.7270 - auc_169: 0.8082\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 799us/step - loss: 0.5245 - binary_accuracy: 0.7293 - auc_169: 0.8118\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 905us/step - loss: 0.5185 - binary_accuracy: 0.7286 - auc_169: 0.8171\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 897us/step - loss: 0.5112 - binary_accuracy: 0.7346 - auc_169: 0.8239\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 892us/step - loss: 0.5077 - binary_accuracy: 0.7431 - auc_169: 0.8268\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 891us/step - loss: 0.5025 - binary_accuracy: 0.7495 - auc_169: 0.8307\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 876us/step - loss: 0.4961 - binary_accuracy: 0.7482 - auc_169: 0.8354\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 915us/step - loss: 0.4954 - binary_accuracy: 0.7505 - auc_169: 0.8363\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 874us/step - loss: 0.4912 - binary_accuracy: 0.7528 - auc_169: 0.8393\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 867us/step - loss: 0.4861 - binary_accuracy: 0.7601 - auc_169: 0.8439\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 888us/step - loss: 0.4826 - binary_accuracy: 0.7569 - auc_169: 0.8452\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 894us/step - loss: 0.4780 - binary_accuracy: 0.7597 - auc_169: 0.8486\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 895us/step - loss: 0.4775 - binary_accuracy: 0.7576 - auc_169: 0.8488\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 893us/step - loss: 0.4717 - binary_accuracy: 0.7622 - auc_169: 0.8533\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 893us/step - loss: 0.4686 - binary_accuracy: 0.7661 - auc_169: 0.8554\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 817us/step - loss: 0.4663 - binary_accuracy: 0.7631 - auc_169: 0.8564\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 939us/step - loss: 0.4624 - binary_accuracy: 0.7672 - auc_169: 0.8593\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 825us/step - loss: 0.4606 - binary_accuracy: 0.7702 - auc_169: 0.8604\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 904us/step - loss: 0.4560 - binary_accuracy: 0.7748 - auc_169: 0.8635\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 936us/step - loss: 0.4554 - binary_accuracy: 0.7748 - auc_169: 0.8638\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 975us/step - loss: 0.4536 - binary_accuracy: 0.7748 - auc_169: 0.8645\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 844us/step - loss: 0.4523 - binary_accuracy: 0.7783 - auc_169: 0.8657\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 818us/step - loss: 0.4474 - binary_accuracy: 0.7787 - auc_169: 0.8684\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 871us/step - loss: 0.4467 - binary_accuracy: 0.7764 - auc_169: 0.8681\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 908us/step - loss: 0.4454 - binary_accuracy: 0.7741 - auc_169: 0.8692\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 888us/step - loss: 0.4415 - binary_accuracy: 0.7796 - auc_169: 0.8725\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 892us/step - loss: 0.4389 - binary_accuracy: 0.7796 - auc_169: 0.8739\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 919us/step - loss: 0.4388 - binary_accuracy: 0.7824 - auc_169: 0.8729\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4368 - binary_accuracy: 0.7865 - auc_169: 0.8750\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 861us/step - loss: 0.4343 - binary_accuracy: 0.7810 - auc_169: 0.8764\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 895us/step - loss: 0.4321 - binary_accuracy: 0.7858 - auc_169: 0.8779\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 915us/step - loss: 0.4315 - binary_accuracy: 0.7817 - auc_169: 0.8779\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 885us/step - loss: 0.4286 - binary_accuracy: 0.7856 - auc_169: 0.8796\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 918us/step - loss: 0.4279 - binary_accuracy: 0.7923 - auc_169: 0.8801\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4261 - binary_accuracy: 0.7909 - auc_169: 0.8813\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 874us/step - loss: 0.4237 - binary_accuracy: 0.7916 - auc_169: 0.8826\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 817us/step - loss: 0.4242 - binary_accuracy: 0.7941 - auc_169: 0.8828\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 896us/step - loss: 0.4224 - binary_accuracy: 0.7918 - auc_169: 0.8831\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 967us/step - loss: 0.4223 - binary_accuracy: 0.7943 - auc_169: 0.8832\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 833us/step - loss: 0.4216 - binary_accuracy: 0.7911 - auc_169: 0.8836\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 794us/step - loss: 0.4172 - binary_accuracy: 0.7920 - auc_169: 0.8864\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 882us/step - loss: 0.4185 - binary_accuracy: 0.7895 - auc_169: 0.8846\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 838us/step - loss: 0.4156 - binary_accuracy: 0.7985 - auc_169: 0.8871\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 899us/step - loss: 0.4127 - binary_accuracy: 0.7939 - auc_169: 0.8883\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 751us/step - loss: 0.4131 - binary_accuracy: 0.7987 - auc_169: 0.8883\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 827us/step - loss: 0.4138 - binary_accuracy: 0.7966 - auc_169: 0.8878\n",
      "16/16 [==============================] - 0s 404us/step - loss: 0.5528 - binary_accuracy: 0.7273 - auc_169: 0.8186\n",
      " binary_accuracy: 72.73%\t ------ auc_169: 81.86%\t \n",
      "--------------Kfold: 4 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 870us/step - loss: 0.6841 - binary_accuracy: 0.5558 - auc_170: 0.6234\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 915us/step - loss: 0.6555 - binary_accuracy: 0.6404 - auc_170: 0.6864\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 948us/step - loss: 0.6244 - binary_accuracy: 0.6606 - auc_170: 0.7182\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 837us/step - loss: 0.6040 - binary_accuracy: 0.6705 - auc_170: 0.7374\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 846us/step - loss: 0.5859 - binary_accuracy: 0.6861 - auc_170: 0.7564\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 958us/step - loss: 0.5736 - binary_accuracy: 0.6914 - auc_170: 0.7666\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 882us/step - loss: 0.5624 - binary_accuracy: 0.6946 - auc_170: 0.7774\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 826us/step - loss: 0.5520 - binary_accuracy: 0.7054 - auc_170: 0.7870\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 864us/step - loss: 0.5432 - binary_accuracy: 0.7144 - auc_170: 0.7956\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 905us/step - loss: 0.5365 - binary_accuracy: 0.7192 - auc_170: 0.8016\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 885us/step - loss: 0.5300 - binary_accuracy: 0.7229 - auc_170: 0.8075\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 890us/step - loss: 0.5239 - binary_accuracy: 0.7300 - auc_170: 0.8130\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 899us/step - loss: 0.5185 - binary_accuracy: 0.7344 - auc_170: 0.8172\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 904us/step - loss: 0.5111 - binary_accuracy: 0.7367 - auc_170: 0.8235\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 900us/step - loss: 0.5051 - binary_accuracy: 0.7392 - auc_170: 0.8279\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 815us/step - loss: 0.4997 - binary_accuracy: 0.7470 - auc_170: 0.8327\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 907us/step - loss: 0.4957 - binary_accuracy: 0.7491 - auc_170: 0.8368\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 904us/step - loss: 0.4911 - binary_accuracy: 0.7539 - auc_170: 0.8400\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 906us/step - loss: 0.4868 - binary_accuracy: 0.7622 - auc_170: 0.8430\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 936us/step - loss: 0.4824 - binary_accuracy: 0.7619 - auc_170: 0.8462\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 825us/step - loss: 0.4763 - binary_accuracy: 0.7647 - auc_170: 0.8502\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 817us/step - loss: 0.4739 - binary_accuracy: 0.7619 - auc_170: 0.8514\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 903us/step - loss: 0.4689 - binary_accuracy: 0.7679 - auc_170: 0.8551\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 800us/step - loss: 0.4668 - binary_accuracy: 0.7684 - auc_170: 0.8566\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 926us/step - loss: 0.4634 - binary_accuracy: 0.7700 - auc_170: 0.8592\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 908us/step - loss: 0.4611 - binary_accuracy: 0.7737 - auc_170: 0.8609\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 886us/step - loss: 0.4558 - binary_accuracy: 0.7760 - auc_170: 0.8646\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 925us/step - loss: 0.4558 - binary_accuracy: 0.7725 - auc_170: 0.8637\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4514 - binary_accuracy: 0.7753 - auc_170: 0.8663\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 953us/step - loss: 0.4482 - binary_accuracy: 0.7739 - auc_170: 0.8684\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 882us/step - loss: 0.4474 - binary_accuracy: 0.7748 - auc_170: 0.8692\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 912us/step - loss: 0.4465 - binary_accuracy: 0.7755 - auc_170: 0.8690\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 883us/step - loss: 0.4415 - binary_accuracy: 0.7778 - auc_170: 0.8722\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 910us/step - loss: 0.4387 - binary_accuracy: 0.7767 - auc_170: 0.8741\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 824us/step - loss: 0.4376 - binary_accuracy: 0.7833 - auc_170: 0.8748\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 869us/step - loss: 0.4365 - binary_accuracy: 0.7824 - auc_170: 0.8757\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 815us/step - loss: 0.4324 - binary_accuracy: 0.7812 - auc_170: 0.8774\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 841us/step - loss: 0.4318 - binary_accuracy: 0.7812 - auc_170: 0.8780\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 921us/step - loss: 0.4285 - binary_accuracy: 0.7835 - auc_170: 0.8803\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 850us/step - loss: 0.4288 - binary_accuracy: 0.7847 - auc_170: 0.8796\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 872us/step - loss: 0.4265 - binary_accuracy: 0.7863 - auc_170: 0.8804\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 808us/step - loss: 0.4258 - binary_accuracy: 0.7831 - auc_170: 0.8811\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 879us/step - loss: 0.4208 - binary_accuracy: 0.7907 - auc_170: 0.8842\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 892us/step - loss: 0.4215 - binary_accuracy: 0.7881 - auc_170: 0.8835\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 860us/step - loss: 0.4198 - binary_accuracy: 0.7822 - auc_170: 0.8842\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 875us/step - loss: 0.4178 - binary_accuracy: 0.7852 - auc_170: 0.8853\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 878us/step - loss: 0.4168 - binary_accuracy: 0.7838 - auc_170: 0.8856\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 813us/step - loss: 0.4167 - binary_accuracy: 0.7835 - auc_170: 0.8854\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 857us/step - loss: 0.4155 - binary_accuracy: 0.7900 - auc_170: 0.8871\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 884us/step - loss: 0.4101 - binary_accuracy: 0.7904 - auc_170: 0.8896\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 903us/step - loss: 0.4122 - binary_accuracy: 0.7923 - auc_170: 0.8888\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 887us/step - loss: 0.4099 - binary_accuracy: 0.7893 - auc_170: 0.8893\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 885us/step - loss: 0.4068 - binary_accuracy: 0.7953 - auc_170: 0.8917\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 803us/step - loss: 0.4081 - binary_accuracy: 0.7939 - auc_170: 0.8904\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 812us/step - loss: 0.4055 - binary_accuracy: 0.7957 - auc_170: 0.8922\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 835us/step - loss: 0.4039 - binary_accuracy: 0.7916 - auc_170: 0.8925\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 818us/step - loss: 0.4033 - binary_accuracy: 0.7925 - auc_170: 0.8928\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 882us/step - loss: 0.4015 - binary_accuracy: 0.8003 - auc_170: 0.8946\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 892us/step - loss: 0.4013 - binary_accuracy: 0.7989 - auc_170: 0.8943\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 886us/step - loss: 0.3993 - binary_accuracy: 0.7980 - auc_170: 0.8953\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 885us/step - loss: 0.3978 - binary_accuracy: 0.7996 - auc_170: 0.8967\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 892us/step - loss: 0.3967 - binary_accuracy: 0.7978 - auc_170: 0.8966\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 911us/step - loss: 0.3985 - binary_accuracy: 0.7978 - auc_170: 0.8954\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 922us/step - loss: 0.3955 - binary_accuracy: 0.8056 - auc_170: 0.8977\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 812us/step - loss: 0.3959 - binary_accuracy: 0.8031 - auc_170: 0.8970\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 905us/step - loss: 0.3933 - binary_accuracy: 0.7989 - auc_170: 0.8985\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.3926 - binary_accuracy: 0.8006 - auc_170: 0.8988\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.3930 - binary_accuracy: 0.7973 - auc_170: 0.8980\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 898us/step - loss: 0.3923 - binary_accuracy: 0.7966 - auc_170: 0.8982\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 921us/step - loss: 0.3896 - binary_accuracy: 0.8054 - auc_170: 0.9004\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 913us/step - loss: 0.3912 - binary_accuracy: 0.7960 - auc_170: 0.8993\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 926us/step - loss: 0.3902 - binary_accuracy: 0.7987 - auc_170: 0.8993\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5795 - binary_accuracy: 0.7500 - auc_170: 0.0000e+00WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "16/16 [==============================] - 0s 874us/step - loss: 0.7016 - binary_accuracy: 0.6860 - auc_170: 0.7551\n",
      " binary_accuracy: 68.60%\t ------ auc_170: 75.51%\t \n",
      "--------------Kfold: 5 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6804 - binary_accuracy: 0.5648 - auc_171: 0.6275\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6481 - binary_accuracy: 0.6356 - auc_171: 0.6864\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 970us/step - loss: 0.6212 - binary_accuracy: 0.6581 - auc_171: 0.7160\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 955us/step - loss: 0.6035 - binary_accuracy: 0.6691 - auc_171: 0.7351\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 904us/step - loss: 0.5877 - binary_accuracy: 0.6859 - auc_171: 0.7530\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 897us/step - loss: 0.5747 - binary_accuracy: 0.7001 - auc_171: 0.7668\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 879us/step - loss: 0.5621 - binary_accuracy: 0.7059 - auc_171: 0.7801\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 874us/step - loss: 0.5517 - binary_accuracy: 0.7190 - auc_171: 0.7902\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 901us/step - loss: 0.5410 - binary_accuracy: 0.7252 - auc_171: 0.7999\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 819us/step - loss: 0.5325 - binary_accuracy: 0.7325 - auc_171: 0.8069\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5246 - binary_accuracy: 0.7351 - auc_171: 0.8140\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5177 - binary_accuracy: 0.7392 - auc_171: 0.8198\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 977us/step - loss: 0.5128 - binary_accuracy: 0.7463 - auc_171: 0.8234\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5065 - binary_accuracy: 0.7495 - auc_171: 0.8286\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5008 - binary_accuracy: 0.7491 - auc_171: 0.8331\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4979 - binary_accuracy: 0.7541 - auc_171: 0.8349\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 985us/step - loss: 0.4898 - binary_accuracy: 0.7601 - auc_171: 0.8416\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4891 - binary_accuracy: 0.7622 - auc_171: 0.8418\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 948us/step - loss: 0.4842 - binary_accuracy: 0.7633 - auc_171: 0.8452\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 869us/step - loss: 0.4794 - binary_accuracy: 0.7698 - auc_171: 0.8488\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 875us/step - loss: 0.4758 - binary_accuracy: 0.7656 - auc_171: 0.8513\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 818us/step - loss: 0.4708 - binary_accuracy: 0.7746 - auc_171: 0.8550\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 734us/step - loss: 0.4699 - binary_accuracy: 0.7711 - auc_171: 0.8555\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 859us/step - loss: 0.4662 - binary_accuracy: 0.7691 - auc_171: 0.8579\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 859us/step - loss: 0.4637 - binary_accuracy: 0.7799 - auc_171: 0.8592\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 847us/step - loss: 0.4597 - binary_accuracy: 0.7785 - auc_171: 0.8621\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 806us/step - loss: 0.4562 - binary_accuracy: 0.7773 - auc_171: 0.8649\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 864us/step - loss: 0.4549 - binary_accuracy: 0.7790 - auc_171: 0.8651\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 883us/step - loss: 0.4514 - binary_accuracy: 0.7794 - auc_171: 0.8671\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 872us/step - loss: 0.4472 - binary_accuracy: 0.7806 - auc_171: 0.8698\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 800us/step - loss: 0.4477 - binary_accuracy: 0.7799 - auc_171: 0.8697\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 807us/step - loss: 0.4459 - binary_accuracy: 0.7826 - auc_171: 0.8713\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 843us/step - loss: 0.4420 - binary_accuracy: 0.7831 - auc_171: 0.8726\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 849us/step - loss: 0.4404 - binary_accuracy: 0.7891 - auc_171: 0.8743\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 812us/step - loss: 0.4387 - binary_accuracy: 0.7856 - auc_171: 0.8751\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 916us/step - loss: 0.4365 - binary_accuracy: 0.7863 - auc_171: 0.8767\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 961us/step - loss: 0.4343 - binary_accuracy: 0.7858 - auc_171: 0.8775\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 868us/step - loss: 0.4329 - binary_accuracy: 0.7941 - auc_171: 0.8786\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 911us/step - loss: 0.4294 - binary_accuracy: 0.7907 - auc_171: 0.8812\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 905us/step - loss: 0.4292 - binary_accuracy: 0.7902 - auc_171: 0.8808\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 879us/step - loss: 0.4283 - binary_accuracy: 0.7888 - auc_171: 0.8812\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 836us/step - loss: 0.4251 - binary_accuracy: 0.7925 - auc_171: 0.8831\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 892us/step - loss: 0.4236 - binary_accuracy: 0.7980 - auc_171: 0.8843\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 806us/step - loss: 0.4241 - binary_accuracy: 0.7920 - auc_171: 0.8836\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 936us/step - loss: 0.4225 - binary_accuracy: 0.7960 - auc_171: 0.8842\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 859us/step - loss: 0.4200 - binary_accuracy: 0.7957 - auc_171: 0.8860\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 846us/step - loss: 0.4201 - binary_accuracy: 0.7939 - auc_171: 0.8858\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 813us/step - loss: 0.4179 - binary_accuracy: 0.7976 - auc_171: 0.8869\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 888us/step - loss: 0.4170 - binary_accuracy: 0.7973 - auc_171: 0.8875\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 982us/step - loss: 0.4161 - binary_accuracy: 0.7969 - auc_171: 0.8878\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 810us/step - loss: 0.4134 - binary_accuracy: 0.7976 - auc_171: 0.8896\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 897us/step - loss: 0.4132 - binary_accuracy: 0.7987 - auc_171: 0.8896\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 800us/step - loss: 0.4104 - binary_accuracy: 0.8010 - auc_171: 0.8914\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 902us/step - loss: 0.4110 - binary_accuracy: 0.7992 - auc_171: 0.8905\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 799us/step - loss: 0.4100 - binary_accuracy: 0.8033 - auc_171: 0.8909\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 991us/step - loss: 0.4080 - binary_accuracy: 0.8006 - auc_171: 0.8920\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 847us/step - loss: 0.4067 - binary_accuracy: 0.8049 - auc_171: 0.8929\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 983us/step - loss: 0.4027 - binary_accuracy: 0.8051 - auc_171: 0.8953\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 873us/step - loss: 0.4030 - binary_accuracy: 0.8047 - auc_171: 0.8946\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 858us/step - loss: 0.4002 - binary_accuracy: 0.8040 - auc_171: 0.8962\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 888us/step - loss: 0.4026 - binary_accuracy: 0.8038 - auc_171: 0.8950\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 904us/step - loss: 0.4001 - binary_accuracy: 0.8038 - auc_171: 0.8959\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 919us/step - loss: 0.4012 - binary_accuracy: 0.8008 - auc_171: 0.8951\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 805us/step - loss: 0.3972 - binary_accuracy: 0.8065 - auc_171: 0.8980\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 902us/step - loss: 0.3989 - binary_accuracy: 0.8008 - auc_171: 0.8963\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 903us/step - loss: 0.3960 - binary_accuracy: 0.8079 - auc_171: 0.8984\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 929us/step - loss: 0.3952 - binary_accuracy: 0.8077 - auc_171: 0.8990\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 883us/step - loss: 0.3943 - binary_accuracy: 0.8102 - auc_171: 0.8988\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 800us/step - loss: 0.3942 - binary_accuracy: 0.8077 - auc_171: 0.8992\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 826us/step - loss: 0.3941 - binary_accuracy: 0.8091 - auc_171: 0.8992\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.3924 - binary_accuracy: 0.8093 - auc_171: 0.8999\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.3915 - binary_accuracy: 0.8104 - auc_171: 0.9001\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.3900 - binary_accuracy: 0.8107 - auc_171: 0.9011\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 0s 887us/step - loss: 0.3897 - binary_accuracy: 0.8104 - auc_171: 0.9010\n",
      "Epoch 75/100\n",
      "136/136 [==============================] - 0s 816us/step - loss: 0.3907 - binary_accuracy: 0.8061 - auc_171: 0.8998\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.3893 - binary_accuracy: 0.8086 - auc_171: 0.9012\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 0s 948us/step - loss: 0.3864 - binary_accuracy: 0.8111 - auc_171: 0.9024\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 0s 922us/step - loss: 0.3870 - binary_accuracy: 0.8104 - auc_171: 0.9018\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 0s 881us/step - loss: 0.3854 - binary_accuracy: 0.8100 - auc_171: 0.9032\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 0s 901us/step - loss: 0.3858 - binary_accuracy: 0.8123 - auc_171: 0.9026\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 0s 715us/step - loss: 0.3853 - binary_accuracy: 0.8146 - auc_171: 0.9031\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 0s 852us/step - loss: 0.3843 - binary_accuracy: 0.8107 - auc_171: 0.9033\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 0s 808us/step - loss: 0.3839 - binary_accuracy: 0.8132 - auc_171: 0.9035\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 0s 900us/step - loss: 0.3841 - binary_accuracy: 0.8102 - auc_171: 0.9041\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 0s 889us/step - loss: 0.3821 - binary_accuracy: 0.8125 - auc_171: 0.9044\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 0s 889us/step - loss: 0.3822 - binary_accuracy: 0.8084 - auc_171: 0.9039\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 986us/step - loss: 0.3818 - binary_accuracy: 0.8109 - auc_171: 0.9041\n",
      "Epoch 88/100\n",
      "136/136 [==============================] - 0s 751us/step - loss: 0.3813 - binary_accuracy: 0.8125 - auc_171: 0.9050\n",
      "Epoch 89/100\n",
      "136/136 [==============================] - 0s 815us/step - loss: 0.3806 - binary_accuracy: 0.8070 - auc_171: 0.9046\n",
      "Epoch 90/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.3790 - binary_accuracy: 0.8146 - auc_171: 0.9058\n",
      "Epoch 91/100\n",
      "136/136 [==============================] - 0s 900us/step - loss: 0.3787 - binary_accuracy: 0.8123 - auc_171: 0.9057\n",
      "Epoch 92/100\n",
      "136/136 [==============================] - 0s 789us/step - loss: 0.3760 - binary_accuracy: 0.8153 - auc_171: 0.9075\n",
      "Epoch 93/100\n",
      "136/136 [==============================] - 0s 861us/step - loss: 0.3797 - binary_accuracy: 0.8114 - auc_171: 0.9052\n",
      "Epoch 94/100\n",
      "136/136 [==============================] - 0s 993us/step - loss: 0.3782 - binary_accuracy: 0.8111 - auc_171: 0.9064\n",
      "16/16 [==============================] - 0s 331us/step - loss: 0.6927 - binary_accuracy: 0.7066 - auc_171: 0.7698\n",
      " binary_accuracy: 70.66%\t ------ auc_171: 76.98%\t \n",
      "--------------Kfold: 6 iter\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6790 - binary_accuracy: 0.5570 - auc_172: 0.6052\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 847us/step - loss: 0.6449 - binary_accuracy: 0.6243 - auc_172: 0.6727\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 940us/step - loss: 0.6190 - binary_accuracy: 0.6422 - auc_172: 0.7103\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 879us/step - loss: 0.6023 - binary_accuracy: 0.6634 - auc_172: 0.7318\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 903us/step - loss: 0.5879 - binary_accuracy: 0.6742 - auc_172: 0.7489\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 917us/step - loss: 0.5787 - binary_accuracy: 0.6877 - auc_172: 0.7600\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 994us/step - loss: 0.5703 - binary_accuracy: 0.6951 - auc_172: 0.7686\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 823us/step - loss: 0.5622 - binary_accuracy: 0.7093 - auc_172: 0.7781\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 901us/step - loss: 0.5548 - binary_accuracy: 0.7123 - auc_172: 0.7856\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 820us/step - loss: 0.5480 - binary_accuracy: 0.7210 - auc_172: 0.7916\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 870us/step - loss: 0.5420 - binary_accuracy: 0.7259 - auc_172: 0.7971\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 805us/step - loss: 0.5368 - binary_accuracy: 0.7270 - auc_172: 0.8015\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 808us/step - loss: 0.5321 - binary_accuracy: 0.7300 - auc_172: 0.8064\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 812us/step - loss: 0.5257 - binary_accuracy: 0.7309 - auc_172: 0.8116\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 915us/step - loss: 0.5211 - binary_accuracy: 0.7353 - auc_172: 0.8155\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 812us/step - loss: 0.5175 - binary_accuracy: 0.7433 - auc_172: 0.8185\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 909us/step - loss: 0.5126 - binary_accuracy: 0.7422 - auc_172: 0.8228\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 847us/step - loss: 0.5092 - binary_accuracy: 0.7417 - auc_172: 0.8258\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 926us/step - loss: 0.5030 - binary_accuracy: 0.7466 - auc_172: 0.8303\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 831us/step - loss: 0.5006 - binary_accuracy: 0.7486 - auc_172: 0.8333\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 903us/step - loss: 0.4962 - binary_accuracy: 0.7555 - auc_172: 0.8361\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 799us/step - loss: 0.4918 - binary_accuracy: 0.7548 - auc_172: 0.8391\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 815us/step - loss: 0.4902 - binary_accuracy: 0.7557 - auc_172: 0.8414\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 903us/step - loss: 0.4840 - binary_accuracy: 0.7608 - auc_172: 0.8454\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 940us/step - loss: 0.4821 - binary_accuracy: 0.7576 - auc_172: 0.8462\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 829us/step - loss: 0.4788 - binary_accuracy: 0.7654 - auc_172: 0.8496\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 821us/step - loss: 0.4742 - binary_accuracy: 0.7693 - auc_172: 0.8526\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 954us/step - loss: 0.4703 - binary_accuracy: 0.7721 - auc_172: 0.8552\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 885us/step - loss: 0.4698 - binary_accuracy: 0.7695 - auc_172: 0.8553\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 801us/step - loss: 0.4642 - binary_accuracy: 0.7732 - auc_172: 0.8591\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 835us/step - loss: 0.4641 - binary_accuracy: 0.7741 - auc_172: 0.8590\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4615 - binary_accuracy: 0.7672 - auc_172: 0.8606\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 970us/step - loss: 0.4556 - binary_accuracy: 0.7769 - auc_172: 0.8644\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 870us/step - loss: 0.4578 - binary_accuracy: 0.7732 - auc_172: 0.8625\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 861us/step - loss: 0.4552 - binary_accuracy: 0.7790 - auc_172: 0.8644\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 952us/step - loss: 0.4532 - binary_accuracy: 0.7764 - auc_172: 0.8664\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 830us/step - loss: 0.4503 - binary_accuracy: 0.7783 - auc_172: 0.8677\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 986us/step - loss: 0.4493 - binary_accuracy: 0.7852 - auc_172: 0.8683\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4463 - binary_accuracy: 0.7838 - auc_172: 0.8702\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 933us/step - loss: 0.4436 - binary_accuracy: 0.7796 - auc_172: 0.8718\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 830us/step - loss: 0.4447 - binary_accuracy: 0.7833 - auc_172: 0.8710\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 902us/step - loss: 0.4421 - binary_accuracy: 0.7817 - auc_172: 0.8727\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 912us/step - loss: 0.4393 - binary_accuracy: 0.7840 - auc_172: 0.8742\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 935us/step - loss: 0.4368 - binary_accuracy: 0.7835 - auc_172: 0.8753\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 889us/step - loss: 0.4368 - binary_accuracy: 0.7790 - auc_172: 0.8755\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 930us/step - loss: 0.4354 - binary_accuracy: 0.7877 - auc_172: 0.8767\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4328 - binary_accuracy: 0.7870 - auc_172: 0.8785\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 933us/step - loss: 0.4302 - binary_accuracy: 0.7881 - auc_172: 0.8796\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 926us/step - loss: 0.4303 - binary_accuracy: 0.7856 - auc_172: 0.8791 0s - loss: 0.4191 - binary_accuracy: 0.7915 - auc_172: 0.88\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 978us/step - loss: 0.4289 - binary_accuracy: 0.7920 - auc_172: 0.8805\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 981us/step - loss: 0.4270 - binary_accuracy: 0.7898 - auc_172: 0.8814\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4246 - binary_accuracy: 0.7955 - auc_172: 0.8829\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 833us/step - loss: 0.4241 - binary_accuracy: 0.7898 - auc_172: 0.8828\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 821us/step - loss: 0.4246 - binary_accuracy: 0.7916 - auc_172: 0.8827\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4210 - binary_accuracy: 0.7941 - auc_172: 0.8848\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 974us/step - loss: 0.4204 - binary_accuracy: 0.7943 - auc_172: 0.8851\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 827us/step - loss: 0.4201 - binary_accuracy: 0.7934 - auc_172: 0.8852\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 924us/step - loss: 0.4174 - binary_accuracy: 0.7964 - auc_172: 0.8871\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 904us/step - loss: 0.4170 - binary_accuracy: 0.7978 - auc_172: 0.8869\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4150 - binary_accuracy: 0.7932 - auc_172: 0.8879\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4142 - binary_accuracy: 0.7976 - auc_172: 0.8884\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 977us/step - loss: 0.4131 - binary_accuracy: 0.7941 - auc_172: 0.8891\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4122 - binary_accuracy: 0.7969 - auc_172: 0.8901\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 844us/step - loss: 0.4102 - binary_accuracy: 0.8010 - auc_172: 0.8907\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4116 - binary_accuracy: 0.7943 - auc_172: 0.8894\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4092 - binary_accuracy: 0.7980 - auc_172: 0.8910\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4101 - binary_accuracy: 0.7946 - auc_172: 0.8904\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4072 - binary_accuracy: 0.7985 - auc_172: 0.8917\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 876us/step - loss: 0.4075 - binary_accuracy: 0.7976 - auc_172: 0.8921\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 809us/step - loss: 0.4040 - binary_accuracy: 0.8026 - auc_172: 0.8940\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 986us/step - loss: 0.4081 - binary_accuracy: 0.8019 - auc_172: 0.8917\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 980us/step - loss: 0.4043 - binary_accuracy: 0.7987 - auc_172: 0.8934\n",
      "16/16 [==============================] - 0s 727us/step - loss: 0.5679 - binary_accuracy: 0.7045 - auc_172: 0.8028\n",
      " binary_accuracy: 70.45%\t ------ auc_172: 80.28%\t \n",
      "--------------Kfold: 7 iter\n",
      "Epoch 1/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6730 - binary_accuracy: 0.5757 - auc_173: 0.6282\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6329 - binary_accuracy: 0.6442 - auc_173: 0.6931\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 0s 935us/step - loss: 0.6081 - binary_accuracy: 0.6575 - auc_173: 0.7271\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 0s 846us/step - loss: 0.5913 - binary_accuracy: 0.6713 - auc_173: 0.7460\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 0s 965us/step - loss: 0.5772 - binary_accuracy: 0.6853 - auc_173: 0.7615\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 0s 968us/step - loss: 0.5638 - binary_accuracy: 0.6979 - auc_173: 0.7763\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 0s 966us/step - loss: 0.5527 - binary_accuracy: 0.7087 - auc_173: 0.7875\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - 0s 905us/step - loss: 0.5439 - binary_accuracy: 0.7124 - auc_173: 0.7953\n",
      "Epoch 9/100\n",
      "137/137 [==============================] - 0s 830us/step - loss: 0.5362 - binary_accuracy: 0.7241 - auc_173: 0.8034\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - 0s 922us/step - loss: 0.5272 - binary_accuracy: 0.7289 - auc_173: 0.8114\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - 0s 889us/step - loss: 0.5210 - binary_accuracy: 0.7344 - auc_173: 0.8165\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - 0s 882us/step - loss: 0.5162 - binary_accuracy: 0.7358 - auc_173: 0.8203\n",
      "Epoch 13/100\n",
      "137/137 [==============================] - 0s 821us/step - loss: 0.5116 - binary_accuracy: 0.7377 - auc_173: 0.8240\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - 0s 961us/step - loss: 0.5066 - binary_accuracy: 0.7416 - auc_173: 0.8285\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - 0s 928us/step - loss: 0.5013 - binary_accuracy: 0.7457 - auc_173: 0.8322\n",
      "Epoch 16/100\n",
      "137/137 [==============================] - 0s 827us/step - loss: 0.4985 - binary_accuracy: 0.7439 - auc_173: 0.8338\n",
      "Epoch 17/100\n",
      "137/137 [==============================] - 0s 812us/step - loss: 0.4927 - binary_accuracy: 0.7519 - auc_173: 0.8391\n",
      "Epoch 18/100\n",
      "137/137 [==============================] - 0s 922us/step - loss: 0.4907 - binary_accuracy: 0.7510 - auc_173: 0.8402\n",
      "Epoch 19/100\n",
      "137/137 [==============================] - 0s 845us/step - loss: 0.4869 - binary_accuracy: 0.7588 - auc_173: 0.8433\n",
      "Epoch 20/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4816 - binary_accuracy: 0.7581 - auc_173: 0.8461\n",
      "Epoch 21/100\n",
      "137/137 [==============================] - 0s 978us/step - loss: 0.4795 - binary_accuracy: 0.7581 - auc_173: 0.8483\n",
      "Epoch 22/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4759 - binary_accuracy: 0.7666 - auc_173: 0.8510\n",
      "Epoch 23/100\n",
      "137/137 [==============================] - 0s 920us/step - loss: 0.4733 - binary_accuracy: 0.7611 - auc_173: 0.8531\n",
      "Epoch 24/100\n",
      "137/137 [==============================] - 0s 830us/step - loss: 0.4709 - binary_accuracy: 0.7657 - auc_173: 0.8536\n",
      "Epoch 25/100\n",
      "137/137 [==============================] - 0s 805us/step - loss: 0.4704 - binary_accuracy: 0.7613 - auc_173: 0.8545\n",
      "Epoch 26/100\n",
      "137/137 [==============================] - 0s 986us/step - loss: 0.4636 - binary_accuracy: 0.7684 - auc_173: 0.8586\n",
      "Epoch 27/100\n",
      "137/137 [==============================] - 0s 970us/step - loss: 0.4623 - binary_accuracy: 0.7703 - auc_173: 0.8592\n",
      "Epoch 28/100\n",
      "137/137 [==============================] - 0s 925us/step - loss: 0.4611 - binary_accuracy: 0.7723 - auc_173: 0.8606\n",
      "Epoch 29/100\n",
      "137/137 [==============================] - 0s 970us/step - loss: 0.4589 - binary_accuracy: 0.7707 - auc_173: 0.8618\n",
      "Epoch 30/100\n",
      "137/137 [==============================] - 0s 993us/step - loss: 0.4571 - binary_accuracy: 0.7721 - auc_173: 0.8627\n",
      "Epoch 31/100\n",
      "137/137 [==============================] - 0s 919us/step - loss: 0.4570 - binary_accuracy: 0.7687 - auc_173: 0.8621\n",
      "Epoch 32/100\n",
      "137/137 [==============================] - 0s 919us/step - loss: 0.4528 - binary_accuracy: 0.7772 - auc_173: 0.8656\n",
      "Epoch 33/100\n",
      "137/137 [==============================] - 0s 912us/step - loss: 0.4516 - binary_accuracy: 0.7804 - auc_173: 0.8661\n",
      "Epoch 34/100\n",
      "137/137 [==============================] - 0s 862us/step - loss: 0.4499 - binary_accuracy: 0.7746 - auc_173: 0.8668\n",
      "Epoch 35/100\n",
      "137/137 [==============================] - 0s 868us/step - loss: 0.4474 - binary_accuracy: 0.7751 - auc_173: 0.8693\n",
      "Epoch 36/100\n",
      "137/137 [==============================] - 0s 806us/step - loss: 0.4460 - binary_accuracy: 0.7774 - auc_173: 0.8698\n",
      "Epoch 37/100\n",
      "137/137 [==============================] - 0s 829us/step - loss: 0.4441 - binary_accuracy: 0.7806 - auc_173: 0.8711\n",
      "Epoch 38/100\n",
      "137/137 [==============================] - 0s 940us/step - loss: 0.4435 - binary_accuracy: 0.7799 - auc_173: 0.8710\n",
      "Epoch 39/100\n",
      "137/137 [==============================] - 0s 901us/step - loss: 0.4408 - binary_accuracy: 0.7811 - auc_173: 0.8727\n",
      "Epoch 40/100\n",
      "137/137 [==============================] - 0s 818us/step - loss: 0.4390 - binary_accuracy: 0.7838 - auc_173: 0.8741\n",
      "Epoch 41/100\n",
      "137/137 [==============================] - 0s 916us/step - loss: 0.4355 - binary_accuracy: 0.7838 - auc_173: 0.8757\n",
      "Epoch 42/100\n",
      "137/137 [==============================] - 0s 902us/step - loss: 0.4362 - binary_accuracy: 0.7808 - auc_173: 0.8755\n",
      "Epoch 43/100\n",
      "137/137 [==============================] - 0s 941us/step - loss: 0.4355 - binary_accuracy: 0.7776 - auc_173: 0.8754\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 811us/step - loss: 0.4322 - binary_accuracy: 0.7868 - auc_173: 0.8779\n",
      "Epoch 45/100\n",
      "137/137 [==============================] - 0s 828us/step - loss: 0.4337 - binary_accuracy: 0.7834 - auc_173: 0.8768\n",
      "Epoch 46/100\n",
      "137/137 [==============================] - 0s 905us/step - loss: 0.4323 - binary_accuracy: 0.7870 - auc_173: 0.8780\n",
      "16/16 [==============================] - 0s 872us/step - loss: 0.5847 - binary_accuracy: 0.7039 - auc_173: 0.7849\n",
      " binary_accuracy: 70.39%\t ------ auc_173: 78.49%\t \n",
      "--------------Kfold: 8 iter\n",
      "Epoch 1/100\n",
      "137/137 [==============================] - 0s 948us/step - loss: 0.6761 - binary_accuracy: 0.5881 - auc_174: 0.6318\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 0s 948us/step - loss: 0.6317 - binary_accuracy: 0.6290 - auc_174: 0.6990\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 0s 897us/step - loss: 0.6043 - binary_accuracy: 0.6586 - auc_174: 0.7276\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5891 - binary_accuracy: 0.6759 - auc_174: 0.7468\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 0s 939us/step - loss: 0.5754 - binary_accuracy: 0.6818 - auc_174: 0.7621\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 0s 970us/step - loss: 0.5646 - binary_accuracy: 0.7046 - auc_174: 0.7743\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 0s 917us/step - loss: 0.5531 - binary_accuracy: 0.7082 - auc_174: 0.7865\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - 0s 949us/step - loss: 0.5450 - binary_accuracy: 0.7147 - auc_174: 0.7940\n",
      "Epoch 9/100\n",
      "137/137 [==============================] - 0s 853us/step - loss: 0.5360 - binary_accuracy: 0.7287 - auc_174: 0.8023\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - 0s 924us/step - loss: 0.5281 - binary_accuracy: 0.7255 - auc_174: 0.8089\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - 0s 930us/step - loss: 0.5218 - binary_accuracy: 0.7282 - auc_174: 0.8143\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - 0s 979us/step - loss: 0.5144 - binary_accuracy: 0.7358 - auc_174: 0.8213\n",
      "Epoch 13/100\n",
      "137/137 [==============================] - 0s 942us/step - loss: 0.5098 - binary_accuracy: 0.7347 - auc_174: 0.8245\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - 0s 940us/step - loss: 0.5024 - binary_accuracy: 0.7434 - auc_174: 0.8304\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - 0s 938us/step - loss: 0.4967 - binary_accuracy: 0.7526 - auc_174: 0.8351\n",
      "Epoch 16/100\n",
      "137/137 [==============================] - 0s 970us/step - loss: 0.4911 - binary_accuracy: 0.7510 - auc_174: 0.8391\n",
      "Epoch 17/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4856 - binary_accuracy: 0.7540 - auc_174: 0.8437\n",
      "Epoch 18/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4811 - binary_accuracy: 0.7579 - auc_174: 0.8475\n",
      "Epoch 19/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4763 - binary_accuracy: 0.7627 - auc_174: 0.8504\n",
      "Epoch 20/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4735 - binary_accuracy: 0.7641 - auc_174: 0.8523\n",
      "Epoch 21/100\n",
      "137/137 [==============================] - 0s 934us/step - loss: 0.4717 - binary_accuracy: 0.7629 - auc_174: 0.8532\n",
      "Epoch 22/100\n",
      "137/137 [==============================] - 0s 905us/step - loss: 0.4687 - binary_accuracy: 0.7632 - auc_174: 0.8554\n",
      "Epoch 23/100\n",
      "137/137 [==============================] - 0s 905us/step - loss: 0.4641 - binary_accuracy: 0.7687 - auc_174: 0.8588\n",
      "Epoch 24/100\n",
      "137/137 [==============================] - 0s 905us/step - loss: 0.4604 - binary_accuracy: 0.7742 - auc_174: 0.8609\n",
      "Epoch 25/100\n",
      "137/137 [==============================] - 0s 903us/step - loss: 0.4563 - binary_accuracy: 0.7751 - auc_174: 0.8640\n",
      "Epoch 26/100\n",
      "137/137 [==============================] - 0s 897us/step - loss: 0.4539 - binary_accuracy: 0.7733 - auc_174: 0.8650\n",
      "Epoch 27/100\n",
      "137/137 [==============================] - 0s 905us/step - loss: 0.4489 - binary_accuracy: 0.7792 - auc_174: 0.8687\n",
      "Epoch 28/100\n",
      "137/137 [==============================] - 0s 897us/step - loss: 0.4495 - binary_accuracy: 0.7746 - auc_174: 0.8680\n",
      "Epoch 29/100\n",
      "137/137 [==============================] - 0s 926us/step - loss: 0.4462 - binary_accuracy: 0.7806 - auc_174: 0.8700\n",
      "Epoch 30/100\n",
      "137/137 [==============================] - 0s 912us/step - loss: 0.4440 - binary_accuracy: 0.7753 - auc_174: 0.8712\n",
      "Epoch 31/100\n",
      "137/137 [==============================] - 0s 905us/step - loss: 0.4417 - binary_accuracy: 0.7781 - auc_174: 0.8723\n",
      "Epoch 32/100\n",
      "137/137 [==============================] - 0s 948us/step - loss: 0.4357 - binary_accuracy: 0.7859 - auc_174: 0.8766\n",
      "Epoch 33/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4364 - binary_accuracy: 0.7820 - auc_174: 0.8759\n",
      "Epoch 34/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4353 - binary_accuracy: 0.7859 - auc_174: 0.8771\n",
      "Epoch 35/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4327 - binary_accuracy: 0.7896 - auc_174: 0.8783\n",
      "Epoch 36/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4304 - binary_accuracy: 0.7822 - auc_174: 0.8787\n",
      "Epoch 37/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4293 - binary_accuracy: 0.7889 - auc_174: 0.8798\n",
      "Epoch 38/100\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4279 - binary_accuracy: 0.7875 - auc_174: 0.8802\n",
      "Epoch 39/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4258 - binary_accuracy: 0.7891 - auc_174: 0.8816\n",
      "Epoch 40/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4238 - binary_accuracy: 0.7926 - auc_174: 0.8834\n",
      "Epoch 41/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4151 - binary_accuracy: 0.7921 - auc_174: 0.8872\n",
      "Epoch 42/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4195 - binary_accuracy: 0.7969 - auc_174: 0.8854\n",
      "Epoch 43/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4168 - binary_accuracy: 0.7942 - auc_174: 0.8864\n",
      "16/16 [==============================] - 0s 999us/step - loss: 0.6991 - binary_accuracy: 0.6936 - auc_174: 0.7528\n",
      " binary_accuracy: 69.36%\t ------ auc_174: 75.28%\t \n",
      "--------------Kfold: 9 iter\n",
      "Epoch 1/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6785 - binary_accuracy: 0.5621 - auc_175: 0.6115\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6380 - binary_accuracy: 0.6262 - auc_175: 0.6842\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6114 - binary_accuracy: 0.6570 - auc_175: 0.7198\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5942 - binary_accuracy: 0.6742 - auc_175: 0.7425\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5813 - binary_accuracy: 0.6827 - auc_175: 0.7558\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5714 - binary_accuracy: 0.6915 - auc_175: 0.7675\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5611 - binary_accuracy: 0.7009 - auc_175: 0.7789\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5526 - binary_accuracy: 0.7039 - auc_175: 0.7871\n",
      "Epoch 9/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5432 - binary_accuracy: 0.7179 - auc_175: 0.7970\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5357 - binary_accuracy: 0.7179 - auc_175: 0.8037\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5288 - binary_accuracy: 0.7308 - auc_175: 0.8096\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5202 - binary_accuracy: 0.7370 - auc_175: 0.8169\n",
      "Epoch 13/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5161 - binary_accuracy: 0.7418 - auc_175: 0.8207\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5093 - binary_accuracy: 0.7432 - auc_175: 0.8255\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5044 - binary_accuracy: 0.7484 - auc_175: 0.8297\n",
      "Epoch 16/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4992 - binary_accuracy: 0.7455 - auc_175: 0.8331\n",
      "Epoch 17/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4921 - binary_accuracy: 0.7553 - auc_175: 0.8390\n",
      "Epoch 18/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4919 - binary_accuracy: 0.7489 - auc_175: 0.8388\n",
      "Epoch 19/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4862 - binary_accuracy: 0.7549 - auc_175: 0.8428\n",
      "Epoch 20/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4822 - binary_accuracy: 0.7606 - auc_175: 0.8466\n",
      "Epoch 21/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4794 - binary_accuracy: 0.7574 - auc_175: 0.8485\n",
      "Epoch 22/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4730 - binary_accuracy: 0.7620 - auc_175: 0.8527\n",
      "Epoch 23/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4732 - binary_accuracy: 0.7618 - auc_175: 0.8530\n",
      "Epoch 24/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4701 - binary_accuracy: 0.7645 - auc_175: 0.8540\n",
      "Epoch 25/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4645 - binary_accuracy: 0.7682 - auc_175: 0.8592\n",
      "Epoch 26/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4624 - binary_accuracy: 0.7668 - auc_175: 0.8599\n",
      "Epoch 27/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4616 - binary_accuracy: 0.7746 - auc_175: 0.8600\n",
      "Epoch 28/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4575 - binary_accuracy: 0.7694 - auc_175: 0.8626\n",
      "Epoch 29/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4536 - binary_accuracy: 0.7744 - auc_175: 0.8650\n",
      "Epoch 30/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4534 - binary_accuracy: 0.7733 - auc_175: 0.8656\n",
      "Epoch 31/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4515 - binary_accuracy: 0.7767 - auc_175: 0.8671\n",
      "Epoch 32/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4460 - binary_accuracy: 0.7762 - auc_175: 0.8702\n",
      "Epoch 33/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4467 - binary_accuracy: 0.7797 - auc_175: 0.8699\n",
      "Epoch 34/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4430 - binary_accuracy: 0.7797 - auc_175: 0.8720\n",
      "Epoch 35/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4415 - binary_accuracy: 0.7772 - auc_175: 0.8729\n",
      "Epoch 36/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4404 - binary_accuracy: 0.7847 - auc_175: 0.8733\n",
      "Epoch 37/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4373 - binary_accuracy: 0.7887 - auc_175: 0.8754\n",
      "Epoch 38/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4366 - binary_accuracy: 0.7834 - auc_175: 0.8756\n",
      "Epoch 39/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4349 - binary_accuracy: 0.7870 - auc_175: 0.8768\n",
      "Epoch 40/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4321 - binary_accuracy: 0.7884 - auc_175: 0.8778\n",
      "Epoch 41/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4295 - binary_accuracy: 0.7866 - auc_175: 0.8797\n",
      "Epoch 42/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4283 - binary_accuracy: 0.7937 - auc_175: 0.8802\n",
      "Epoch 43/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4263 - binary_accuracy: 0.7903 - auc_175: 0.8821\n",
      "Epoch 44/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4259 - binary_accuracy: 0.7859 - auc_175: 0.8819\n",
      "Epoch 45/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4233 - binary_accuracy: 0.7930 - auc_175: 0.8832\n",
      "Epoch 46/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4233 - binary_accuracy: 0.7935 - auc_175: 0.8836\n",
      "Epoch 47/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4199 - binary_accuracy: 0.7905 - auc_175: 0.8852\n",
      "Epoch 48/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4200 - binary_accuracy: 0.7896 - auc_175: 0.8848\n",
      "Epoch 49/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4207 - binary_accuracy: 0.7887 - auc_175: 0.8843\n",
      "16/16 [==============================] - 0s 999us/step - loss: 0.6725 - binary_accuracy: 0.6832 - auc_175: 0.7612\n",
      " binary_accuracy: 68.32%\t ------ auc_175: 76.12%\t \n",
      "--------------Kfold: 10 iter\n",
      "Epoch 1/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6736 - binary_accuracy: 0.5780 - auc_176: 0.6292\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6385 - binary_accuracy: 0.6283 - auc_176: 0.6856\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.6128 - binary_accuracy: 0.6550 - auc_176: 0.7186\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5945 - binary_accuracy: 0.6759 - auc_176: 0.7408\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5802 - binary_accuracy: 0.6807 - auc_176: 0.7562\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5673 - binary_accuracy: 0.6972 - auc_176: 0.7723\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5566 - binary_accuracy: 0.7064 - auc_176: 0.7833\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5474 - binary_accuracy: 0.7101 - auc_176: 0.7922\n",
      "Epoch 9/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5390 - binary_accuracy: 0.7229 - auc_176: 0.8008\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5320 - binary_accuracy: 0.7255 - auc_176: 0.8075\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5240 - binary_accuracy: 0.7335 - auc_176: 0.8147\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5203 - binary_accuracy: 0.7354 - auc_176: 0.8176\n",
      "Epoch 13/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5141 - binary_accuracy: 0.7429 - auc_176: 0.8235\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5083 - binary_accuracy: 0.7466 - auc_176: 0.8277\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.5042 - binary_accuracy: 0.7478 - auc_176: 0.8307\n",
      "Epoch 16/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4993 - binary_accuracy: 0.7565 - auc_176: 0.8348\n",
      "Epoch 17/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4954 - binary_accuracy: 0.7565 - auc_176: 0.8378\n",
      "Epoch 18/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4905 - binary_accuracy: 0.7622 - auc_176: 0.8411\n",
      "Epoch 19/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4870 - binary_accuracy: 0.7615 - auc_176: 0.8428\n",
      "Epoch 20/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4831 - binary_accuracy: 0.7632 - auc_176: 0.8464\n",
      "Epoch 21/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4809 - binary_accuracy: 0.7654 - auc_176: 0.8477\n",
      "Epoch 22/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4770 - binary_accuracy: 0.7659 - auc_176: 0.8507\n",
      "Epoch 23/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4744 - binary_accuracy: 0.7664 - auc_176: 0.8525\n",
      "Epoch 24/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4696 - binary_accuracy: 0.7684 - auc_176: 0.8557\n",
      "Epoch 25/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4681 - binary_accuracy: 0.7726 - auc_176: 0.8567\n",
      "Epoch 26/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4641 - binary_accuracy: 0.7721 - auc_176: 0.8589\n",
      "Epoch 27/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4607 - binary_accuracy: 0.7792 - auc_176: 0.8612\n",
      "Epoch 28/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4582 - binary_accuracy: 0.7765 - auc_176: 0.8634\n",
      "Epoch 29/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4542 - binary_accuracy: 0.7790 - auc_176: 0.8655\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4556 - binary_accuracy: 0.7758 - auc_176: 0.8647\n",
      "Epoch 31/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4533 - binary_accuracy: 0.7756 - auc_176: 0.8660\n",
      "Epoch 32/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4490 - binary_accuracy: 0.7827 - auc_176: 0.8688\n",
      "Epoch 33/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4475 - binary_accuracy: 0.7847 - auc_176: 0.8696\n",
      "Epoch 34/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4458 - binary_accuracy: 0.7841 - auc_176: 0.8705\n",
      "Epoch 35/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4450 - binary_accuracy: 0.7797 - auc_176: 0.8709\n",
      "Epoch 36/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4414 - binary_accuracy: 0.7831 - auc_176: 0.8734\n",
      "Epoch 37/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4374 - binary_accuracy: 0.7903 - auc_176: 0.8757\n",
      "Epoch 38/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4381 - binary_accuracy: 0.7884 - auc_176: 0.8755\n",
      "Epoch 39/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4366 - binary_accuracy: 0.7859 - auc_176: 0.8758\n",
      "Epoch 40/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4355 - binary_accuracy: 0.7905 - auc_176: 0.8772\n",
      "Epoch 41/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4331 - binary_accuracy: 0.7877 - auc_176: 0.8781\n",
      "Epoch 42/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4314 - binary_accuracy: 0.7859 - auc_176: 0.8787\n",
      "Epoch 43/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4290 - binary_accuracy: 0.7916 - auc_176: 0.8804\n",
      "Epoch 44/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4282 - binary_accuracy: 0.7926 - auc_176: 0.8810\n",
      "Epoch 45/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4265 - binary_accuracy: 0.7926 - auc_176: 0.8818\n",
      "Epoch 46/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4262 - binary_accuracy: 0.7903 - auc_176: 0.8821\n",
      "Epoch 47/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4233 - binary_accuracy: 0.7928 - auc_176: 0.8837\n",
      "Epoch 48/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4206 - binary_accuracy: 0.7946 - auc_176: 0.8851\n",
      "Epoch 49/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4222 - binary_accuracy: 0.7953 - auc_176: 0.8843\n",
      "Epoch 50/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4182 - binary_accuracy: 0.8011 - auc_176: 0.8866\n",
      "Epoch 51/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4176 - binary_accuracy: 0.7937 - auc_176: 0.8868\n",
      "Epoch 52/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4174 - binary_accuracy: 0.7921 - auc_176: 0.8870\n",
      "Epoch 53/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4160 - binary_accuracy: 0.7974 - auc_176: 0.8878\n",
      "Epoch 54/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4135 - binary_accuracy: 0.7937 - auc_176: 0.8890\n",
      "Epoch 55/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4129 - binary_accuracy: 0.8017 - auc_176: 0.8896\n",
      "Epoch 56/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4142 - binary_accuracy: 0.7896 - auc_176: 0.8879\n",
      "Epoch 57/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4114 - binary_accuracy: 0.7981 - auc_176: 0.8891\n",
      "Epoch 58/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4082 - binary_accuracy: 0.7999 - auc_176: 0.8920\n",
      "Epoch 59/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4082 - binary_accuracy: 0.8001 - auc_176: 0.8919\n",
      "Epoch 60/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4069 - binary_accuracy: 0.7978 - auc_176: 0.8918\n",
      "Epoch 61/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4055 - binary_accuracy: 0.7999 - auc_176: 0.8931\n",
      "Epoch 62/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4070 - binary_accuracy: 0.8043 - auc_176: 0.8925\n",
      "Epoch 63/100\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.4058 - binary_accuracy: 0.8054 - auc_176: 0.8934\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6417 - binary_accuracy: 0.6687 - auc_176: 0.7565\n",
      " binary_accuracy: 66.87%\t ------ auc_176: 75.65%\t \n",
      "ave_acc: 69.95% (+/- 2.00%)\t ------ ave_auc: 77.45% (+/- 2.16%)\t \n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1504 (Dense)           (None, 32)                1568      \n",
      "_________________________________________________________________\n",
      "dense_1505 (Dense)           (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_1506 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1507 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1508 (Dense)           (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 3,297\n",
      "Trainable params: 3,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "i=1\n",
    "cvacc=[]\n",
    "cvauc=[]\n",
    "\n",
    "mesh_label_concat=np.array(mesh_label_concat)\n",
    "\n",
    "seed = 22\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# np.random.seed(seed)\n",
    "python_random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy=1)\n",
    "\n",
    "rus_x,rus_y = rus.fit_resample(X=mesh_label_concat,y=y)\n",
    "print('rus_y的长度:',len(rus_y))\n",
    "\n",
    "\n",
    "Kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\n",
    "for train,test in Kfold.split(X=rus_x,y=rus_y):\n",
    "    print(\"--------------Kfold: {} iter\".format(i))\n",
    "    i+=1\n",
    "    rms = RMSprop()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=2)\n",
    "    ##################################################################################################\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_dim=48))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    \"\"\"L2\"\"\"\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer,'kernel_regularizer'):\n",
    "            layer.kernel_regularizer = tf.keras.regularizers.l2(0.001)\n",
    "    ##################################################################################################\n",
    "    model.compile(\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            optimizer=rms,\n",
    "            metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    \n",
    "    model.fit(rus_x[train],rus_y[train],\n",
    "              #class_weight={0:1,1:1},\n",
    "              batch_size=None,\n",
    "              epochs=100,\n",
    "              callbacks=callback,\n",
    "              shuffle=True)\n",
    "    \n",
    "    scores = model.evaluate(rus_x[test],rus_y[test],\n",
    "                            verbose=1,\n",
    "                            batch_size=None,\n",
    "                            callbacks=callback)\n",
    "    \n",
    "    cvacc.append(scores[1] * 100)\n",
    "    cvauc.append(scores[2] * 100)\n",
    "    \n",
    "    print(\" %s: %.2f%%\\t ------ %s: %.2f%%\\t \" % \n",
    "           (model.metrics_names[1],scores[1]*100,model.metrics_names[2],scores[2]*100))\n",
    "    \n",
    "#print(\"ave_auc_keras: %.2f%% (+/- %.2f%%)\\t ave_topk_pre: %.2f%% (+/- %.2f%%)\\t ave_auc:%.2f%% (+/- %.2f%%)\" % \n",
    "print(\"ave_acc: %.2f%% (+/- %.2f%%)\\t ------ ave_auc: %.2f%% (+/- %.2f%%)\\t \" % \n",
    "      (np.mean(cvacc), np.std(cvacc),np.mean(cvauc), np.std(cvauc)))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ave_acc: 69.93% (+/- 1.77%)\t ------ ave_auc: 76.81% (+/- 1.83%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test5:Mesh+sym（Siamese）\n",
    "splicing disease symptom and mesh features of diseasea pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35245, 346)\n"
     ]
    }
   ],
   "source": [
    "confeaA = pd.read_csv(\"concat_mesh_label/con_feaA.csv\",index_col=0)\n",
    "print(confeaA.shape)\n",
    "confeaB = pd.read_csv(\"concat_mesh_label/con_feaB.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus_y的长度: 4836\n",
      "--------------Kfold: 1 iter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'PYTHONHASHSEED' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 1.1800 - binary_accuracy: 0.4998\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.9645 - binary_accuracy: 0.5062\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.9236 - binary_accuracy: 0.5221\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8810 - binary_accuracy: 0.5257\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8539 - binary_accuracy: 0.5618\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8712 - binary_accuracy: 0.5558\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8573 - binary_accuracy: 0.5492\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8289 - binary_accuracy: 0.5724\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7878 - binary_accuracy: 0.5767\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7857 - binary_accuracy: 0.5970\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7822 - binary_accuracy: 0.5873\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7781 - binary_accuracy: 0.5915\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7659 - binary_accuracy: 0.6172\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7950 - binary_accuracy: 0.6020\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7586 - binary_accuracy: 0.6257\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7450 - binary_accuracy: 0.6140\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7278 - binary_accuracy: 0.6025\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7124 - binary_accuracy: 0.6234\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7191 - binary_accuracy: 0.6163\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7047 - binary_accuracy: 0.6248\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6943 - binary_accuracy: 0.6312\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6869 - binary_accuracy: 0.6450\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6689 - binary_accuracy: 0.6523\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6826 - binary_accuracy: 0.6494\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6868 - binary_accuracy: 0.6358\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6645 - binary_accuracy: 0.6466\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6715 - binary_accuracy: 0.6530\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6647 - binary_accuracy: 0.6553\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6613 - binary_accuracy: 0.6528\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6619 - binary_accuracy: 0.6523\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6526 - binary_accuracy: 0.6551\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6427 - binary_accuracy: 0.6675\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6512 - binary_accuracy: 0.6641\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6633 - binary_accuracy: 0.6643\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6544 - binary_accuracy: 0.6622\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6591 - binary_accuracy: 0.6673\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6602 - binary_accuracy: 0.6703\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001C8934950D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6677 - binary_accuracy: 0.6529\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C8A469EEE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "y_pred长度： 484\n",
      " binary_accuracy: 65.29%\t topk_pre: 26.00%\t AUC_all: 70.21%\t\n",
      "--------------Kfold: 2 iter\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 1.2319 - binary_accuracy: 0.5053\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.9548 - binary_accuracy: 0.5195\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.9333 - binary_accuracy: 0.5136\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.9091 - binary_accuracy: 0.5269\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.9024 - binary_accuracy: 0.5437\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8711 - binary_accuracy: 0.5577\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8598 - binary_accuracy: 0.5464\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8604 - binary_accuracy: 0.5260\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8134 - binary_accuracy: 0.5591\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8323 - binary_accuracy: 0.5646\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8177 - binary_accuracy: 0.5641\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7789 - binary_accuracy: 0.5816\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7499 - binary_accuracy: 0.6050\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7670 - binary_accuracy: 0.5926\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7494 - binary_accuracy: 0.6013\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7438 - binary_accuracy: 0.6000\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7258 - binary_accuracy: 0.6209\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7154 - binary_accuracy: 0.6243\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7084 - binary_accuracy: 0.6395\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6793 - binary_accuracy: 0.6540\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6784 - binary_accuracy: 0.6402\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6824 - binary_accuracy: 0.6340\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6799 - binary_accuracy: 0.6468\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6680 - binary_accuracy: 0.6581\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6703 - binary_accuracy: 0.6452\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6763 - binary_accuracy: 0.6487\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6772 - binary_accuracy: 0.6526\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6627 - binary_accuracy: 0.6432\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6716 - binary_accuracy: 0.6514\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6521 - binary_accuracy: 0.6535\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6527 - binary_accuracy: 0.6459\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6659 - binary_accuracy: 0.6498\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6658 - binary_accuracy: 0.6556\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6637 - binary_accuracy: 0.6549\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6575 - binary_accuracy: 0.6641\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001C8955B65E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7468 - binary_accuracy: 0.6033\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C8A6C548B8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "y_pred长度： 484\n",
      " binary_accuracy: 60.33%\t topk_pre: 29.00%\t AUC_all: 66.33%\t\n",
      "--------------Kfold: 3 iter\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 1.1322 - binary_accuracy: 0.5094\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.9511 - binary_accuracy: 0.5149\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.9023 - binary_accuracy: 0.5161\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8440 - binary_accuracy: 0.5335\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8432 - binary_accuracy: 0.5441\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8369 - binary_accuracy: 0.5478\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8199 - binary_accuracy: 0.5630\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7761 - binary_accuracy: 0.5602\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7839 - binary_accuracy: 0.5616\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7743 - binary_accuracy: 0.5754\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7482 - binary_accuracy: 0.5889\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7442 - binary_accuracy: 0.5944\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7224 - binary_accuracy: 0.6045\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7163 - binary_accuracy: 0.6211\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7238 - binary_accuracy: 0.6057\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7086 - binary_accuracy: 0.6151\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6953 - binary_accuracy: 0.6356\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6948 - binary_accuracy: 0.6271\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6756 - binary_accuracy: 0.6388\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6950 - binary_accuracy: 0.6358\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6933 - binary_accuracy: 0.6356\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6682 - binary_accuracy: 0.6411\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6902 - binary_accuracy: 0.6544\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6776 - binary_accuracy: 0.6461\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6812 - binary_accuracy: 0.6595\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6640 - binary_accuracy: 0.6700\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6581 - binary_accuracy: 0.6739\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6513 - binary_accuracy: 0.6776\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6416 - binary_accuracy: 0.6838\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6545 - binary_accuracy: 0.6654\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6473 - binary_accuracy: 0.6746\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6446 - binary_accuracy: 0.6756\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6299 - binary_accuracy: 0.6774\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6379 - binary_accuracy: 0.6668\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6140 - binary_accuracy: 0.6900\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6178 - binary_accuracy: 0.6884\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6173 - binary_accuracy: 0.6843\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6450 - binary_accuracy: 0.6590\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6411 - binary_accuracy: 0.6615\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6434 - binary_accuracy: 0.6625\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001C8935A0EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 0s/step - loss: 0.6765 - binary_accuracy: 0.6260\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C893495048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "y_pred长度： 484\n",
      " binary_accuracy: 62.60%\t topk_pre: 28.00%\t AUC_all: 67.87%\t\n",
      "--------------Kfold: 4 iter\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 1.1670 - binary_accuracy: 0.5119\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.9337 - binary_accuracy: 0.5154\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.9119 - binary_accuracy: 0.5179\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8938 - binary_accuracy: 0.5400\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8707 - binary_accuracy: 0.5308\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8729 - binary_accuracy: 0.5349\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8546 - binary_accuracy: 0.5437\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8339 - binary_accuracy: 0.5604\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.8160 - binary_accuracy: 0.5551\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7955 - binary_accuracy: 0.5604\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8143 - binary_accuracy: 0.5570\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7699 - binary_accuracy: 0.5924\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7658 - binary_accuracy: 0.5933\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7459 - binary_accuracy: 0.6050\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7466 - binary_accuracy: 0.6105\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7662 - binary_accuracy: 0.5800\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7493 - binary_accuracy: 0.5912\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7529 - binary_accuracy: 0.5875\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7487 - binary_accuracy: 0.6016\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001C8A469ED38> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 986us/step - loss: 0.7523 - binary_accuracy: 0.5868\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C8A6CB0CA8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "y_pred长度： 484\n",
      " binary_accuracy: 58.68%\t topk_pre: 31.00%\t AUC_all: 64.69%\t\n",
      "--------------Kfold: 5 iter\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 1.2493 - binary_accuracy: 0.5028\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.9593 - binary_accuracy: 0.5037\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.9127 - binary_accuracy: 0.5186\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8429 - binary_accuracy: 0.5469\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8907 - binary_accuracy: 0.5473\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8699 - binary_accuracy: 0.5462\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8454 - binary_accuracy: 0.5506\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8548 - binary_accuracy: 0.5441\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8193 - binary_accuracy: 0.5726\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8180 - binary_accuracy: 0.5816\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8023 - binary_accuracy: 0.5719\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7714 - binary_accuracy: 0.5954\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7699 - binary_accuracy: 0.5848\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7322 - binary_accuracy: 0.6055\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7331 - binary_accuracy: 0.6080\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7192 - binary_accuracy: 0.6261\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7066 - binary_accuracy: 0.6471\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7009 - binary_accuracy: 0.6585\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6921 - binary_accuracy: 0.6411\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7086 - binary_accuracy: 0.6241\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6978 - binary_accuracy: 0.6395\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6797 - binary_accuracy: 0.6441\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6653 - binary_accuracy: 0.6498\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6585 - binary_accuracy: 0.6712\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6641 - binary_accuracy: 0.6551\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6569 - binary_accuracy: 0.6677\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6656 - binary_accuracy: 0.6712\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6670 - binary_accuracy: 0.6556\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6766 - binary_accuracy: 0.6588\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6431 - binary_accuracy: 0.6579\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6421 - binary_accuracy: 0.6608\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6462 - binary_accuracy: 0.6661\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6425 - binary_accuracy: 0.6602\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6305 - binary_accuracy: 0.6710\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6417 - binary_accuracy: 0.6719\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6199 - binary_accuracy: 0.6811\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6286 - binary_accuracy: 0.6687\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6283 - binary_accuracy: 0.6767\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6423 - binary_accuracy: 0.6682\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6590 - binary_accuracy: 0.6604\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6506 - binary_accuracy: 0.6751\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001C8A4CF3DC8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7021 - binary_accuracy: 0.6446\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C8A4CF3798> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "y_pred长度： 484\n",
      " binary_accuracy: 64.46%\t topk_pre: 15.00%\t AUC_all: 70.91%\t\n",
      "--------------Kfold: 6 iter\n",
      "Epoch 1/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 1.2191 - binary_accuracy: 0.5011\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.9446 - binary_accuracy: 0.5154\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.9192 - binary_accuracy: 0.5271\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.9070 - binary_accuracy: 0.5368\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8842 - binary_accuracy: 0.5361\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8229 - binary_accuracy: 0.5623\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8356 - binary_accuracy: 0.5492\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8535 - binary_accuracy: 0.5483\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.8152 - binary_accuracy: 0.5535\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7966 - binary_accuracy: 0.5742\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7947 - binary_accuracy: 0.5786\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7847 - binary_accuracy: 0.5958\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7672 - binary_accuracy: 0.5846\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7858 - binary_accuracy: 0.5834\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7680 - binary_accuracy: 0.5944\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7258 - binary_accuracy: 0.6045\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7141 - binary_accuracy: 0.6153\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6995 - binary_accuracy: 0.6234\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7147 - binary_accuracy: 0.6333\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7213 - binary_accuracy: 0.6261\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7013 - binary_accuracy: 0.6356\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.7072 - binary_accuracy: 0.6213\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6913 - binary_accuracy: 0.6369\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6679 - binary_accuracy: 0.6588\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6796 - binary_accuracy: 0.6480\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6724 - binary_accuracy: 0.6438\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6744 - binary_accuracy: 0.6386\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6751 - binary_accuracy: 0.6294\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6604 - binary_accuracy: 0.6406\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6671 - binary_accuracy: 0.6328\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6772 - binary_accuracy: 0.6374\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6667 - binary_accuracy: 0.6487\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.6661 - binary_accuracy: 0.6558\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6581 - binary_accuracy: 0.6482\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6829 - binary_accuracy: 0.6379\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6575 - binary_accuracy: 0.6526\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6670 - binary_accuracy: 0.6459\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6552 - binary_accuracy: 0.6530\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6487 - binary_accuracy: 0.6558\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6649 - binary_accuracy: 0.6450\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6525 - binary_accuracy: 0.6622\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6507 - binary_accuracy: 0.6542\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6392 - binary_accuracy: 0.6693\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6433 - binary_accuracy: 0.6599\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6349 - binary_accuracy: 0.6650\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6355 - binary_accuracy: 0.6675\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6324 - binary_accuracy: 0.6661\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6234 - binary_accuracy: 0.6788\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6234 - binary_accuracy: 0.6797\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6285 - binary_accuracy: 0.6756\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6103 - binary_accuracy: 0.6875\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6197 - binary_accuracy: 0.6756\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6286 - binary_accuracy: 0.6739\n",
      "Epoch 54/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6222 - binary_accuracy: 0.6781\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6271 - binary_accuracy: 0.6737\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.6140 - binary_accuracy: 0.6824\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001C8A4973318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 984us/step - loss: 0.6533 - binary_accuracy: 0.6674\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C8934958B8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "y_pred长度： 484\n",
      " binary_accuracy: 66.74%\t topk_pre: 26.00%\t AUC_all: 72.22%\t\n",
      "--------------Kfold: 7 iter\n",
      "Epoch 1/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 1.0866 - binary_accuracy: 0.5072\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.9411 - binary_accuracy: 0.5084\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.9202 - binary_accuracy: 0.5116\n",
      "Epoch 4/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8734 - binary_accuracy: 0.5309\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8760 - binary_accuracy: 0.5378\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8667 - binary_accuracy: 0.5550\n",
      "Epoch 7/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8588 - binary_accuracy: 0.5598\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8271 - binary_accuracy: 0.5801\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7968 - binary_accuracy: 0.5847\n",
      "Epoch 10/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7838 - binary_accuracy: 0.5904\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7704 - binary_accuracy: 0.5961\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7734 - binary_accuracy: 0.5948\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7502 - binary_accuracy: 0.6003\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7509 - binary_accuracy: 0.6102\n",
      "Epoch 15/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7303 - binary_accuracy: 0.6102\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7332 - binary_accuracy: 0.6164\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7228 - binary_accuracy: 0.6143\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7191 - binary_accuracy: 0.6274\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6805 - binary_accuracy: 0.6416\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6877 - binary_accuracy: 0.6501\n",
      "Epoch 21/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6903 - binary_accuracy: 0.6384\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6920 - binary_accuracy: 0.6566\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6805 - binary_accuracy: 0.6444\n",
      "Epoch 24/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7339 - binary_accuracy: 0.6320\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7359 - binary_accuracy: 0.6343\n",
      "Epoch 26/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7306 - binary_accuracy: 0.6315\n",
      "Epoch 27/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7159 - binary_accuracy: 0.6370\n",
      "Epoch 28/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7035 - binary_accuracy: 0.6471\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001C895FBD4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 983us/step - loss: 0.7422 - binary_accuracy: 0.6356\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C8A714BCA8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "y_pred长度： 483\n",
      " binary_accuracy: 63.56%\t topk_pre: 30.00%\t AUC_all: 68.41%\t\n",
      "--------------Kfold: 8 iter\n",
      "Epoch 1/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 1.1524 - binary_accuracy: 0.5020\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.9648 - binary_accuracy: 0.5164\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.9076 - binary_accuracy: 0.5288\n",
      "Epoch 4/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8883 - binary_accuracy: 0.5279\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8536 - binary_accuracy: 0.5394\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8535 - binary_accuracy: 0.5440\n",
      "Epoch 7/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8424 - binary_accuracy: 0.5325\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8061 - binary_accuracy: 0.5559\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8014 - binary_accuracy: 0.5587\n",
      "Epoch 10/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8024 - binary_accuracy: 0.5773\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7922 - binary_accuracy: 0.5837\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7680 - binary_accuracy: 0.6026\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7501 - binary_accuracy: 0.5964\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7333 - binary_accuracy: 0.6060\n",
      "Epoch 15/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7303 - binary_accuracy: 0.6143\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7414 - binary_accuracy: 0.6120\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7421 - binary_accuracy: 0.6088\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7352 - binary_accuracy: 0.6285\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7336 - binary_accuracy: 0.6244\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7098 - binary_accuracy: 0.6251\n",
      "Epoch 21/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7236 - binary_accuracy: 0.6120\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7060 - binary_accuracy: 0.6102\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7010 - binary_accuracy: 0.6216\n",
      "Epoch 24/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6812 - binary_accuracy: 0.6327\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6846 - binary_accuracy: 0.6391\n",
      "Epoch 26/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7233 - binary_accuracy: 0.6400\n",
      "Epoch 27/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6769 - binary_accuracy: 0.6504\n",
      "Epoch 28/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6651 - binary_accuracy: 0.6607\n",
      "Epoch 29/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6684 - binary_accuracy: 0.6639\n",
      "Epoch 30/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6886 - binary_accuracy: 0.6559\n",
      "Epoch 31/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6654 - binary_accuracy: 0.6584\n",
      "Epoch 32/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6638 - binary_accuracy: 0.6543\n",
      "Epoch 33/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6561 - binary_accuracy: 0.6630\n",
      "Epoch 34/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6582 - binary_accuracy: 0.6674\n",
      "Epoch 35/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6529 - binary_accuracy: 0.6607\n",
      "Epoch 36/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6547 - binary_accuracy: 0.6623\n",
      "Epoch 37/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6464 - binary_accuracy: 0.6671\n",
      "Epoch 38/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6486 - binary_accuracy: 0.6641\n",
      "Epoch 39/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6309 - binary_accuracy: 0.6892\n",
      "Epoch 40/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6380 - binary_accuracy: 0.6878\n",
      "Epoch 41/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6391 - binary_accuracy: 0.6747\n",
      "Epoch 42/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6490 - binary_accuracy: 0.6614\n",
      "Epoch 43/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6402 - binary_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6337 - binary_accuracy: 0.6830\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001C8A7B66678> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6852 - binary_accuracy: 0.6667\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C8A3D59DC8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "y_pred长度： 483\n",
      " binary_accuracy: 66.67%\t topk_pre: 28.00%\t AUC_all: 71.75%\t\n",
      "--------------Kfold: 9 iter\n",
      "Epoch 1/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 1.1937 - binary_accuracy: 0.4925\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.9628 - binary_accuracy: 0.5134\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.9200 - binary_accuracy: 0.5224\n",
      "Epoch 4/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.9001 - binary_accuracy: 0.5337\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8745 - binary_accuracy: 0.5373\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8632 - binary_accuracy: 0.5426\n",
      "Epoch 7/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8333 - binary_accuracy: 0.5635\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8317 - binary_accuracy: 0.5431\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8032 - binary_accuracy: 0.5681\n",
      "Epoch 10/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7865 - binary_accuracy: 0.5833\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7864 - binary_accuracy: 0.5693\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7575 - binary_accuracy: 0.5925\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7567 - binary_accuracy: 0.5941\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7397 - binary_accuracy: 0.6014\n",
      "Epoch 15/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7389 - binary_accuracy: 0.6074\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7602 - binary_accuracy: 0.6069\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7402 - binary_accuracy: 0.6203\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7351 - binary_accuracy: 0.6214\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7237 - binary_accuracy: 0.6164\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7337 - binary_accuracy: 0.6081\n",
      "Epoch 21/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7047 - binary_accuracy: 0.6184\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7099 - binary_accuracy: 0.6276\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7045 - binary_accuracy: 0.6345\n",
      "Epoch 24/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7150 - binary_accuracy: 0.6283\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6910 - binary_accuracy: 0.6402\n",
      "Epoch 26/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7047 - binary_accuracy: 0.6435\n",
      "Epoch 27/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7061 - binary_accuracy: 0.6377\n",
      "Epoch 28/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6890 - binary_accuracy: 0.6407\n",
      "Epoch 29/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6827 - binary_accuracy: 0.6391\n",
      "Epoch 30/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6855 - binary_accuracy: 0.6561\n",
      "Epoch 31/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6794 - binary_accuracy: 0.6593\n",
      "Epoch 32/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6730 - binary_accuracy: 0.6612\n",
      "Epoch 33/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6668 - binary_accuracy: 0.6559\n",
      "Epoch 34/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6497 - binary_accuracy: 0.6692\n",
      "Epoch 35/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6694 - binary_accuracy: 0.6540\n",
      "Epoch 36/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6565 - binary_accuracy: 0.6623\n",
      "Epoch 37/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6619 - binary_accuracy: 0.6547\n",
      "Epoch 38/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6493 - binary_accuracy: 0.6742\n",
      "Epoch 39/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6578 - binary_accuracy: 0.6749\n",
      "Epoch 40/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6697 - binary_accuracy: 0.6370\n",
      "Epoch 41/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6937 - binary_accuracy: 0.6407\n",
      "Epoch 42/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6675 - binary_accuracy: 0.6529\n",
      "Epoch 43/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6690 - binary_accuracy: 0.6469\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001C8A6CB0558> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7096 - binary_accuracy: 0.6377\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C8A6C54D38> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "y_pred长度： 483\n",
      " binary_accuracy: 63.77%\t topk_pre: 28.00%\t AUC_all: 68.31%\t\n",
      "--------------Kfold: 10 iter\n",
      "Epoch 1/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 1.1450 - binary_accuracy: 0.5082\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.9081 - binary_accuracy: 0.5167\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.9011 - binary_accuracy: 0.5183\n",
      "Epoch 4/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8812 - binary_accuracy: 0.5355\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8509 - binary_accuracy: 0.5318\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8519 - binary_accuracy: 0.5472\n",
      "Epoch 7/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8374 - binary_accuracy: 0.5500\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8320 - binary_accuracy: 0.5454\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8357 - binary_accuracy: 0.5564\n",
      "Epoch 10/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8126 - binary_accuracy: 0.5518\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8164 - binary_accuracy: 0.5651\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8106 - binary_accuracy: 0.5580\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.8372 - binary_accuracy: 0.5633\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7904 - binary_accuracy: 0.5697\n",
      "Epoch 15/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7841 - binary_accuracy: 0.5725\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7521 - binary_accuracy: 0.5932\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7710 - binary_accuracy: 0.5791\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7436 - binary_accuracy: 0.5945\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7265 - binary_accuracy: 0.6058\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7505 - binary_accuracy: 0.6046\n",
      "Epoch 21/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7416 - binary_accuracy: 0.6122\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6947 - binary_accuracy: 0.6345\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7225 - binary_accuracy: 0.6396\n",
      "Epoch 24/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.7091 - binary_accuracy: 0.6467\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6922 - binary_accuracy: 0.6393\n",
      "Epoch 26/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6787 - binary_accuracy: 0.6508\n",
      "Epoch 27/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6835 - binary_accuracy: 0.6545\n",
      "Epoch 28/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6601 - binary_accuracy: 0.6547\n",
      "Epoch 29/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6717 - binary_accuracy: 0.6501\n",
      "Epoch 30/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6703 - binary_accuracy: 0.6446\n",
      "Epoch 31/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6779 - binary_accuracy: 0.6380\n",
      "Epoch 32/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6732 - binary_accuracy: 0.6451\n",
      "Epoch 33/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6517 - binary_accuracy: 0.6474\n",
      "Epoch 34/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6524 - binary_accuracy: 0.6527\n",
      "Epoch 35/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6539 - binary_accuracy: 0.6515\n",
      "Epoch 36/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6477 - binary_accuracy: 0.6586\n",
      "Epoch 37/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6700 - binary_accuracy: 0.6593\n",
      "Epoch 38/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6525 - binary_accuracy: 0.6598\n",
      "Epoch 39/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6423 - binary_accuracy: 0.6550\n",
      "Epoch 40/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6422 - binary_accuracy: 0.6616\n",
      "Epoch 41/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6322 - binary_accuracy: 0.6699\n",
      "Epoch 42/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6327 - binary_accuracy: 0.6671\n",
      "Epoch 43/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6232 - binary_accuracy: 0.6745\n",
      "Epoch 44/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6280 - binary_accuracy: 0.6713\n",
      "Epoch 45/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6397 - binary_accuracy: 0.6641\n",
      "Epoch 46/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6367 - binary_accuracy: 0.6694\n",
      "Epoch 47/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6244 - binary_accuracy: 0.6717\n",
      "Epoch 48/200\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.6239 - binary_accuracy: 0.6768\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001C8A70103A8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 984us/step - loss: 0.6877 - binary_accuracy: 0.6542\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C8A4D75558> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "y_pred长度： 483\n",
      " binary_accuracy: 65.42%\t topk_pre: 23.00%\t AUC_all: 70.90%\t\n",
      "ave_acc: 63.75% (+/- 2.49%)\t ave_topk_pre: 26.40% (+/- 4.36%)\t ave_auc:69.16% (+/- 2.33%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics  import roc_curve,auc,roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "cvacc=[]\n",
    "cvauc=[]\n",
    "cvauc_keras=[]\n",
    "cvpre=[]\n",
    "cvpre_top_keras =[]\n",
    "cvpre_top=[]\n",
    "cvrec=[]\n",
    "i =1\n",
    "\n",
    "y=np.array(y)\n",
    "disA_fea_mat=np.array(confeaA)\n",
    "disB_fea_mat=np.array(confeaB)\n",
    "\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy=1,random_state=11)\n",
    "rus_dfmA,rus_y = rus.fit_resample(X=disA_fea_mat,y=y)\n",
    "rus_dfmB,rus_y = rus.fit_resample(X=disB_fea_mat,y=y)\n",
    "print('rus_y的长度:',len(rus_y))\n",
    "\n",
    "\n",
    "seed = 201202\n",
    "!PYTHONHASHSEED=0 \n",
    "np.random.seed(seed) # This is necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "python_random.seed(seed)# This is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\"\"\"10-fold\"\"\"\n",
    "Kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\n",
    "for train,test in Kfold.split(X=rus_dfmA,y=rus_y):\n",
    "    print(\"--------------Kfold: {} iter\".format(i))\n",
    "    i+=1\n",
    "\n",
    "    rms = RMSprop()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=5)\n",
    "    \n",
    "    ####################################################################################\n",
    "\n",
    "    input_shape=(346)\n",
    "    base_network = create_base_network(input_shape)\n",
    "    \n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "    # because we re-use the same instance `base_network`,the weights of the network will be shared across the two branches\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    \n",
    "\n",
    "    output_distance = euclLayer()(processed_a,processed_b)\n",
    "    final_out= 1-tf.keras.activations.tanh(output_distance)\n",
    "\n",
    "    \n",
    "    model = Model([input_a, input_b], final_out)\n",
    "    \n",
    "    ####################################################################################\n",
    "    model.compile(\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "#             loss= contrastive_loss,\n",
    "#             loss= tfa.losses.ContrastiveLoss(),\n",
    "            optimizer=Adam(),\n",
    "            metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "            #metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    \n",
    "    # fit()中 shuffle=True\n",
    "    model.fit([rus_dfmA[train],rus_dfmB[train]],rus_y[train],\n",
    "              class_weight={0:1,1:1},\n",
    "              batch_size=None,\n",
    "              epochs=200,\n",
    "              callbacks=callback,\n",
    "              shuffle=True)\n",
    "    \n",
    "    scores = model.evaluate([rus_dfmA[test],rus_dfmB[test]],rus_y[test],\n",
    "                            verbose=1,\n",
    "                            batch_size=len(rus_y[test]),\n",
    "                            callbacks=callback)\n",
    "    \n",
    "    \"\"\"###########################AUC,topk Precision#############################\"\"\"\n",
    "    y_pred = model.predict([rus_dfmA[test],rus_dfmB[test]],\n",
    "                          batch_size = len(rus_y[test]))\n",
    "    print(\"y_pred长度：\",len(y_pred))\n",
    "    \n",
    "    roc_auc = roc_auc_score(rus_y[test],y_pred)\n",
    "    \n",
    "    #top_Precision\n",
    "    top = 100\n",
    "    y_pred = y_pred.flatten()\n",
    "    topk_pre = top_precision(rus_y[test],y_pred,top)\n",
    "    #print(rus_y[test])\n",
    "    #print(y_pred)\n",
    "    \n",
    "    cvauc.append(roc_auc * 100)\n",
    "    cvpre_top.append(topk_pre * 100)\n",
    "    cvacc.append(scores[1] * 100)\n",
    "    #cvpre_top_keras.append(scores[2] * 100)\n",
    "    #cvauc_keras.append(scores[1] * 100)\n",
    "    \n",
    "    print(\" %s: %.2f%%\\t %s: %.2f%%\\t %s: %.2f%%\\t\" % \n",
    "           (model.metrics_names[1],scores[1]*100,\"topk_pre\",topk_pre*100, \"AUC_all\",roc_auc*100))\n",
    "     \n",
    "#print(\"ave_auc_keras: %.2f%% (+/- %.2f%%)\\t ave_topk_pre: %.2f%% (+/- %.2f%%)\\t ave_auc:%.2f%% (+/- %.2f%%)\" % \n",
    "print(\"ave_acc: %.2f%% (+/- %.2f%%)\\t ave_topk_pre: %.2f%% (+/- %.2f%%)\\t ave_auc:%.2f%% (+/- %.2f%%)\" % \n",
    "      (np.mean(cvacc), np.std(cvacc),np.mean(cvpre_top), np.std(cvpre_top),np.mean(cvauc), np.std(cvauc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ave_acc: 63.75% (+/- 2.49%)\t ave_topk_pre: 26.40% (+/- 4.36%)\t ave_auc:69.16% (+/- 2.33%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "env": {
   "PYTHONHASHSEED": "0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "580px",
    "left": "613px",
    "top": "110.8px",
    "width": "317.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
